{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YLbBP7SWshbz"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > main.py << 'EOF'\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "# ─── CharVocab & Dataset ─────────────────────────────────────────────────────\n",
        "class CharVocab:\n",
        "    def __init__(self, filepaths: List[str]):\n",
        "        self.rom_char2idx: Dict[str,int] = {}\n",
        "        self.dev_char2idx: Dict[str,int] = {}\n",
        "        self.rom_idx2char: Dict[int,str] = {}\n",
        "        self.dev_idx2char: Dict[int,str] = {}\n",
        "        self._build_vocab(filepaths)\n",
        "\n",
        "    def _build_vocab(self, filepaths: List[str]):\n",
        "        rom_chars = set()\n",
        "        dev_chars = set()\n",
        "        for fp in filepaths:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f, delimiter=\"\\t\")\n",
        "                for row in reader:\n",
        "                    if len(row) < 2:\n",
        "                        continue\n",
        "                    devanagari = row[0].strip()\n",
        "                    roman      = row[1].strip()\n",
        "                    rom_chars.update(list(roman))\n",
        "                    dev_chars.update(list(devanagari))\n",
        "\n",
        "        PAD, SOS, EOS = \"<pad>\", \"<sos>\", \"<eos>\"\n",
        "\n",
        "        all_rom = [PAD, SOS, EOS] + sorted(rom_chars)\n",
        "        for i, ch in enumerate(all_rom):\n",
        "            self.rom_char2idx[ch] = i\n",
        "            self.rom_idx2char[i] = ch\n",
        "\n",
        "        all_dev = [PAD, SOS, EOS] + sorted(dev_chars)\n",
        "        for i, ch in enumerate(all_dev):\n",
        "            self.dev_char2idx[ch] = i\n",
        "            self.dev_idx2char[i] = ch\n",
        "\n",
        "        self.rom_pad_idx = self.rom_char2idx[PAD]\n",
        "        self.rom_sos_idx = self.rom_char2idx[SOS]\n",
        "        self.rom_eos_idx = self.rom_char2idx[EOS]\n",
        "\n",
        "        self.dev_pad_idx = self.dev_char2idx[PAD]\n",
        "        self.dev_sos_idx = self.dev_char2idx[SOS]\n",
        "        self.dev_eos_idx = self.dev_char2idx[EOS]\n",
        "\n",
        "    @property\n",
        "    def rom_vocab_size(self) -> int:\n",
        "        return len(self.rom_char2idx)\n",
        "\n",
        "    @property\n",
        "    def dev_vocab_size(self) -> int:\n",
        "        return len(self.dev_char2idx)\n",
        "\n",
        "    def roman_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.rom_sos_idx] + [self.rom_char2idx[ch] for ch in s] + [self.rom_eos_idx]\n",
        "\n",
        "    def dev_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.dev_sos_idx] + [self.dev_char2idx[ch] for ch in s] + [self.dev_eos_idx]\n",
        "\n",
        "    def indices_to_dev(self, idxs: List[int]) -> str:\n",
        "        chars = []\n",
        "        for i in idxs:\n",
        "            if i in (self.dev_sos_idx, self.dev_eos_idx, self.dev_pad_idx):\n",
        "                continue\n",
        "            chars.append(self.dev_idx2char[i])\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "def read_tsv(path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Expects each line of the TSV to be:\n",
        "      Devanagari_word    Roman_word    <something_to_ignore>\n",
        "    We only need (Roman, Devanagari) for training.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            # If there are fewer than 2 columns, skip\n",
        "            if len(row) < 2:\n",
        "                continue\n",
        "\n",
        "            # Unpack: Dev is first column, Roman is second, ignore anything else\n",
        "            devana = row[0].strip()\n",
        "            roman  = row[1].strip()\n",
        "\n",
        "            if not roman or not devana:\n",
        "                continue\n",
        "            # Append (roman, devana)—this matches our CharVocab convention\n",
        "            pairs.append((roman, devana))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, filepath, vocab):\n",
        "        super().__init__()\n",
        "        self.pairs = read_tsv(filepath)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        roman, devanagari = self.pairs[idx]\n",
        "        roman_idxs = self.vocab.roman_to_indices(roman)\n",
        "        dev_idxs = self.vocab.dev_to_indices(devanagari)\n",
        "        return torch.tensor(roman_idxs, dtype=torch.long), torch.tensor(dev_idxs, dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        roman_seqs, dev_seqs = zip(*batch)\n",
        "        max_rom_len = max(len(x) for x in roman_seqs)\n",
        "        max_dev_len = max(len(x) for x in dev_seqs)\n",
        "        rom_padded = []\n",
        "        dev_padded = []\n",
        "        for r, d in zip(roman_seqs, dev_seqs):\n",
        "            pad_r = torch.cat([r, r.new_full((max_rom_len - len(r),), r.new_tensor(0))])\n",
        "            pad_d = torch.cat([d, d.new_full((max_dev_len - len(d),), d.new_tensor(0))])\n",
        "            rom_padded.append(pad_r)\n",
        "            dev_padded.append(pad_d)\n",
        "        return torch.stack(rom_padded), torch.stack(dev_padded)\n",
        "\n",
        "\n",
        "# ─── Attention Module ─────────────────────────────────────────────────────────\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        # For computing alignment scores\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.V = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (num_layers, batch_size, hidden_size) --> take top layer\n",
        "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
        "        # Extract the top-layer hidden state for computing scores:\n",
        "        h = hidden[-1]  # (B, H)\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "\n",
        "        # Expand h to (B, seq_len, H) so we can add it to encoder_outputs-transformed\n",
        "        h_expanded = h.unsqueeze(1).repeat(1, seq_len, 1)  # (B, T, H)\n",
        "\n",
        "        # alignment: score = V^T * tanh(W1(enc_out) + W2(h_expanded))\n",
        "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(h_expanded)))  # (B, T, 1)\n",
        "\n",
        "        # Compute attention weights over all encoder time steps:\n",
        "        attention_weights = F.softmax(score.squeeze(2), dim=1)  # (B, T)\n",
        "\n",
        "        # Weighted sum of encoder_outputs, giving a context vector of shape (B, H)\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (B, 1, H)\n",
        "        context = context.squeeze(1)  # (B, H)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "# ─── Encoder ───────────────────────────────────────────────────────────────────\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(input_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        emb = self.embed(x)  # (B, T, E)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (h_n, c_n) = self.rnn(emb)\n",
        "            return outputs, (h_n, c_n)\n",
        "        else:\n",
        "            outputs, h_n = self.rnn(emb)\n",
        "            return outputs, h_n\n",
        "\n",
        "\n",
        "# ─── Decoder with Attention ────────────────────────────────────────────────────\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(output_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "\n",
        "        # The RNN input dimension is now (embedding + context_vector)\n",
        "        self.input_dim = embed_dim + hidden_size\n",
        "\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                self.input_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                self.input_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                self.input_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        # Bahdanau attention module\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "        # Final linear layer to project RNN output to vocabulary size\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, tgt_seq, hidden, cell=None, encoder_outputs=None, teacher_forcing_ratio=0.0):\n",
        "        \"\"\"\n",
        "        tgt_seq: (batch_size, T_tgt)         -- includes <sos> ... <eos>\n",
        "        hidden:   (num_layers, batch_size, hidden_size)\n",
        "        cell:     (num_layers, batch_size, hidden_size)  # only for LSTM\n",
        "        encoder_outputs: (batch_size, T_src, hidden_size)\n",
        "        \"\"\"\n",
        "        B, T = tgt_seq.size()\n",
        "        outputs = torch.zeros(B, T, self.out.out_features, device=tgt_seq.device)\n",
        "\n",
        "        # The first input token to the decoder is always <sos>\n",
        "        input_step = tgt_seq[:, 0]  # (B,)\n",
        "\n",
        "        # Initialize hidden and cell states\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            h = hidden  # (num_layers, B, H)\n",
        "            c = cell\n",
        "        else:\n",
        "            h = hidden\n",
        "            c = None\n",
        "\n",
        "        for t in range(1, T):\n",
        "            emb_t = self.embed(input_step)  # (B, E)\n",
        "\n",
        "            # 1) Compute context vector via attention:\n",
        "            context, attn_weights = self.attention(h, encoder_outputs)  # context: (B, H)\n",
        "\n",
        "            # 2) Concatenate embedding and context vector, then feed to RNN:\n",
        "            rnn_input = torch.cat([emb_t, context], dim=1).unsqueeze(1)  # (B, 1, E+H)\n",
        "\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                out_t, (h, c) = self.rnn(rnn_input, (h, c))  # out_t: (B, 1, H)\n",
        "            else:\n",
        "                out_t, h = self.rnn(rnn_input, h)  # out_t: (B, 1, H)\n",
        "                c = None\n",
        "\n",
        "            logits = self.out(out_t.squeeze(1))  # (B, V)\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Decide next input (teacher forcing or predicted token)\n",
        "            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "            top1 = logits.argmax(dim=1)  # (B,)\n",
        "            next_input = tgt_seq[:, t] if teacher_force else top1\n",
        "            input_step = next_input\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_decode(\n",
        "        self,\n",
        "        encoder_hidden,\n",
        "        encoder_cell,\n",
        "        encoder_outputs,\n",
        "        max_len,\n",
        "        dev_sos_idx,\n",
        "        dev_eos_idx,\n",
        "        beam_size\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simplified batch_size=1 beam search using attention.\n",
        "        encoder_hidden: hidden state to initialize decoder\n",
        "        encoder_cell:   cell state for LSTM decoder (None if not LSTM)\n",
        "        encoder_outputs: (1, T_src, H)\n",
        "        \"\"\"\n",
        "        hidden, cell = encoder_hidden, encoder_cell\n",
        "        live = [([dev_sos_idx], hidden, cell, 0.0)]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_hyps = []\n",
        "            for (seq, h, c, score) in live:\n",
        "                last_token = seq[-1]\n",
        "                if last_token == dev_eos_idx:\n",
        "                    completed.append((seq, h, c, score))\n",
        "                    continue\n",
        "\n",
        "                inp = torch.tensor([last_token], dtype=torch.long, device=encoder_outputs.device).unsqueeze(0)\n",
        "                emb_t = self.embed(inp).squeeze(1)  # (1, E)\n",
        "\n",
        "                # Compute attention‐based context for this step:\n",
        "                context, attn_weights = self.attention(h, encoder_outputs)  # (1, H)\n",
        "\n",
        "                rnn_input = torch.cat([emb_t, context], dim=1).unsqueeze(1)  # (1, 1, E+H)\n",
        "\n",
        "                if self.cell_type == \"LSTM\":\n",
        "                    out_t, (h2, c2) = self.rnn(rnn_input, (h, c))  # (1, 1, H)\n",
        "                else:\n",
        "                    out_t, h2 = self.rnn(rnn_input, h)\n",
        "                    c2 = None\n",
        "\n",
        "                logits = self.out(out_t.squeeze(1))  # (1, V)\n",
        "                log_probs = F.log_softmax(logits, dim=1)  # (1, V)\n",
        "\n",
        "                topk_logprobs, topk_indices = torch.topk(log_probs, k=beam_size, dim=1)\n",
        "                topk_logprobs = topk_logprobs.squeeze(0).tolist()\n",
        "                topk_indices = topk_indices.squeeze(0).tolist()\n",
        "\n",
        "                for lp, idx in zip(topk_logprobs, topk_indices):\n",
        "                    new_hyps.append((seq + [idx], h2, c2, score + lp))\n",
        "\n",
        "            # Retain only top k hypotheses\n",
        "            new_hyps = sorted(new_hyps, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "            live = new_hyps\n",
        "\n",
        "            # If all live hypotheses have already ended with <eos>, stop\n",
        "            if all(hyp[0][-1] == dev_eos_idx for hyp in live):\n",
        "                completed.extend(live)\n",
        "                break\n",
        "\n",
        "        if not completed:\n",
        "            completed = live\n",
        "\n",
        "        best = max(completed, key=lambda x: x[3])\n",
        "        return best[0]\n",
        "\n",
        "# ─── Seq2Seq ────────────────────────────────────────────────────────────────────\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        src: (batch_size, T_src)\n",
        "        tgt: (batch_size, T_tgt)      # includes <sos> ... <eos>\n",
        "        Returns: logits over vocabulary at each decoder timestep\n",
        "                 shape = (batch_size, T_tgt, V_out)\n",
        "        \"\"\"\n",
        "        B, T_src = src.size()\n",
        "        outputs = torch.zeros(B, tgt.size(1), self.decoder.out.out_features, device=self.device)\n",
        "\n",
        "        # 1) Run the encoder\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            enc_outputs, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            enc_outputs, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # 2) Prepare initial decoder hidden & cell states from encoder’s final states\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = None\n",
        "\n",
        "        # 3) Decode with attention\n",
        "        logits = self.decoder(\n",
        "            tgt_seq=tgt,\n",
        "            hidden=dec_hidden,\n",
        "            cell=dec_cell,\n",
        "            encoder_outputs=enc_outputs,\n",
        "            teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "        )\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src: torch.Tensor, max_len: int, dev_sos_idx: int, dev_eos_idx: int, beam_size: int = 1):\n",
        "        \"\"\"\n",
        "        Run inference (batch_size=1), either greedily (beam_size=1) or with beam search.\n",
        "        Returns a list of token indices (the decoded sequence).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        B = 1\n",
        "\n",
        "        # 1) Encode the source\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            enc_outputs, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            enc_outputs, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # Prepare decoder initial states\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = None\n",
        "\n",
        "        if beam_size == 1:\n",
        "            # GREEDY DECODING\n",
        "            seq = [dev_sos_idx]\n",
        "            hidden_, cell_ = hidden_state, cell_state\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                last_token = torch.tensor([seq[-1]], dtype=torch.long, device=self.device).unsqueeze(0)  # (1,1)\n",
        "                emb = self.decoder.embed(last_token).squeeze(1)  # (1, E)\n",
        "\n",
        "                # Compute attention context\n",
        "                context, attn_weights = self.decoder.attention(hidden_, enc_outputs)  # (1, H)\n",
        "\n",
        "                rnn_input = torch.cat([emb, context], dim=1).unsqueeze(1)  # (1, 1, E+H)\n",
        "\n",
        "                if self.decoder.cell_type == \"LSTM\":\n",
        "                    out, (h_next, c_next) = self.decoder.rnn(rnn_input, (hidden_, cell_))\n",
        "                    hidden_, cell_ = h_next, c_next\n",
        "                else:\n",
        "                    out, h_next = self.decoder.rnn(rnn_input, hidden_)\n",
        "                    hidden_, cell_ = h_next, None\n",
        "\n",
        "                logits = self.decoder.out(out.squeeze(1))  # (1, V)\n",
        "                next_token = logits.argmax(dim=1).item()\n",
        "                seq.append(next_token)\n",
        "                if next_token == dev_eos_idx:\n",
        "                    break\n",
        "\n",
        "            return seq\n",
        "        else:\n",
        "            # BEAM SEARCH DECODING\n",
        "            best_seq = self.decoder.beam_search_decode(\n",
        "                encoder_hidden=hidden_state,\n",
        "                encoder_cell=cell_state,\n",
        "                encoder_outputs=enc_outputs,\n",
        "                max_len=max_len,\n",
        "                dev_sos_idx=dev_sos_idx,\n",
        "                dev_eos_idx=dev_eos_idx,\n",
        "                beam_size=beam_size,\n",
        "            )\n",
        "            return best_seq\n",
        "\n",
        "# ─── train / evaluate ───────────────────────────────────────────────────────\n",
        "def char_accuracy(\n",
        "    logits: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int\n",
        ") -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Compute (num_correct_chars, num_valid_chars) ignoring <pad>, <sos>, and <eos>.\n",
        "    logits: (B, T, V)    – raw decoder logits\n",
        "    target: (B, T)       – ground‐truth indices (including <sos>, <eos>, <pad>)\n",
        "    pad_idx, sos_idx, eos_idx: indices for the special tokens\n",
        "\n",
        "    Returns:\n",
        "       (correct_count, valid_count)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        B, T, V = logits.size()\n",
        "        pred = logits.argmax(dim=2)   # (B, T)\n",
        "\n",
        "        # Mask out positions where target is pad, sos, or eos\n",
        "        ignore_mask = (\n",
        "            (target != pad_idx) &\n",
        "            (target != sos_idx) &\n",
        "            (target != eos_idx)\n",
        "        )  # (B, T)\n",
        "\n",
        "        correct = ((pred == target) & ignore_mask).sum().item()\n",
        "        valid   = ignore_mask.sum().item()\n",
        "        return correct, valid\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int,\n",
        "    device: torch.device,\n",
        "    teacher_forcing_ratio: float,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Runs one epoch of training, returning:\n",
        "      (train_loss, train_char_acc, train_word_acc)\n",
        "\n",
        "    – train_char_acc is computed as (total_correct_chars / total_valid_chars)\n",
        "    – train_word_acc is (num_exact_word_matches / total_words)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # For micro‐average char‐accuracy:\n",
        "    total_correct_chars = 0\n",
        "    total_valid_chars   = 0\n",
        "\n",
        "    # For word‐accuracy (we’ll count how many sequences are exactly correct)\n",
        "    total_exact_words = 0\n",
        "    total_words       = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(device, non_blocking=True)  # (B, T_src)\n",
        "        tgt = tgt.to(device, non_blocking=True)  # (B, T_tgt)\n",
        "        B, T_tgt = tgt.size()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output_logits = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        # output_logits: (B, T_tgt, V_out)\n",
        "\n",
        "        # 1) Compute token‐level (character) loss\n",
        "        V = output_logits.size(-1)\n",
        "        loss = criterion(output_logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "        # 2) Compute char‐level correct/valid counts (ignore special tokens)\n",
        "        correct, valid = char_accuracy(\n",
        "            output_logits, tgt,\n",
        "            pad_idx, sos_idx, eos_idx\n",
        "        )\n",
        "        total_correct_chars += correct\n",
        "        total_valid_chars   += valid\n",
        "\n",
        "        # 3) Compute word‐level accuracy for this batch\n",
        "        #    We already have output_logits. Let’s get preds:\n",
        "        with torch.no_grad():\n",
        "            pred_inds = output_logits.argmax(dim=2)  # (B, T_tgt)\n",
        "\n",
        "            # For each example, ignore index 0 (<sos>). Then check that\n",
        "            #   for every position t where tgt[b,t] != pad_idx,\n",
        "            #   pred_inds[b,t] == tgt[b,t].\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)  # which positions to verify\n",
        "\n",
        "            # A word is correct if for all non‐pad positions, pred == tgt\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)  # (B,)\n",
        "            total_exact_words += exact_matches.sum().item()\n",
        "            total_words       += B\n",
        "\n",
        "        # 4) Backward + step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Final epoch‐level metrics:\n",
        "    train_loss = epoch_loss / len(iterator)\n",
        "    train_char_acc = total_correct_chars / max(total_valid_chars, 1)\n",
        "    train_word_acc = total_exact_words   / max(total_words, 1)\n",
        "\n",
        "    return train_loss, train_char_acc, train_word_acc\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int,\n",
        "    device: torch.device,\n",
        "    beam_size: int,\n",
        "    max_dev_len: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    One validation pass (no teacher forcing). Returns:\n",
        "      (val_loss, val_char_acc, val_word_acc)\n",
        "\n",
        "    – val_char_acc is (total_correct_chars / total_valid_chars)\n",
        "    – val_word_acc is (num_exact_word_matches / total_words)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    total_correct_chars = 0\n",
        "    total_valid_chars   = 0\n",
        "    total_exact_words   = 0\n",
        "    total_words         = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "            src = src.to(device, non_blocking=True)\n",
        "            tgt = tgt.to(device, non_blocking=True)   # (B, T_tgt)\n",
        "            B, T_tgt = tgt.size()\n",
        "\n",
        "            # 1) Forward pass (teacher_forcing_ratio=0.0)\n",
        "            logits = model(src, tgt, teacher_forcing_ratio=0.0)  # (B, T_tgt, V_out)\n",
        "            V = logits.size(-1)\n",
        "            loss = criterion(logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "            # 2) Character‐level correct/valid counts (ignore special tokens)\n",
        "            correct, valid = char_accuracy(\n",
        "                logits, tgt,\n",
        "                pad_idx, sos_idx, eos_idx\n",
        "            )\n",
        "            total_correct_chars += correct\n",
        "            total_valid_chars   += valid\n",
        "\n",
        "            # 3) Word‐level accuracy (greedy decode)\n",
        "            pred_inds = logits.argmax(dim=2)  # (B, T_tgt)\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)\n",
        "\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)  # (B,)\n",
        "            total_exact_words += exact_matches.sum().item()\n",
        "            total_words       += B\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    val_loss = epoch_loss / len(iterator)\n",
        "    val_char_acc = total_correct_chars / max(total_valid_chars, 1)\n",
        "    val_word_acc = total_exact_words / max(total_words, 1)\n",
        "\n",
        "    return val_loss, val_char_acc, val_word_acc\n",
        "\n",
        "\n",
        "# ─── main() with parse_known_args ─────────────────────────────────────────────\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content\")\n",
        "    parser.add_argument(\"--train_file\", type=str, default=\"bn.translit.sampled.train.tsv\")\n",
        "    parser.add_argument(\"--dev_file\", type=str, default=\"bn.translit.sampled.dev.tsv\")\n",
        "    parser.add_argument(\"--test_file\", type=str, default=\"bn.translit.sampled.test.tsv\")\n",
        "    parser.add_argument(\"--emb_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--enc_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--dec_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--cell_type\", type=str, default=\"RNN\", choices=[\"RNN\", \"GRU\", \"LSTM\"])\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--tf_ratio\", type=float, default=0.5)\n",
        "    parser.add_argument(\"--beam_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--max_dev_len\", type=int, default=32)\n",
        "    parser.add_argument(\"--project_name\", type=str, default=\"Attention_RNN\")\n",
        "    parser.add_argument(\"--run_name\", type=str, default=None)\n",
        "\n",
        "    # ── use parse_known_args to ignore Colab’s extra \"-f …json\"\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_path = os.path.join(args.data_dir, args.train_file)\n",
        "    dev_path   = os.path.join(args.data_dir, args.dev_file)\n",
        "    test_path  = os.path.join(args.data_dir, args.test_file)\n",
        "    vocab = CharVocab([train_path, dev_path, test_path])\n",
        "\n",
        "    # Extract special‐token indices for convenience\n",
        "    pad_idx = vocab.dev_pad_idx\n",
        "    sos_idx = vocab.dev_sos_idx\n",
        "    eos_idx = vocab.dev_eos_idx\n",
        "\n",
        "    train_ds = TransliterationDataset(train_path, vocab)\n",
        "    dev_ds   = TransliterationDataset(dev_path, vocab)\n",
        "    test_ds  = TransliterationDataset(test_path, vocab)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        dev_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    if args.run_name is None:\n",
        "        args.run_name = (\n",
        "            f\"{args.cell_type}\"\n",
        "            f\"_emb{args.emb_size}\"\n",
        "            f\"_hid{args.hidden_size}\"\n",
        "            f\"_layers{args.enc_layers}x{args.dec_layers}\"\n",
        "            f\"_lr{args.lr}\"\n",
        "            f\"_tf{args.tf_ratio}\"\n",
        "        )\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.project_name,\n",
        "        name=args.run_name,\n",
        "        config={\n",
        "            \"emb_size\": args.emb_size,\n",
        "            \"hidden_size\": args.hidden_size,\n",
        "            \"enc_layers\": args.enc_layers,\n",
        "            \"dec_layers\": args.dec_layers,\n",
        "            \"cell_type\": args.cell_type,\n",
        "            \"dropout\": args.dropout,\n",
        "            \"lr\": args.lr,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"epochs\": args.epochs,\n",
        "            \"tf_ratio\": args.tf_ratio,\n",
        "            \"beam_size\": args.beam_size,\n",
        "        },\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_vocab_size=vocab.rom_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.enc_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    decoder = Decoder(\n",
        "        output_vocab_size=vocab.dev_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.dec_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.dev_pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-5)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1) Train\n",
        "        train_loss, train_char_acc, train_word_acc = train_one_epoch(\n",
        "            model=model,\n",
        "            iterator=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            pad_idx=pad_idx,\n",
        "            sos_idx=sos_idx,\n",
        "            eos_idx=eos_idx,\n",
        "            device=device,\n",
        "            teacher_forcing_ratio=config.tf_ratio,\n",
        "        )\n",
        "\n",
        "        # 2) Evaluate\n",
        "        val_loss, val_char_acc, val_word_acc = evaluate(\n",
        "            model=model,\n",
        "            iterator=dev_loader,\n",
        "            criterion=criterion,\n",
        "            pad_idx=pad_idx,\n",
        "            sos_idx=sos_idx,\n",
        "            eos_idx=eos_idx,\n",
        "            device=device,\n",
        "            beam_size=config.beam_size,\n",
        "            max_dev_len=args.max_dev_len,\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # 3) Save best model based on val_char_acc\n",
        "        if val_char_acc > best_val_acc:\n",
        "            best_val_acc = val_char_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "        # 4) Log to W&B\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_char_acc\": train_char_acc,\n",
        "                \"train_word_acc\": train_word_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_char_acc\": val_char_acc,\n",
        "                \"val_word_acc\": val_word_acc,\n",
        "                \"epoch_time_sec\": elapsed,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 5) Print progress\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, \"\n",
        "            f\"Train Char Acc: {train_char_acc:.4f}, \"\n",
        "            f\"Train Word Acc: {train_word_acc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Char Acc: {val_char_acc:.4f}, \"\n",
        "            f\"Val Word Acc: {val_word_acc:.4f} | \"\n",
        "            f\"Time: {elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IHjg2NT0si4x"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > sweep.yaml << 'EOF'\n",
        "program: main.py\n",
        "method: bayes\n",
        "project: Attention_RNN\n",
        "entity: mrsagarbiswas-iit-madras\n",
        "metric:\n",
        "  name: val_char_acc\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  emb_size:\n",
        "    values: [128, 256]\n",
        "  hidden_size:\n",
        "    values: [128, 256]\n",
        "  enc_layers:\n",
        "    values: [3, 5]\n",
        "  epochs:\n",
        "    values: [15]\n",
        "  dec_layers:\n",
        "    values: [1, 2, 3]\n",
        "  cell_type:\n",
        "    values: [\"RNN\", \"GRU\", \"LSTM\"]\n",
        "  dropout:\n",
        "    values: [0.2, 0.3]\n",
        "  lr:\n",
        "    values: [1e-3, 1e-4]\n",
        "  batch_size:\n",
        "    values: [32, 64, 128]\n",
        "  tf_ratio:\n",
        "    values: [0.3, 0.5, 0.7]\n",
        "  beam_size:\n",
        "    values: [3, 5]\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuSvKgqQs66p",
        "outputId": "976bfa1a-68e0-4c4a-ff7f-bba93d9961d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sweep ID = \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Currently logged in as: mrsagarbiswas (mrsagarbiswas-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Creating sweep from: sweep.yaml\n",
            "wandb: Creating sweep with ID: 7hka6agi\n",
            "wandb: View sweep at: https://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\n",
            "wandb: Run sweep agent with: wandb agent mrsagarbiswas-iit-madras/Attention_RNN/7hka6agi\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export WANDB_API_KEY=\"Your-API-Key\"\n",
        "wandb login\n",
        "SWEEP_ID=$(wandb sweep sweep.yaml)\n",
        "echo \"Sweep ID = $SWEEP_ID\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc33AXMhyJpg",
        "outputId": "19786939-2b7e-4fc0-cd49-3ad75c4c51fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Login to W&B to use the sweep agent feature\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2025-05-19 19:30:19,723 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2025-05-19 19:30:19,969 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 19:30:19,969 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 3\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.2\n",
            "\temb_size: 128\n",
            "\tenc_layers: 3\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 19:30:19,972 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=3 --cell_type=GRU --dec_layers=1 --dropout=0.2 --emb_size=128 --enc_layers=3 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "2025-05-19 19:30:24,983 - wandb.wandb_agent - INFO - Running runs: ['xla6piyx']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_193027-xla6piyx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb128_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/xla6piyx\u001b[0m\n",
            "Epoch 01 | Train Loss: 2.5510, Train Char Acc: 0.3047, Train Word Acc: 0.0162 | Val Loss: 1.8427, Val Char Acc: 0.5183, Val Word Acc: 0.0746 | Time: 64.9s\n",
            "Epoch 02 | Train Loss: 1.5506, Train Char Acc: 0.6094, Train Word Acc: 0.1254 | Val Loss: 1.5907, Val Char Acc: 0.6005, Val Word Acc: 0.1555 | Time: 62.9s\n",
            "Epoch 03 | Train Loss: 1.3576, Train Char Acc: 0.6703, Train Word Acc: 0.1899 | Val Loss: 1.5086, Val Char Acc: 0.6177, Val Word Acc: 0.1965 | Time: 62.7s\n",
            "Epoch 04 | Train Loss: 1.2488, Train Char Acc: 0.7078, Train Word Acc: 0.2335 | Val Loss: 1.4696, Val Char Acc: 0.6337, Val Word Acc: 0.2235 | Time: 63.2s\n",
            "Epoch 05 | Train Loss: 1.1772, Train Char Acc: 0.7334, Train Word Acc: 0.2685 | Val Loss: 1.4714, Val Char Acc: 0.6328, Val Word Acc: 0.2354 | Time: 63.3s\n",
            "Epoch 06 | Train Loss: 1.1210, Train Char Acc: 0.7541, Train Word Acc: 0.3000 | Val Loss: 1.4202, Val Char Acc: 0.6669, Val Word Acc: 0.2561 | Time: 63.6s\n",
            "Epoch 07 | Train Loss: 1.0753, Train Char Acc: 0.7713, Train Word Acc: 0.3274 | Val Loss: 1.4173, Val Char Acc: 0.6586, Val Word Acc: 0.2624 | Time: 62.6s\n",
            "Epoch 08 | Train Loss: 1.0405, Train Char Acc: 0.7839, Train Word Acc: 0.3506 | Val Loss: 1.3918, Val Char Acc: 0.6763, Val Word Acc: 0.2788 | Time: 63.0s\n",
            "Epoch 09 | Train Loss: 1.0077, Train Char Acc: 0.7963, Train Word Acc: 0.3735 | Val Loss: 1.3673, Val Char Acc: 0.6886, Val Word Acc: 0.2922 | Time: 62.8s\n",
            "Epoch 10 | Train Loss: 0.9786, Train Char Acc: 0.8072, Train Word Acc: 0.3959 | Val Loss: 1.3541, Val Char Acc: 0.7010, Val Word Acc: 0.2993 | Time: 63.0s\n",
            "Epoch 11 | Train Loss: 0.9532, Train Char Acc: 0.8170, Train Word Acc: 0.4148 | Val Loss: 1.3687, Val Char Acc: 0.6871, Val Word Acc: 0.3020 | Time: 63.8s\n",
            "Epoch 12 | Train Loss: 0.9298, Train Char Acc: 0.8261, Train Word Acc: 0.4346 | Val Loss: 1.3473, Val Char Acc: 0.7005, Val Word Acc: 0.3125 | Time: 62.9s\n",
            "Epoch 13 | Train Loss: 0.9063, Train Char Acc: 0.8347, Train Word Acc: 0.4515 | Val Loss: 1.3381, Val Char Acc: 0.7086, Val Word Acc: 0.3184 | Time: 62.9s\n",
            "Epoch 14 | Train Loss: 0.8826, Train Char Acc: 0.8434, Train Word Acc: 0.4675 | Val Loss: 1.3428, Val Char Acc: 0.7091, Val Word Acc: 0.3224 | Time: 62.8s\n",
            "Epoch 15 | Train Loss: 0.8641, Train Char Acc: 0.8502, Train Word Acc: 0.4847 | Val Loss: 1.3379, Val Char Acc: 0.7142, Val Word Acc: 0.3257 | Time: 62.9s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec █▂▁▃▃▄▁▂▂▂▅▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▅▆▆▇▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▃▄▄▅▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▅▅▆▆▇▇█▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▃▃▃▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▃▄▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 62.94349\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.85018\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.86407\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.48473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.71419\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.33786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.32568\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb128_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/xla6piyx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_193027-xla6piyx/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 19:46:28,839 - wandb.wandb_agent - INFO - Cleaning up finished run: xla6piyx\n",
            "2025-05-19 19:46:30,126 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 19:46:30,126 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.2\n",
            "\temb_size: 256\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 128\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 19:46:30,128 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=5 --cell_type=LSTM --dec_layers=3 --dropout=0.2 --emb_size=256 --enc_layers=5 --epochs=15 --hidden_size=128 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_194634-43z9d2zl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid128_layers5x3_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/43z9d2zl\u001b[0m\n",
            "2025-05-19 19:46:35,145 - wandb.wandb_agent - INFO - Running runs: ['43z9d2zl']\n",
            "Epoch 01 | Train Loss: 3.1051, Train Char Acc: 0.1224, Train Word Acc: 0.0000 | Val Loss: 2.9911, Val Char Acc: 0.1416, Val Word Acc: 0.0000 | Time: 143.9s\n",
            "Epoch 02 | Train Loss: 2.7395, Train Char Acc: 0.2332, Train Word Acc: 0.0005 | Val Loss: 2.5891, Val Char Acc: 0.2679, Val Word Acc: 0.0012 | Time: 144.1s\n",
            "Epoch 03 | Train Loss: 2.3335, Train Char Acc: 0.3422, Train Word Acc: 0.0060 | Val Loss: 2.2216, Val Char Acc: 0.3625, Val Word Acc: 0.0119 | Time: 144.4s\n",
            "Epoch 04 | Train Loss: 2.0381, Train Char Acc: 0.4278, Train Word Acc: 0.0208 | Val Loss: 1.9988, Val Char Acc: 0.4275, Val Word Acc: 0.0331 | Time: 143.8s\n",
            "Epoch 05 | Train Loss: 1.8339, Train Char Acc: 0.4879, Train Word Acc: 0.0430 | Val Loss: 1.8442, Val Char Acc: 0.4784, Val Word Acc: 0.0565 | Time: 145.0s\n",
            "Epoch 06 | Train Loss: 1.6954, Train Char Acc: 0.5330, Train Word Acc: 0.0662 | Val Loss: 1.7561, Val Char Acc: 0.5092, Val Word Acc: 0.0855 | Time: 143.1s\n",
            "Epoch 07 | Train Loss: 1.5888, Train Char Acc: 0.5703, Train Word Acc: 0.0917 | Val Loss: 1.6758, Val Char Acc: 0.5437, Val Word Acc: 0.1181 | Time: 145.1s\n",
            "Epoch 08 | Train Loss: 1.5060, Train Char Acc: 0.6009, Train Word Acc: 0.1142 | Val Loss: 1.6379, Val Char Acc: 0.5628, Val Word Acc: 0.1329 | Time: 143.7s\n",
            "Epoch 09 | Train Loss: 1.4338, Train Char Acc: 0.6290, Train Word Acc: 0.1389 | Val Loss: 1.5863, Val Char Acc: 0.5795, Val Word Acc: 0.1580 | Time: 145.9s\n",
            "Epoch 10 | Train Loss: 1.3773, Train Char Acc: 0.6500, Train Word Acc: 0.1597 | Val Loss: 1.5530, Val Char Acc: 0.5958, Val Word Acc: 0.1704 | Time: 144.6s\n",
            "Epoch 11 | Train Loss: 1.3254, Train Char Acc: 0.6703, Train Word Acc: 0.1831 | Val Loss: 1.5545, Val Char Acc: 0.5991, Val Word Acc: 0.1842 | Time: 144.8s\n",
            "Epoch 12 | Train Loss: 1.2806, Train Char Acc: 0.6880, Train Word Acc: 0.2042 | Val Loss: 1.5270, Val Char Acc: 0.6200, Val Word Acc: 0.1987 | Time: 167.9s\n",
            "Epoch 13 | Train Loss: 1.2404, Train Char Acc: 0.7037, Train Word Acc: 0.2232 | Val Loss: 1.5069, Val Char Acc: 0.6201, Val Word Acc: 0.2010 | Time: 144.9s\n",
            "Epoch 14 | Train Loss: 1.2049, Train Char Acc: 0.7182, Train Word Acc: 0.2444 | Val Loss: 1.4800, Val Char Acc: 0.6297, Val Word Acc: 0.2140 | Time: 145.1s\n",
            "Epoch 15 | Train Loss: 1.1715, Train Char Acc: 0.7305, Train Word Acc: 0.2627 | Val Loss: 1.4841, Val Char Acc: 0.6417, Val Word Acc: 0.2241 | Time: 147.6s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.004 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▁▁▁▁▂▁▂▁▂▁▁█▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▄▅▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▅▄▃▃▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▂▃▃▄▅▅▆▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▄▅▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▄▃▃▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▂▃▄▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 147.6399\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.7305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.17154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.26274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.64174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.48414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.22405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid128_layers5x3_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/43z9d2zl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_194634-43z9d2zl/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 20:23:21,028 - wandb.wandb_agent - INFO - Cleaning up finished run: 43z9d2zl\n",
            "2025-05-19 20:23:21,453 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 20:23:21,454 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 128\n",
            "\tenc_layers: 3\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 20:23:21,455 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=3 --cell_type=LSTM --dec_layers=2 --dropout=0.2 --emb_size=128 --enc_layers=3 --epochs=15 --hidden_size=256 --lr=0.001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "2025-05-19 20:23:26,465 - wandb.wandb_agent - INFO - Running runs: ['aepnu3u7']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_202326-aepnu3u7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb128_hid256_layers3x2_lr0.001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/aepnu3u7\u001b[0m\n",
            "Epoch 01 | Train Loss: 2.1995, Train Char Acc: 0.4033, Train Word Acc: 0.0510 | Val Loss: 1.7270, Val Char Acc: 0.5727, Val Word Acc: 0.1629 | Time: 75.6s\n",
            "Epoch 02 | Train Loss: 1.1065, Train Char Acc: 0.7650, Train Word Acc: 0.2935 | Val Loss: 1.5854, Val Char Acc: 0.6509, Val Word Acc: 0.2629 | Time: 75.0s\n",
            "Epoch 03 | Train Loss: 0.8963, Train Char Acc: 0.8433, Train Word Acc: 0.4441 | Val Loss: 1.5147, Val Char Acc: 0.6845, Val Word Acc: 0.3038 | Time: 76.4s\n",
            "Epoch 04 | Train Loss: 0.8025, Train Char Acc: 0.8772, Train Word Acc: 0.5323 | Val Loss: 1.5629, Val Char Acc: 0.6853, Val Word Acc: 0.3102 | Time: 74.6s\n",
            "Epoch 05 | Train Loss: 0.7412, Train Char Acc: 0.8992, Train Word Acc: 0.5945 | Val Loss: 1.5448, Val Char Acc: 0.6990, Val Word Acc: 0.3380 | Time: 75.7s\n",
            "Epoch 06 | Train Loss: 0.7018, Train Char Acc: 0.9126, Train Word Acc: 0.6368 | Val Loss: 1.6589, Val Char Acc: 0.6889, Val Word Acc: 0.3206 | Time: 74.2s\n",
            "Epoch 07 | Train Loss: 0.6732, Train Char Acc: 0.9230, Train Word Acc: 0.6700 | Val Loss: 1.5841, Val Char Acc: 0.7022, Val Word Acc: 0.3259 | Time: 75.1s\n",
            "Epoch 08 | Train Loss: 0.6559, Train Char Acc: 0.9286, Train Word Acc: 0.6902 | Val Loss: 1.6341, Val Char Acc: 0.7016, Val Word Acc: 0.3324 | Time: 74.4s\n",
            "Epoch 09 | Train Loss: 0.6363, Train Char Acc: 0.9354, Train Word Acc: 0.7127 | Val Loss: 1.6646, Val Char Acc: 0.6936, Val Word Acc: 0.3192 | Time: 74.3s\n",
            "Epoch 10 | Train Loss: 0.6237, Train Char Acc: 0.9395, Train Word Acc: 0.7275 | Val Loss: 1.6689, Val Char Acc: 0.6976, Val Word Acc: 0.3193 | Time: 74.2s\n",
            "Epoch 11 | Train Loss: 0.6150, Train Char Acc: 0.9424, Train Word Acc: 0.7380 | Val Loss: 1.6927, Val Char Acc: 0.7052, Val Word Acc: 0.3304 | Time: 76.6s\n",
            "Epoch 12 | Train Loss: 0.6079, Train Char Acc: 0.9453, Train Word Acc: 0.7494 | Val Loss: 1.7171, Val Char Acc: 0.6959, Val Word Acc: 0.3213 | Time: 74.2s\n",
            "Epoch 13 | Train Loss: 0.6008, Train Char Acc: 0.9475, Train Word Acc: 0.7570 | Val Loss: 1.7686, Val Char Acc: 0.6899, Val Word Acc: 0.3163 | Time: 74.3s\n",
            "Epoch 14 | Train Loss: 0.5949, Train Char Acc: 0.9497, Train Word Acc: 0.7655 | Val Loss: 1.7506, Val Char Acc: 0.6909, Val Word Acc: 0.3109 | Time: 74.7s\n",
            "Epoch 15 | Train Loss: 0.5894, Train Char Acc: 0.9515, Train Word Acc: 0.7719 | Val Loss: 1.7511, Val Char Acc: 0.6915, Val Word Acc: 0.3171 | Time: 75.2s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▅▄█▂▅▁▄▂▂▁█▁▁▃▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▆▇▇▇██████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▃▅▆▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▅▇▇█▇██▇███▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇▃▁▂▂▅▃▄▅▅▆▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▅▇▇█▇██▇▇█▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 75.18497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.95147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.58944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.77191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.69155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.75107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.31706\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb128_hid256_layers3x2_lr0.001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/aepnu3u7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_202326-aepnu3u7/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 20:42:22,837 - wandb.wandb_agent - INFO - Cleaning up finished run: aepnu3u7\n",
            "2025-05-19 20:42:23,188 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 20:42:23,188 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 256\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 20:42:23,190 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=GRU --dec_layers=2 --dropout=0.2 --emb_size=256 --enc_layers=5 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_204227-qbh5deff\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb256_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/qbh5deff\u001b[0m\n",
            "2025-05-19 20:42:28,200 - wandb.wandb_agent - INFO - Running runs: ['qbh5deff']\n",
            "Epoch 01 | Train Loss: 3.0838, Train Char Acc: 0.1318, Train Word Acc: 0.0000 | Val Loss: 3.1052, Val Char Acc: 0.1285, Val Word Acc: 0.0000 | Time: 41.9s\n",
            "Epoch 02 | Train Loss: 2.7733, Train Char Acc: 0.2185, Train Word Acc: 0.0003 | Val Loss: 2.8142, Val Char Acc: 0.2144, Val Word Acc: 0.0005 | Time: 42.2s\n",
            "Epoch 03 | Train Loss: 2.3977, Train Char Acc: 0.3388, Train Word Acc: 0.0029 | Val Loss: 2.5026, Val Char Acc: 0.3367, Val Word Acc: 0.0091 | Time: 41.4s\n",
            "Epoch 04 | Train Loss: 1.9672, Train Char Acc: 0.4741, Train Word Acc: 0.0235 | Val Loss: 2.0691, Val Char Acc: 0.4600, Val Word Acc: 0.0512 | Time: 42.3s\n",
            "Epoch 05 | Train Loss: 1.6095, Train Char Acc: 0.5987, Train Word Acc: 0.0853 | Val Loss: 1.8392, Val Char Acc: 0.5542, Val Word Acc: 0.1302 | Time: 42.4s\n",
            "Epoch 06 | Train Loss: 1.4032, Train Char Acc: 0.6716, Train Word Acc: 0.1498 | Val Loss: 1.7536, Val Char Acc: 0.5905, Val Word Acc: 0.1701 | Time: 41.6s\n",
            "Epoch 07 | Train Loss: 1.2829, Train Char Acc: 0.7139, Train Word Acc: 0.2011 | Val Loss: 1.6727, Val Char Acc: 0.6180, Val Word Acc: 0.2048 | Time: 42.1s\n",
            "Epoch 08 | Train Loss: 1.2028, Train Char Acc: 0.7396, Train Word Acc: 0.2358 | Val Loss: 1.6655, Val Char Acc: 0.6304, Val Word Acc: 0.2259 | Time: 41.5s\n",
            "Epoch 09 | Train Loss: 1.1350, Train Char Acc: 0.7629, Train Word Acc: 0.2709 | Val Loss: 1.5890, Val Char Acc: 0.6474, Val Word Acc: 0.2506 | Time: 42.3s\n",
            "Epoch 10 | Train Loss: 1.0903, Train Char Acc: 0.7781, Train Word Acc: 0.2971 | Val Loss: 1.5704, Val Char Acc: 0.6572, Val Word Acc: 0.2574 | Time: 41.5s\n",
            "Epoch 11 | Train Loss: 1.0475, Train Char Acc: 0.7922, Train Word Acc: 0.3261 | Val Loss: 1.5473, Val Char Acc: 0.6685, Val Word Acc: 0.2748 | Time: 41.8s\n",
            "Epoch 12 | Train Loss: 1.0149, Train Char Acc: 0.8034, Train Word Acc: 0.3467 | Val Loss: 1.5626, Val Char Acc: 0.6671, Val Word Acc: 0.2896 | Time: 42.6s\n",
            "Epoch 13 | Train Loss: 0.9788, Train Char Acc: 0.8156, Train Word Acc: 0.3693 | Val Loss: 1.5355, Val Char Acc: 0.6832, Val Word Acc: 0.3016 | Time: 41.6s\n",
            "Epoch 14 | Train Loss: 0.9549, Train Char Acc: 0.8233, Train Word Acc: 0.3868 | Val Loss: 1.5429, Val Char Acc: 0.6803, Val Word Acc: 0.3135 | Time: 42.4s\n",
            "Epoch 15 | Train Loss: 0.9290, Train Char Acc: 0.8327, Train Word Acc: 0.4080 | Val Loss: 1.5054, Val Char Acc: 0.6939, Val Word Acc: 0.3242 | Time: 41.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄▆▁▆▇▂▅▁▆▂▄█▂▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▃▄▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▄▃▃▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▂▄▄▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▄▅▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▃▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▂▄▅▅▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 41.48906\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.83268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.92905\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.40799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.6939\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.50545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.32417\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb256_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/qbh5deff\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_204227-qbh5deff/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 20:53:07,146 - wandb.wandb_agent - INFO - Cleaning up finished run: qbh5deff\n",
            "2025-05-19 20:53:07,499 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 20:53:07,499 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.3\n",
            "\temb_size: 256\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 20:53:07,501 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=LSTM --dec_layers=1 --dropout=0.3 --emb_size=256 --enc_layers=5 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_205311-sp5k6p0s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers5x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/sp5k6p0s\u001b[0m\n",
            "2025-05-19 20:53:12,507 - wandb.wandb_agent - INFO - Running runs: ['sp5k6p0s']\n",
            "Epoch 01 | Train Loss: 3.1531, Train Char Acc: 0.1166, Train Word Acc: 0.0000 | Val Loss: 3.0946, Val Char Acc: 0.1240, Val Word Acc: 0.0000 | Time: 40.5s\n",
            "Epoch 02 | Train Loss: 2.9370, Train Char Acc: 0.1669, Train Word Acc: 0.0000 | Val Loss: 2.9552, Val Char Acc: 0.1710, Val Word Acc: 0.0002 | Time: 42.0s\n",
            "Epoch 03 | Train Loss: 2.6901, Train Char Acc: 0.2480, Train Word Acc: 0.0006 | Val Loss: 2.6834, Val Char Acc: 0.2582, Val Word Acc: 0.0017 | Time: 41.2s\n",
            "Epoch 04 | Train Loss: 2.3533, Train Char Acc: 0.3476, Train Word Acc: 0.0064 | Val Loss: 2.3099, Val Char Acc: 0.3537, Val Word Acc: 0.0109 | Time: 41.0s\n",
            "Epoch 05 | Train Loss: 2.0493, Train Char Acc: 0.4367, Train Word Acc: 0.0220 | Val Loss: 2.0639, Val Char Acc: 0.4198, Val Word Acc: 0.0378 | Time: 40.6s\n",
            "Epoch 06 | Train Loss: 1.8225, Train Char Acc: 0.5068, Train Word Acc: 0.0495 | Val Loss: 1.8937, Val Char Acc: 0.4917, Val Word Acc: 0.0689 | Time: 39.8s\n",
            "Epoch 07 | Train Loss: 1.6419, Train Char Acc: 0.5686, Train Word Acc: 0.0863 | Val Loss: 1.7431, Val Char Acc: 0.5394, Val Word Acc: 0.1060 | Time: 40.9s\n",
            "Epoch 08 | Train Loss: 1.5174, Train Char Acc: 0.6129, Train Word Acc: 0.1235 | Val Loss: 1.6664, Val Char Acc: 0.5698, Val Word Acc: 0.1353 | Time: 40.1s\n",
            "Epoch 09 | Train Loss: 1.4228, Train Char Acc: 0.6471, Train Word Acc: 0.1566 | Val Loss: 1.6162, Val Char Acc: 0.5923, Val Word Acc: 0.1584 | Time: 40.0s\n",
            "Epoch 10 | Train Loss: 1.3486, Train Char Acc: 0.6743, Train Word Acc: 0.1886 | Val Loss: 1.6233, Val Char Acc: 0.6041, Val Word Acc: 0.1742 | Time: 41.3s\n",
            "Epoch 11 | Train Loss: 1.2943, Train Char Acc: 0.6946, Train Word Acc: 0.2132 | Val Loss: 1.5443, Val Char Acc: 0.6152, Val Word Acc: 0.1965 | Time: 40.2s\n",
            "Epoch 12 | Train Loss: 1.2378, Train Char Acc: 0.7157, Train Word Acc: 0.2406 | Val Loss: 1.5105, Val Char Acc: 0.6359, Val Word Acc: 0.2114 | Time: 40.2s\n",
            "Epoch 13 | Train Loss: 1.1974, Train Char Acc: 0.7302, Train Word Acc: 0.2644 | Val Loss: 1.4822, Val Char Acc: 0.6404, Val Word Acc: 0.2263 | Time: 40.2s\n",
            "Epoch 14 | Train Loss: 1.1585, Train Char Acc: 0.7445, Train Word Acc: 0.2872 | Val Loss: 1.4920, Val Char Acc: 0.6531, Val Word Acc: 0.2360 | Time: 40.7s\n",
            "Epoch 15 | Train Loss: 1.1189, Train Char Acc: 0.7595, Train Word Acc: 0.3093 | Val Loss: 1.4563, Val Char Acc: 0.6584, Val Word Acc: 0.2589 | Time: 40.3s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.005 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.005 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▃█▅▅▃▁▄▂▂▆▂▂▂▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▂▄▄▅▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▅▄▃▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▁▂▃▄▅▅▆▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▃▄▅▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▅▄▃▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▂▃▄▅▅▆▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 40.34299\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.75954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.11891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.30932\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.6584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.45627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.25886\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers5x1_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/sp5k6p0s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_205311-sp5k6p0s/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 21:03:31,557 - wandb.wandb_agent - INFO - Cleaning up finished run: sp5k6p0s\n",
            "2025-05-19 21:03:32,198 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 21:03:32,198 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 128\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 21:03:32,200 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=GRU --dec_layers=2 --dropout=0.2 --emb_size=128 --enc_layers=5 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_210336-ojfkfg5h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb128_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/ojfkfg5h\u001b[0m\n",
            "2025-05-19 21:03:37,210 - wandb.wandb_agent - INFO - Running runs: ['ojfkfg5h']\n",
            "Epoch 01 | Train Loss: 3.1096, Train Char Acc: 0.1243, Train Word Acc: 0.0000 | Val Loss: 3.1141, Val Char Acc: 0.1293, Val Word Acc: 0.0000 | Time: 43.6s\n",
            "Epoch 02 | Train Loss: 2.8048, Train Char Acc: 0.2097, Train Word Acc: 0.0003 | Val Loss: 2.8564, Val Char Acc: 0.2172, Val Word Acc: 0.0005 | Time: 42.1s\n",
            "Epoch 03 | Train Loss: 2.3715, Train Char Acc: 0.3492, Train Word Acc: 0.0040 | Val Loss: 2.3861, Val Char Acc: 0.3525, Val Word Acc: 0.0083 | Time: 42.5s\n",
            "Epoch 04 | Train Loss: 1.9312, Train Char Acc: 0.4928, Train Word Acc: 0.0290 | Val Loss: 2.0365, Val Char Acc: 0.4825, Val Word Acc: 0.0684 | Time: 41.7s\n",
            "Epoch 05 | Train Loss: 1.6033, Train Char Acc: 0.6052, Train Word Acc: 0.0860 | Val Loss: 1.8397, Val Char Acc: 0.5635, Val Word Acc: 0.1319 | Time: 42.3s\n",
            "Epoch 06 | Train Loss: 1.4221, Train Char Acc: 0.6680, Train Word Acc: 0.1433 | Val Loss: 1.7626, Val Char Acc: 0.5883, Val Word Acc: 0.1692 | Time: 41.9s\n",
            "Epoch 07 | Train Loss: 1.3118, Train Char Acc: 0.7046, Train Word Acc: 0.1862 | Val Loss: 1.6894, Val Char Acc: 0.6075, Val Word Acc: 0.2058 | Time: 41.8s\n",
            "Epoch 08 | Train Loss: 1.2351, Train Char Acc: 0.7292, Train Word Acc: 0.2205 | Val Loss: 1.6724, Val Char Acc: 0.6260, Val Word Acc: 0.2231 | Time: 43.4s\n",
            "Epoch 09 | Train Loss: 1.1763, Train Char Acc: 0.7486, Train Word Acc: 0.2498 | Val Loss: 1.6265, Val Char Acc: 0.6293, Val Word Acc: 0.2358 | Time: 41.8s\n",
            "Epoch 10 | Train Loss: 1.1250, Train Char Acc: 0.7651, Train Word Acc: 0.2745 | Val Loss: 1.5997, Val Char Acc: 0.6535, Val Word Acc: 0.2438 | Time: 42.4s\n",
            "Epoch 11 | Train Loss: 1.0853, Train Char Acc: 0.7782, Train Word Acc: 0.3002 | Val Loss: 1.5377, Val Char Acc: 0.6612, Val Word Acc: 0.2658 | Time: 41.8s\n",
            "Epoch 12 | Train Loss: 1.0463, Train Char Acc: 0.7914, Train Word Acc: 0.3216 | Val Loss: 1.5329, Val Char Acc: 0.6686, Val Word Acc: 0.2775 | Time: 42.4s\n",
            "Epoch 13 | Train Loss: 1.0132, Train Char Acc: 0.8017, Train Word Acc: 0.3416 | Val Loss: 1.5696, Val Char Acc: 0.6653, Val Word Acc: 0.2875 | Time: 41.8s\n",
            "Epoch 14 | Train Loss: 0.9839, Train Char Acc: 0.8115, Train Word Acc: 0.3600 | Val Loss: 1.5209, Val Char Acc: 0.6895, Val Word Acc: 0.2862 | Time: 43.3s\n",
            "Epoch 15 | Train Loss: 0.9625, Train Char Acc: 0.8186, Train Word Acc: 0.3783 | Val Loss: 1.5250, Val Char Acc: 0.6768, Val Word Acc: 0.3022 | Time: 41.8s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.004 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec █▂▄▁▃▂▁▇▁▄▁▄▁▇▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▃▅▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▄▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▃▄▄▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▄▅▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▃▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▃▄▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 41.82068\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.81856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.96251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.37829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.67684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.52497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.30219\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb128_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/ojfkfg5h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_210336-ojfkfg5h/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 21:14:21,546 - wandb.wandb_agent - INFO - Cleaning up finished run: ojfkfg5h\n",
            "2025-05-19 21:14:21,865 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 21:14:21,866 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 128\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 21:14:21,867 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=GRU --dec_layers=2 --dropout=0.2 --emb_size=128 --enc_layers=5 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_211426-bveu05pj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb128_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/bveu05pj\u001b[0m\n",
            "2025-05-19 21:14:26,877 - wandb.wandb_agent - INFO - Running runs: ['bveu05pj']\n",
            "Epoch 01 | Train Loss: 3.1096, Train Char Acc: 0.1243, Train Word Acc: 0.0000 | Val Loss: 3.1141, Val Char Acc: 0.1293, Val Word Acc: 0.0000 | Time: 42.7s\n",
            "Epoch 02 | Train Loss: 2.8048, Train Char Acc: 0.2097, Train Word Acc: 0.0003 | Val Loss: 2.8564, Val Char Acc: 0.2172, Val Word Acc: 0.0005 | Time: 41.5s\n",
            "Epoch 03 | Train Loss: 2.3715, Train Char Acc: 0.3492, Train Word Acc: 0.0040 | Val Loss: 2.3861, Val Char Acc: 0.3525, Val Word Acc: 0.0083 | Time: 41.8s\n",
            "Epoch 04 | Train Loss: 1.9312, Train Char Acc: 0.4928, Train Word Acc: 0.0290 | Val Loss: 2.0365, Val Char Acc: 0.4825, Val Word Acc: 0.0684 | Time: 42.0s\n",
            "Epoch 05 | Train Loss: 1.6033, Train Char Acc: 0.6052, Train Word Acc: 0.0860 | Val Loss: 1.8397, Val Char Acc: 0.5635, Val Word Acc: 0.1319 | Time: 42.3s\n",
            "Epoch 06 | Train Loss: 1.4221, Train Char Acc: 0.6680, Train Word Acc: 0.1433 | Val Loss: 1.7626, Val Char Acc: 0.5883, Val Word Acc: 0.1692 | Time: 42.4s\n",
            "Epoch 07 | Train Loss: 1.3118, Train Char Acc: 0.7046, Train Word Acc: 0.1862 | Val Loss: 1.6894, Val Char Acc: 0.6075, Val Word Acc: 0.2058 | Time: 41.5s\n",
            "Epoch 08 | Train Loss: 1.2351, Train Char Acc: 0.7292, Train Word Acc: 0.2205 | Val Loss: 1.6724, Val Char Acc: 0.6260, Val Word Acc: 0.2231 | Time: 42.1s\n",
            "Epoch 09 | Train Loss: 1.1763, Train Char Acc: 0.7486, Train Word Acc: 0.2498 | Val Loss: 1.6265, Val Char Acc: 0.6293, Val Word Acc: 0.2358 | Time: 41.2s\n",
            "Epoch 10 | Train Loss: 1.1250, Train Char Acc: 0.7651, Train Word Acc: 0.2745 | Val Loss: 1.5997, Val Char Acc: 0.6535, Val Word Acc: 0.2438 | Time: 41.4s\n",
            "Epoch 11 | Train Loss: 1.0853, Train Char Acc: 0.7782, Train Word Acc: 0.3002 | Val Loss: 1.5377, Val Char Acc: 0.6612, Val Word Acc: 0.2658 | Time: 42.9s\n",
            "Epoch 12 | Train Loss: 1.0463, Train Char Acc: 0.7914, Train Word Acc: 0.3216 | Val Loss: 1.5329, Val Char Acc: 0.6686, Val Word Acc: 0.2775 | Time: 41.4s\n",
            "Epoch 13 | Train Loss: 1.0132, Train Char Acc: 0.8017, Train Word Acc: 0.3416 | Val Loss: 1.5696, Val Char Acc: 0.6653, Val Word Acc: 0.2875 | Time: 42.2s\n",
            "Epoch 14 | Train Loss: 0.9839, Train Char Acc: 0.8115, Train Word Acc: 0.3600 | Val Loss: 1.5209, Val Char Acc: 0.6895, Val Word Acc: 0.2862 | Time: 41.6s\n",
            "Epoch 15 | Train Loss: 0.9625, Train Char Acc: 0.8186, Train Word Acc: 0.3783 | Val Loss: 1.5250, Val Char Acc: 0.6768, Val Word Acc: 0.3022 | Time: 41.8s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.004 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▇▂▃▄▅▆▂▅▁▂█▂▅▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▃▅▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▄▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▃▄▄▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▄▅▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▃▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▃▄▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 41.81527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.81856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.96251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.37829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.67684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.52497\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.30219\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb128_hid256_layers5x2_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/bveu05pj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_211426-bveu05pj/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 21:25:05,795 - wandb.wandb_agent - INFO - Cleaning up finished run: bveu05pj\n",
            "2025-05-19 21:25:06,148 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 21:25:06,148 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.3\n",
            "\temb_size: 128\n",
            "\tenc_layers: 3\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 21:25:06,149 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=5 --cell_type=LSTM --dec_layers=1 --dropout=0.3 --emb_size=128 --enc_layers=3 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "2025-05-19 21:25:11,159 - wandb.wandb_agent - INFO - Running runs: ['re6igrrz']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_212511-re6igrrz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb128_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/re6igrrz\u001b[0m\n",
            "Epoch 01 | Train Loss: 2.9599, Train Char Acc: 0.1696, Train Word Acc: 0.0005 | Val Loss: 2.6160, Val Char Acc: 0.2618, Val Word Acc: 0.0026 | Time: 65.5s\n",
            "Epoch 02 | Train Loss: 2.1885, Train Char Acc: 0.3774, Train Word Acc: 0.0180 | Val Loss: 2.0141, Val Char Acc: 0.4234, Val Word Acc: 0.0393 | Time: 66.8s\n",
            "Epoch 03 | Train Loss: 1.7164, Train Char Acc: 0.5314, Train Word Acc: 0.0702 | Val Loss: 1.7401, Val Char Acc: 0.5295, Val Word Acc: 0.0991 | Time: 64.8s\n",
            "Epoch 04 | Train Loss: 1.4825, Train Char Acc: 0.6179, Train Word Acc: 0.1324 | Val Loss: 1.5963, Val Char Acc: 0.5879, Val Word Acc: 0.1580 | Time: 64.8s\n",
            "Epoch 05 | Train Loss: 1.3478, Train Char Acc: 0.6692, Train Word Acc: 0.1866 | Val Loss: 1.5409, Val Char Acc: 0.6167, Val Word Acc: 0.1863 | Time: 65.4s\n",
            "Epoch 06 | Train Loss: 1.2523, Train Char Acc: 0.7059, Train Word Acc: 0.2338 | Val Loss: 1.4746, Val Char Acc: 0.6324, Val Word Acc: 0.2185 | Time: 65.4s\n",
            "Epoch 07 | Train Loss: 1.1860, Train Char Acc: 0.7315, Train Word Acc: 0.2707 | Val Loss: 1.4475, Val Char Acc: 0.6503, Val Word Acc: 0.2334 | Time: 64.7s\n",
            "Epoch 08 | Train Loss: 1.1276, Train Char Acc: 0.7542, Train Word Acc: 0.3062 | Val Loss: 1.4201, Val Char Acc: 0.6639, Val Word Acc: 0.2540 | Time: 65.3s\n",
            "Epoch 09 | Train Loss: 1.0738, Train Char Acc: 0.7752, Train Word Acc: 0.3412 | Val Loss: 1.4289, Val Char Acc: 0.6773, Val Word Acc: 0.2604 | Time: 64.9s\n",
            "Epoch 10 | Train Loss: 1.0367, Train Char Acc: 0.7893, Train Word Acc: 0.3709 | Val Loss: 1.3941, Val Char Acc: 0.6766, Val Word Acc: 0.2820 | Time: 65.3s\n",
            "Epoch 11 | Train Loss: 1.0014, Train Char Acc: 0.8028, Train Word Acc: 0.3977 | Val Loss: 1.3878, Val Char Acc: 0.6813, Val Word Acc: 0.2819 | Time: 65.3s\n",
            "Epoch 12 | Train Loss: 0.9647, Train Char Acc: 0.8167, Train Word Acc: 0.4234 | Val Loss: 1.4411, Val Char Acc: 0.6860, Val Word Acc: 0.2755 | Time: 64.4s\n",
            "Epoch 13 | Train Loss: 0.9335, Train Char Acc: 0.8286, Train Word Acc: 0.4464 | Val Loss: 1.3860, Val Char Acc: 0.6950, Val Word Acc: 0.2974 | Time: 64.8s\n",
            "Epoch 14 | Train Loss: 0.9076, Train Char Acc: 0.8383, Train Word Acc: 0.4712 | Val Loss: 1.3807, Val Char Acc: 0.6946, Val Word Acc: 0.3051 | Time: 66.0s\n",
            "Epoch 15 | Train Loss: 0.8856, Train Char Acc: 0.8466, Train Word Acc: 0.4914 | Val Loss: 1.3784, Val Char Acc: 0.7084, Val Word Acc: 0.3079 | Time: 64.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄█▂▂▄▄▂▄▂▃▄▁▂▆▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▃▅▆▆▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▂▃▄▄▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▃▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▂▃▅▅▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 64.54589\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.8466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.88555\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.49139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.70838\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.3784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.3079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb128_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/re6igrrz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_212511-re6igrrz/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 21:41:39,841 - wandb.wandb_agent - INFO - Cleaning up finished run: re6igrrz\n",
            "2025-05-19 21:41:40,370 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 21:41:40,370 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.3\n",
            "\temb_size: 128\n",
            "\tenc_layers: 5\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 21:41:40,372 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=GRU --dec_layers=1 --dropout=0.3 --emb_size=128 --enc_layers=5 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_214144-6k0pztsi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb128_hid256_layers5x1_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/6k0pztsi\u001b[0m\n",
            "2025-05-19 21:41:45,382 - wandb.wandb_agent - INFO - Running runs: ['6k0pztsi']\n",
            "Epoch 01 | Train Loss: 3.1032, Train Char Acc: 0.1277, Train Word Acc: 0.0000 | Val Loss: 3.1170, Val Char Acc: 0.1397, Val Word Acc: 0.0000 | Time: 38.0s\n",
            "Epoch 02 | Train Loss: 2.6987, Train Char Acc: 0.2460, Train Word Acc: 0.0005 | Val Loss: 2.6717, Val Char Acc: 0.2647, Val Word Acc: 0.0018 | Time: 38.4s\n",
            "Epoch 03 | Train Loss: 2.1108, Train Char Acc: 0.4335, Train Word Acc: 0.0175 | Val Loss: 2.1295, Val Char Acc: 0.4701, Val Word Acc: 0.0466 | Time: 38.2s\n",
            "Epoch 04 | Train Loss: 1.6774, Train Char Acc: 0.5869, Train Word Acc: 0.0779 | Val Loss: 1.8594, Val Char Acc: 0.5519, Val Word Acc: 0.1116 | Time: 38.6s\n",
            "Epoch 05 | Train Loss: 1.4440, Train Char Acc: 0.6603, Train Word Acc: 0.1380 | Val Loss: 1.7601, Val Char Acc: 0.5855, Val Word Acc: 0.1557 | Time: 37.7s\n",
            "Epoch 06 | Train Loss: 1.3156, Train Char Acc: 0.7000, Train Word Acc: 0.1837 | Val Loss: 1.6695, Val Char Acc: 0.6184, Val Word Acc: 0.1834 | Time: 37.7s\n",
            "Epoch 07 | Train Loss: 1.2242, Train Char Acc: 0.7281, Train Word Acc: 0.2182 | Val Loss: 1.6238, Val Char Acc: 0.6247, Val Word Acc: 0.2027 | Time: 37.8s\n",
            "Epoch 08 | Train Loss: 1.1615, Train Char Acc: 0.7484, Train Word Acc: 0.2480 | Val Loss: 1.5928, Val Char Acc: 0.6376, Val Word Acc: 0.2298 | Time: 37.6s\n",
            "Epoch 09 | Train Loss: 1.1055, Train Char Acc: 0.7657, Train Word Acc: 0.2735 | Val Loss: 1.5565, Val Char Acc: 0.6535, Val Word Acc: 0.2380 | Time: 37.6s\n",
            "Epoch 10 | Train Loss: 1.0661, Train Char Acc: 0.7792, Train Word Acc: 0.2958 | Val Loss: 1.5441, Val Char Acc: 0.6558, Val Word Acc: 0.2532 | Time: 38.6s\n",
            "Epoch 11 | Train Loss: 1.0325, Train Char Acc: 0.7909, Train Word Acc: 0.3173 | Val Loss: 1.5162, Val Char Acc: 0.6676, Val Word Acc: 0.2662 | Time: 37.5s\n",
            "Epoch 12 | Train Loss: 0.9990, Train Char Acc: 0.8016, Train Word Acc: 0.3354 | Val Loss: 1.5300, Val Char Acc: 0.6663, Val Word Acc: 0.2751 | Time: 37.7s\n",
            "Epoch 13 | Train Loss: 0.9769, Train Char Acc: 0.8100, Train Word Acc: 0.3539 | Val Loss: 1.4812, Val Char Acc: 0.6840, Val Word Acc: 0.2867 | Time: 38.4s\n",
            "Epoch 14 | Train Loss: 0.9500, Train Char Acc: 0.8195, Train Word Acc: 0.3714 | Val Loss: 1.4765, Val Char Acc: 0.6809, Val Word Acc: 0.2882 | Time: 37.9s\n",
            "Epoch 15 | Train Loss: 0.9319, Train Char Acc: 0.8261, Train Word Acc: 0.3884 | Val Loss: 1.4646, Val Char Acc: 0.6896, Val Word Acc: 0.2967 | Time: 37.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.004 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.004 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄▆▅█▃▂▃▂▂█▁▂▇▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▄▆▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▅▃▃▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▃▄▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▅▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▄▃▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▂▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 37.53756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.82613\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.93185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.38839\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.68964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.46463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.29669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb128_hid256_layers5x1_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/6k0pztsi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_214144-6k0pztsi/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 21:51:23,480 - wandb.wandb_agent - INFO - Cleaning up finished run: 6k0pztsi\n",
            "2025-05-19 21:51:23,887 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 21:51:23,887 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.3\n",
            "\temb_size: 128\n",
            "\tenc_layers: 3\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 21:51:23,889 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=5 --cell_type=LSTM --dec_layers=1 --dropout=0.3 --emb_size=128 --enc_layers=3 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Attention_RNN' when running a sweep.\n",
            "2025-05-19 21:51:28,899 - wandb.wandb_agent - INFO - Running runs: ['jambuhor']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_215128-jambuhor\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb128_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/sweeps/7hka6agi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Attention_RNN/runs/jambuhor\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!wandb agent mrsagarbiswas-iit-madras/Attention_RNN/7hka6agi --count 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "30A_wUwI0yZZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
