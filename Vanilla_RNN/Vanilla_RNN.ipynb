{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YLbBP7SWshbz"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > main.py << 'EOF'\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "# ─── CharVocab & Dataset ─────────────────────────────────────────────────────\n",
        "class CharVocab:\n",
        "    def __init__(self, filepaths: List[str]):\n",
        "        self.rom_char2idx: Dict[str,int] = {}\n",
        "        self.dev_char2idx: Dict[str,int] = {}\n",
        "        self.rom_idx2char: Dict[int,str] = {}\n",
        "        self.dev_idx2char: Dict[int,str] = {}\n",
        "        self._build_vocab(filepaths)\n",
        "\n",
        "    def _build_vocab(self, filepaths: List[str]):\n",
        "        rom_chars = set()\n",
        "        dev_chars = set()\n",
        "        for fp in filepaths:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f, delimiter=\"\\t\")\n",
        "                for row in reader:\n",
        "                    if len(row) < 2:\n",
        "                        continue\n",
        "                    devanagari = row[0].strip()\n",
        "                    roman      = row[1].strip()\n",
        "                    rom_chars.update(list(roman))\n",
        "                    dev_chars.update(list(devanagari))\n",
        "\n",
        "        PAD, SOS, EOS = \"<pad>\", \"<sos>\", \"<eos>\"\n",
        "\n",
        "        all_rom = [PAD, SOS, EOS] + sorted(rom_chars)\n",
        "        for i, ch in enumerate(all_rom):\n",
        "            self.rom_char2idx[ch] = i\n",
        "            self.rom_idx2char[i] = ch\n",
        "\n",
        "        all_dev = [PAD, SOS, EOS] + sorted(dev_chars)\n",
        "        for i, ch in enumerate(all_dev):\n",
        "            self.dev_char2idx[ch] = i\n",
        "            self.dev_idx2char[i] = ch\n",
        "\n",
        "        self.rom_pad_idx = self.rom_char2idx[PAD]\n",
        "        self.rom_sos_idx = self.rom_char2idx[SOS]\n",
        "        self.rom_eos_idx = self.rom_char2idx[EOS]\n",
        "\n",
        "        self.dev_pad_idx = self.dev_char2idx[PAD]\n",
        "        self.dev_sos_idx = self.dev_char2idx[SOS]\n",
        "        self.dev_eos_idx = self.dev_char2idx[EOS]\n",
        "\n",
        "    @property\n",
        "    def rom_vocab_size(self) -> int:\n",
        "        return len(self.rom_char2idx)\n",
        "\n",
        "    @property\n",
        "    def dev_vocab_size(self) -> int:\n",
        "        return len(self.dev_char2idx)\n",
        "\n",
        "    def roman_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.rom_sos_idx] + [self.rom_char2idx[ch] for ch in s] + [self.rom_eos_idx]\n",
        "\n",
        "    def dev_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.dev_sos_idx] + [self.dev_char2idx[ch] for ch in s] + [self.dev_eos_idx]\n",
        "\n",
        "    def indices_to_dev(self, idxs: List[int]) -> str:\n",
        "        chars = []\n",
        "        for i in idxs:\n",
        "            if i in (self.dev_sos_idx, self.dev_eos_idx, self.dev_pad_idx):\n",
        "                continue\n",
        "            chars.append(self.dev_idx2char[i])\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "def read_tsv(path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Expects each line of the TSV to be:\n",
        "      Devanagari_word    Roman_word    <something_to_ignore>\n",
        "    We only need (Roman, Devanagari) for training.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            # If there are fewer than 2 columns, skip\n",
        "            if len(row) < 2:\n",
        "                continue\n",
        "\n",
        "            # Unpack: Dev is first column, Roman is second, ignore anything else\n",
        "            devana = row[0].strip()\n",
        "            roman  = row[1].strip()\n",
        "\n",
        "            if not roman or not devana:\n",
        "                continue\n",
        "            # Append (roman, devana)—this matches our CharVocab convention\n",
        "            pairs.append((roman, devana))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, filepath, vocab):\n",
        "        super().__init__()\n",
        "        self.pairs = read_tsv(filepath)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        roman, devanagari = self.pairs[idx]\n",
        "        roman_idxs = self.vocab.roman_to_indices(roman)\n",
        "        dev_idxs = self.vocab.dev_to_indices(devanagari)\n",
        "        return torch.tensor(roman_idxs, dtype=torch.long), torch.tensor(dev_idxs, dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        roman_seqs, dev_seqs = zip(*batch)\n",
        "        max_rom_len = max(len(x) for x in roman_seqs)\n",
        "        max_dev_len = max(len(x) for x in dev_seqs)\n",
        "        rom_padded = []\n",
        "        dev_padded = []\n",
        "        for r, d in zip(roman_seqs, dev_seqs):\n",
        "            pad_r = torch.cat([r, r.new_full((max_rom_len - len(r),), r.new_tensor(0))])\n",
        "            pad_d = torch.cat([d, d.new_full((max_dev_len - len(d),), d.new_tensor(0))])\n",
        "            rom_padded.append(pad_r)\n",
        "            dev_padded.append(pad_d)\n",
        "        return torch.stack(rom_padded), torch.stack(dev_padded)\n",
        "\n",
        "\n",
        "# ─── Encoder & Decoder ────────────────────────────────────────────────────────\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(input_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (h_n, c_n) = self.rnn(emb)\n",
        "            return outputs, (h_n, c_n)\n",
        "        else:\n",
        "            outputs, h_n = self.rnn(emb)\n",
        "            return outputs, h_n\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(output_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, tgt_seq, hidden, cell=None, teacher_forcing_ratio=0.0):\n",
        "        B, T = tgt_seq.size()\n",
        "        outputs = torch.zeros(B, T, self.out.out_features, device=tgt_seq.device)\n",
        "        input_step = tgt_seq[:, 0]\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            h = hidden       # h_n: (num_layers, B, hidden_size)\n",
        "            c = cell         # c_n: (num_layers, B, hidden_size)\n",
        "        else:\n",
        "            h = hidden\n",
        "            c = None\n",
        "\n",
        "        for t in range(1, T):\n",
        "            emb_t = self.embed(input_step).unsqueeze(1)\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                out_step, (h, c) = self.rnn(emb_t, (h, c))\n",
        "            else:\n",
        "                out_step, h = self.rnn(emb_t, h)\n",
        "                c = None\n",
        "\n",
        "            logits = self.out(out_step.squeeze(1))\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "            top1 = logits.argmax(dim=1)\n",
        "            next_input = tgt_seq[:, t] if teacher_force else top1\n",
        "            input_step = next_input.view(-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_decode(self, encoder_hidden, encoder_cell, max_len, dev_sos_idx, dev_eos_idx, beam_size):\n",
        "        hidden, cell = encoder_hidden, encoder_cell\n",
        "        live = [([dev_sos_idx], hidden, cell, 0.0)]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_hyps = []\n",
        "            for (seq, h, c, score) in live:\n",
        "                last_token = seq[-1]\n",
        "                if last_token == dev_eos_idx:\n",
        "                    completed.append((seq, h, c, score))\n",
        "                    continue\n",
        "\n",
        "                inp = torch.tensor([last_token], dtype=torch.long, device=h.device).unsqueeze(0)\n",
        "                emb_t = self.embed(inp)\n",
        "\n",
        "                if self.cell_type == \"LSTM\":\n",
        "                    out_t, (h2, c2) = self.rnn(emb_t, (h, c))\n",
        "                else:\n",
        "                    out_t, h2 = self.rnn(emb_t, h)\n",
        "                    c2 = None\n",
        "\n",
        "                logits = self.out(out_t.squeeze(1))\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                topk_logprobs, topk_indices = torch.topk(log_probs, k=beam_size, dim=1)\n",
        "                topk_logprobs = topk_logprobs.squeeze(0).tolist()\n",
        "                topk_indices = topk_indices.squeeze(0).tolist()\n",
        "                for lp, idx in zip(topk_logprobs, topk_indices):\n",
        "                    new_hyps.append((seq + [idx], h2, c2, score + lp))\n",
        "\n",
        "            # Keep only top‐k\n",
        "            new_hyps = sorted(new_hyps, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "            live = new_hyps\n",
        "            if all(hyp[0][-1] == dev_eos_idx for hyp in live):\n",
        "                completed.extend(live)\n",
        "                break\n",
        "\n",
        "        if not completed:\n",
        "            completed = live\n",
        "        best = max(completed, key=lambda x: x[3])\n",
        "        return best[0]\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        src: (B, T_src)\n",
        "        tgt: (B, T_tgt)   # including <sos> … <eos>\n",
        "        Returns:\n",
        "          logits: (B, T_tgt, V_out)\n",
        "        \"\"\"\n",
        "        B, T_src = src.size()\n",
        "        outputs = torch.zeros(B, tgt.size(1), self.decoder.out.out_features, device=self.device)\n",
        "\n",
        "        # 1) Run the encoder\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            enc_outputs, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            enc_outputs, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # 2) Transform encoder's hidden‐state to match decoder layers\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = None\n",
        "\n",
        "        # 3) Run decoder (with teacher forcing)\n",
        "        logits = self.decoder(\n",
        "            tgt_seq=tgt,\n",
        "            hidden=dec_hidden,\n",
        "            cell=dec_cell,\n",
        "            teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "        )\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src: torch.Tensor, max_len: int, dev_sos_idx: int, dev_eos_idx: int, beam_size: int = 1) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Greedy (beam_size=1) or beam search decoding for batch size=1. Returns a list of decoded indices.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            _, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            _, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "        B = 1\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = None\n",
        "\n",
        "        if beam_size == 1:\n",
        "            seq = [dev_sos_idx]\n",
        "            hidden_, cell_ = hidden_state, cell_state\n",
        "            for _ in range(max_len):\n",
        "                last_token = torch.tensor([seq[-1]], dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "                emb = self.decoder.embed(last_token)\n",
        "                if self.decoder.cell_type == \"LSTM\":\n",
        "                    out, (h_next, c_next) = self.decoder.rnn(emb, (hidden_, cell_))\n",
        "                    hidden_, cell_ = h_next, c_next\n",
        "                else:\n",
        "                    out, h_next = self.decoder.rnn(emb, hidden_)\n",
        "                    hidden_, cell_ = h_next, None\n",
        "\n",
        "                logits = self.decoder.out(out.squeeze(1))\n",
        "                next_token = logits.argmax(dim=1).item()\n",
        "                seq.append(next_token)\n",
        "                if next_token == dev_eos_idx:\n",
        "                    break\n",
        "            return [seq]\n",
        "        else:\n",
        "            best_seq = self.decoder.beam_search_decode(\n",
        "                encoder_hidden=hidden_state,\n",
        "                encoder_cell=cell_state,\n",
        "                max_len=max_len,\n",
        "                dev_sos_idx=dev_sos_idx,\n",
        "                dev_eos_idx=dev_eos_idx,\n",
        "                beam_size=beam_size,\n",
        "            )\n",
        "            return [best_seq]\n",
        "\n",
        "\n",
        "# ─── train / evaluate ───────────────────────────────────────────────────────\n",
        "def char_accuracy(\n",
        "    logits: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int\n",
        ") -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Compute (num_correct_chars, num_valid_chars) ignoring <pad>, <sos>, and <eos>.\n",
        "    logits: (B, T, V)    – raw decoder logits\n",
        "    target: (B, T)       – ground‐truth indices (including <sos>, <eos>, <pad>)\n",
        "    pad_idx, sos_idx, eos_idx: indices for the special tokens\n",
        "\n",
        "    Returns:\n",
        "       (correct_count, valid_count)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        B, T, V = logits.size()\n",
        "        pred = logits.argmax(dim=2)   # (B, T)\n",
        "\n",
        "        # Mask out positions where target is pad, sos, or eos\n",
        "        ignore_mask = (\n",
        "            (target != pad_idx) &\n",
        "            (target != sos_idx) &\n",
        "            (target != eos_idx)\n",
        "        )  # (B, T)\n",
        "\n",
        "        correct = ((pred == target) & ignore_mask).sum().item()\n",
        "        valid   = ignore_mask.sum().item()\n",
        "        return correct, valid\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int,\n",
        "    device: torch.device,\n",
        "    teacher_forcing_ratio: float,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Runs one epoch of training, returning:\n",
        "      (train_loss, train_char_acc, train_word_acc)\n",
        "\n",
        "    – train_char_acc is computed as (total_correct_chars / total_valid_chars)\n",
        "    – train_word_acc is (num_exact_word_matches / total_words)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # For micro‐average char‐accuracy:\n",
        "    total_correct_chars = 0\n",
        "    total_valid_chars   = 0\n",
        "\n",
        "    # For word‐accuracy (we’ll count how many sequences are exactly correct)\n",
        "    total_exact_words = 0\n",
        "    total_words       = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(device, non_blocking=True)  # (B, T_src)\n",
        "        tgt = tgt.to(device, non_blocking=True)  # (B, T_tgt)\n",
        "        B, T_tgt = tgt.size()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output_logits = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        # output_logits: (B, T_tgt, V_out)\n",
        "\n",
        "        # 1) Compute token‐level (character) loss\n",
        "        V = output_logits.size(-1)\n",
        "        loss = criterion(output_logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "        # 2) Compute char‐level correct/valid counts (ignore special tokens)\n",
        "        correct, valid = char_accuracy(\n",
        "            output_logits, tgt,\n",
        "            pad_idx, sos_idx, eos_idx\n",
        "        )\n",
        "        total_correct_chars += correct\n",
        "        total_valid_chars   += valid\n",
        "\n",
        "        # 3) Compute word‐level accuracy for this batch\n",
        "        #    We already have output_logits. Let’s get preds:\n",
        "        with torch.no_grad():\n",
        "            pred_inds = output_logits.argmax(dim=2)  # (B, T_tgt)\n",
        "\n",
        "            # For each example, ignore index 0 (<sos>). Then check that\n",
        "            #   for every position t where tgt[b,t] != pad_idx,\n",
        "            #   pred_inds[b,t] == tgt[b,t].\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)  # which positions to verify\n",
        "\n",
        "            # A word is correct if for all non‐pad positions, pred == tgt\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)  # (B,)\n",
        "            total_exact_words += exact_matches.sum().item()\n",
        "            total_words       += B\n",
        "\n",
        "        # 4) Backward + step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Final epoch‐level metrics:\n",
        "    train_loss = epoch_loss / len(iterator)\n",
        "    train_char_acc = total_correct_chars / max(total_valid_chars, 1)\n",
        "    train_word_acc = total_exact_words   / max(total_words, 1)\n",
        "\n",
        "    return train_loss, train_char_acc, train_word_acc\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    sos_idx: int,\n",
        "    eos_idx: int,\n",
        "    device: torch.device,\n",
        "    beam_size: int,\n",
        "    max_dev_len: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    One validation pass (no teacher forcing). Returns:\n",
        "      (val_loss, val_char_acc, val_word_acc)\n",
        "\n",
        "    – val_char_acc is (total_correct_chars / total_valid_chars)\n",
        "    – val_word_acc is (num_exact_word_matches / total_words)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    total_correct_chars = 0\n",
        "    total_valid_chars   = 0\n",
        "    total_exact_words   = 0\n",
        "    total_words         = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "            src = src.to(device, non_blocking=True)\n",
        "            tgt = tgt.to(device, non_blocking=True)   # (B, T_tgt)\n",
        "            B, T_tgt = tgt.size()\n",
        "\n",
        "            # 1) Forward pass (teacher_forcing_ratio=0.0)\n",
        "            logits = model(src, tgt, teacher_forcing_ratio=0.0)  # (B, T_tgt, V_out)\n",
        "            V = logits.size(-1)\n",
        "            loss = criterion(logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "            # 2) Character‐level correct/valid counts (ignore special tokens)\n",
        "            correct, valid = char_accuracy(\n",
        "                logits, tgt,\n",
        "                pad_idx, sos_idx, eos_idx\n",
        "            )\n",
        "            total_correct_chars += correct\n",
        "            total_valid_chars   += valid\n",
        "\n",
        "            # 3) Word‐level accuracy (greedy decode)\n",
        "            pred_inds = logits.argmax(dim=2)  # (B, T_tgt)\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)\n",
        "\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)  # (B,)\n",
        "            total_exact_words += exact_matches.sum().item()\n",
        "            total_words       += B\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    val_loss = epoch_loss / len(iterator)\n",
        "    val_char_acc = total_correct_chars / max(total_valid_chars, 1)\n",
        "    val_word_acc = total_exact_words / max(total_words, 1)\n",
        "\n",
        "    return val_loss, val_char_acc, val_word_acc\n",
        "\n",
        "\n",
        "# ─── main() with parse_known_args ─────────────────────────────────────────────\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content\")\n",
        "    parser.add_argument(\"--train_file\", type=str, default=\"bn.translit.sampled.train.tsv\")\n",
        "    parser.add_argument(\"--dev_file\", type=str, default=\"bn.translit.sampled.dev.tsv\")\n",
        "    parser.add_argument(\"--test_file\", type=str, default=\"bn.translit.sampled.test.tsv\")\n",
        "    parser.add_argument(\"--emb_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--enc_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--dec_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--cell_type\", type=str, default=\"RNN\", choices=[\"RNN\", \"GRU\", \"LSTM\"])\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--tf_ratio\", type=float, default=0.5)\n",
        "    parser.add_argument(\"--beam_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--max_dev_len\", type=int, default=32)\n",
        "    parser.add_argument(\"--project_name\", type=str, default=\"Vanilla_RNN\")\n",
        "    parser.add_argument(\"--run_name\", type=str, default=None)\n",
        "\n",
        "    # ── use parse_known_args to ignore Colab’s extra \"-f …json\"\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_path = os.path.join(args.data_dir, args.train_file)\n",
        "    dev_path   = os.path.join(args.data_dir, args.dev_file)\n",
        "    test_path  = os.path.join(args.data_dir, args.test_file)\n",
        "    vocab = CharVocab([train_path, dev_path, test_path])\n",
        "\n",
        "    # Extract special‐token indices for convenience\n",
        "    pad_idx = vocab.dev_pad_idx\n",
        "    sos_idx = vocab.dev_sos_idx\n",
        "    eos_idx = vocab.dev_eos_idx\n",
        "\n",
        "    train_ds = TransliterationDataset(train_path, vocab)\n",
        "    dev_ds   = TransliterationDataset(dev_path, vocab)\n",
        "    test_ds  = TransliterationDataset(test_path, vocab)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        dev_ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=TransliterationDataset.collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    if args.run_name is None:\n",
        "        args.run_name = (\n",
        "            f\"{args.cell_type}\"\n",
        "            f\"_emb{args.emb_size}\"\n",
        "            f\"_hid{args.hidden_size}\"\n",
        "            f\"_layers{args.enc_layers}x{args.dec_layers}\"\n",
        "            f\"_lr{args.lr}\"\n",
        "            f\"_tf{args.tf_ratio}\"\n",
        "        )\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.project_name,\n",
        "        name=args.run_name,\n",
        "        config={\n",
        "            \"emb_size\": args.emb_size,\n",
        "            \"hidden_size\": args.hidden_size,\n",
        "            \"enc_layers\": args.enc_layers,\n",
        "            \"dec_layers\": args.dec_layers,\n",
        "            \"cell_type\": args.cell_type,\n",
        "            \"dropout\": args.dropout,\n",
        "            \"lr\": args.lr,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"epochs\": args.epochs,\n",
        "            \"tf_ratio\": args.tf_ratio,\n",
        "            \"beam_size\": args.beam_size,\n",
        "        },\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_vocab_size=vocab.rom_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.enc_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    decoder = Decoder(\n",
        "        output_vocab_size=vocab.dev_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.dec_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.dev_pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-5)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1) Train\n",
        "        train_loss, train_char_acc, train_word_acc = train_one_epoch(\n",
        "            model=model,\n",
        "            iterator=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            pad_idx=pad_idx,\n",
        "            sos_idx=sos_idx,\n",
        "            eos_idx=eos_idx,\n",
        "            device=device,\n",
        "            teacher_forcing_ratio=config.tf_ratio,\n",
        "        )\n",
        "\n",
        "        # 2) Evaluate\n",
        "        val_loss, val_char_acc, val_word_acc = evaluate(\n",
        "            model=model,\n",
        "            iterator=dev_loader,\n",
        "            criterion=criterion,\n",
        "            pad_idx=pad_idx,\n",
        "            sos_idx=sos_idx,\n",
        "            eos_idx=eos_idx,\n",
        "            device=device,\n",
        "            beam_size=config.beam_size,\n",
        "            max_dev_len=args.max_dev_len,\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # 3) Save best model based on val_char_acc\n",
        "        if val_char_acc > best_val_acc:\n",
        "            best_val_acc = val_char_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "        # 4) Log to W&B\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_char_acc\": train_char_acc,\n",
        "                \"train_word_acc\": train_word_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_char_acc\": val_char_acc,\n",
        "                \"val_word_acc\": val_word_acc,\n",
        "                \"epoch_time_sec\": elapsed,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 5) Print progress\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, \"\n",
        "            f\"Train Char Acc: {train_char_acc:.4f}, \"\n",
        "            f\"Train Word Acc: {train_word_acc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Char Acc: {val_char_acc:.4f}, \"\n",
        "            f\"Val Word Acc: {val_word_acc:.4f} | \"\n",
        "            f\"Time: {elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IHjg2NT0si4x"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > sweep.yaml << 'EOF'\n",
        "program: main.py\n",
        "method: bayes\n",
        "project: Vanilla_RNN\n",
        "entity: mrsagarbiswas-iit-madras\n",
        "metric:\n",
        "  name: val_char_acc\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  emb_size:\n",
        "    values: [128, 256]\n",
        "  hidden_size:\n",
        "    values: [128, 256]\n",
        "  enc_layers:\n",
        "    values: [3, 5]\n",
        "  epochs:\n",
        "    values: [10, 15]\n",
        "  dec_layers:\n",
        "    values: [1, 2, 3]\n",
        "  cell_type:\n",
        "    values: [\"RNN\", \"GRU\", \"LSTM\"]\n",
        "  dropout:\n",
        "    values: [0.2, 0.3]\n",
        "  lr:\n",
        "    values: [1e-3, 1e-4]\n",
        "  batch_size:\n",
        "    values: [32, 64, 128]\n",
        "  tf_ratio:\n",
        "    values: [0.3, 0.5, 0.7]\n",
        "  beam_size:\n",
        "    values: [3, 5]\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuSvKgqQs66p",
        "outputId": "e3cc33b2-1f5f-4a67-f017-f30ca7219cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sweep ID = \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Currently logged in as: mrsagarbiswas (mrsagarbiswas-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Creating sweep from: sweep.yaml\n",
            "wandb: Creating sweep with ID: lv5aj1ns\n",
            "wandb: View sweep at: https://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\n",
            "wandb: Run sweep agent with: wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/lv5aj1ns\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export WANDB_API_KEY=\"Your-API-Key\"\n",
        "wandb login\n",
        "SWEEP_ID=$(wandb sweep sweep.yaml)\n",
        "echo \"Sweep ID = $SWEEP_ID\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc33AXMhyJpg",
        "outputId": "e3e5d3d0-2ba4-49dd-e64e-559162c7fd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2025-05-19 15:26:53,379 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2025-05-19 15:26:53,765 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 15:26:53,765 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.4\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 15:26:53,766 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=5 --cell_type=LSTM --dec_layers=2 --dropout=0.4 --emb_size=256 --enc_layers=3 --epochs=10 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_152658-erwn268g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers3x2_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/erwn268g\u001b[0m\n",
            "2025-05-19 15:26:58,777 - wandb.wandb_agent - INFO - Running runs: ['erwn268g']\n",
            "Epoch 01 | Train Loss: 2.6174, Train Char Acc: 0.2715, Train Word Acc: 0.0058 | Val Loss: 2.1787, Val Char Acc: 0.3841, Val Word Acc: 0.0301 | Time: 98.1s\n",
            "Epoch 02 | Train Loss: 1.7158, Train Char Acc: 0.5351, Train Word Acc: 0.0636 | Val Loss: 1.8124, Val Char Acc: 0.5116, Val Word Acc: 0.1028 | Time: 98.4s\n",
            "Epoch 03 | Train Loss: 1.4251, Train Char Acc: 0.6378, Train Word Acc: 0.1338 | Val Loss: 1.6770, Val Char Acc: 0.5663, Val Word Acc: 0.1569 | Time: 97.5s\n",
            "Epoch 04 | Train Loss: 1.2667, Train Char Acc: 0.6976, Train Word Acc: 0.1958 | Val Loss: 1.6300, Val Char Acc: 0.5973, Val Word Acc: 0.2001 | Time: 97.1s\n",
            "Epoch 05 | Train Loss: 1.1569, Train Char Acc: 0.7407, Train Word Acc: 0.2496 | Val Loss: 1.5647, Val Char Acc: 0.6263, Val Word Acc: 0.2350 | Time: 98.0s\n",
            "Epoch 06 | Train Loss: 1.0738, Train Char Acc: 0.7736, Train Word Acc: 0.3045 | Val Loss: 1.5529, Val Char Acc: 0.6443, Val Word Acc: 0.2610 | Time: 98.1s\n",
            "Epoch 07 | Train Loss: 1.0099, Train Char Acc: 0.7984, Train Word Acc: 0.3501 | Val Loss: 1.5307, Val Char Acc: 0.6582, Val Word Acc: 0.2807 | Time: 97.3s\n",
            "Epoch 08 | Train Loss: 0.9569, Train Char Acc: 0.8194, Train Word Acc: 0.3945 | Val Loss: 1.5230, Val Char Acc: 0.6691, Val Word Acc: 0.2903 | Time: 97.0s\n",
            "Epoch 09 | Train Loss: 0.9093, Train Char Acc: 0.8371, Train Word Acc: 0.4336 | Val Loss: 1.5196, Val Char Acc: 0.6760, Val Word Acc: 0.3035 | Time: 97.7s\n",
            "Epoch 10 | Train Loss: 0.8711, Train Char Acc: 0.8516, Train Word Acc: 0.4674 | Val Loss: 1.5451, Val Char Acc: 0.6757, Val Word Acc: 0.3014 | Time: 98.7s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.014 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▆▇▃▂▅▆▂▁▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▄▅▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▂▃▄▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▃▄▅▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 98.68544\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.85163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.87108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.46741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.67573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.54511\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.30143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers3x2_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/erwn268g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_152658-erwn268g/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 15:43:28,223 - wandb.wandb_agent - INFO - Cleaning up finished run: erwn268g\n",
            "2025-05-19 15:43:29,587 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 15:43:29,587 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.4\n",
            "\temb_size: 256\n",
            "\tenc_layers: 5\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 15:43:29,588 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=LSTM --dec_layers=3 --dropout=0.4 --emb_size=256 --enc_layers=5 --epochs=10 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "2025-05-19 15:43:34,599 - wandb.wandb_agent - INFO - Running runs: ['jsf7a1nw']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_154335-jsf7a1nw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers5x3_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/jsf7a1nw\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.1950, Train Char Acc: 0.1015, Train Word Acc: 0.0000 | Val Loss: 3.0644, Val Char Acc: 0.1172, Val Word Acc: 0.0000 | Time: 38.5s\n",
            "Epoch 02 | Train Loss: 2.9695, Train Char Acc: 0.1551, Train Word Acc: 0.0001 | Val Loss: 2.9206, Val Char Acc: 0.1825, Val Word Acc: 0.0000 | Time: 37.8s\n",
            "Epoch 03 | Train Loss: 2.6890, Train Char Acc: 0.2456, Train Word Acc: 0.0006 | Val Loss: 2.5981, Val Char Acc: 0.2742, Val Word Acc: 0.0015 | Time: 37.8s\n",
            "Epoch 04 | Train Loss: 2.3794, Train Char Acc: 0.3356, Train Word Acc: 0.0043 | Val Loss: 2.2836, Val Char Acc: 0.3617, Val Word Acc: 0.0084 | Time: 37.7s\n",
            "Epoch 05 | Train Loss: 2.1141, Train Char Acc: 0.4074, Train Word Acc: 0.0141 | Val Loss: 2.0717, Val Char Acc: 0.4203, Val Word Acc: 0.0241 | Time: 37.6s\n",
            "Epoch 06 | Train Loss: 1.9109, Train Char Acc: 0.4655, Train Word Acc: 0.0308 | Val Loss: 1.9197, Val Char Acc: 0.4706, Val Word Acc: 0.0473 | Time: 37.6s\n",
            "Epoch 07 | Train Loss: 1.7608, Train Char Acc: 0.5130, Train Word Acc: 0.0522 | Val Loss: 1.8025, Val Char Acc: 0.4941, Val Word Acc: 0.0652 | Time: 37.6s\n",
            "Epoch 08 | Train Loss: 1.6426, Train Char Acc: 0.5526, Train Word Acc: 0.0748 | Val Loss: 1.7404, Val Char Acc: 0.5048, Val Word Acc: 0.0838 | Time: 37.7s\n",
            "Epoch 09 | Train Loss: 1.5533, Train Char Acc: 0.5836, Train Word Acc: 0.0976 | Val Loss: 1.7068, Val Char Acc: 0.5488, Val Word Acc: 0.1169 | Time: 37.5s\n",
            "Epoch 10 | Train Loss: 1.4797, Train Char Acc: 0.6110, Train Word Acc: 0.1198 | Val Loss: 1.6305, Val Char Acc: 0.5656, Val Word Acc: 0.1239 | Time: 37.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec █▃▃▂▂▁▁▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▃▄▅▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▅▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▂▃▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▃▅▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▄▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▂▄▅▆██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 37.54376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.61101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.47966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.11981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.56562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.63053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.12394\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers5x3_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/jsf7a1nw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_154335-jsf7a1nw/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 15:50:02,213 - wandb.wandb_agent - INFO - Cleaning up finished run: jsf7a1nw\n",
            "2025-05-19 15:50:02,941 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 15:50:02,941 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.4\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 15\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-19 15:50:02,943 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=GRU --dec_layers=3 --dropout=0.4 --emb_size=256 --enc_layers=3 --epochs=15 --hidden_size=256 --lr=0.0001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-19 15:50:07,954 - wandb.wandb_agent - INFO - Running runs: ['r7evo7pz']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_155007-r7evo7pz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb256_hid256_layers3x3_lr0.0001_tf0.7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/r7evo7pz\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.0001, Train Char Acc: 0.1734, Train Word Acc: 0.0005 | Val Loss: 2.8089, Val Char Acc: 0.2569, Val Word Acc: 0.0023 | Time: 32.1s\n",
            "Epoch 02 | Train Loss: 2.1570, Train Char Acc: 0.4114, Train Word Acc: 0.0138 | Val Loss: 2.2786, Val Char Acc: 0.3956, Val Word Acc: 0.0374 | Time: 32.4s\n",
            "Epoch 03 | Train Loss: 1.7188, Train Char Acc: 0.5464, Train Word Acc: 0.0527 | Val Loss: 2.0400, Val Char Acc: 0.4782, Val Word Acc: 0.0846 | Time: 31.9s\n",
            "Epoch 04 | Train Loss: 1.4867, Train Char Acc: 0.6233, Train Word Acc: 0.0956 | Val Loss: 1.9058, Val Char Acc: 0.5219, Val Word Acc: 0.1271 | Time: 32.1s\n",
            "Epoch 05 | Train Loss: 1.3492, Train Char Acc: 0.6717, Train Word Acc: 0.1341 | Val Loss: 1.8348, Val Char Acc: 0.5563, Val Word Acc: 0.1594 | Time: 31.1s\n",
            "Epoch 06 | Train Loss: 1.2480, Train Char Acc: 0.7087, Train Word Acc: 0.1721 | Val Loss: 1.7865, Val Char Acc: 0.5879, Val Word Acc: 0.1924 | Time: 32.2s\n",
            "Epoch 07 | Train Loss: 1.1768, Train Char Acc: 0.7350, Train Word Acc: 0.2041 | Val Loss: 1.7352, Val Char Acc: 0.6000, Val Word Acc: 0.2024 | Time: 31.2s\n",
            "Epoch 08 | Train Loss: 1.1162, Train Char Acc: 0.7578, Train Word Acc: 0.2346 | Val Loss: 1.7295, Val Char Acc: 0.6182, Val Word Acc: 0.2317 | Time: 32.0s\n",
            "Epoch 09 | Train Loss: 1.0660, Train Char Acc: 0.7764, Train Word Acc: 0.2641 | Val Loss: 1.7034, Val Char Acc: 0.6317, Val Word Acc: 0.2475 | Time: 31.7s\n",
            "Epoch 10 | Train Loss: 1.0270, Train Char Acc: 0.7910, Train Word Acc: 0.2887 | Val Loss: 1.6689, Val Char Acc: 0.6490, Val Word Acc: 0.2631 | Time: 31.3s\n",
            "Epoch 11 | Train Loss: 0.9934, Train Char Acc: 0.8039, Train Word Acc: 0.3129 | Val Loss: 1.6663, Val Char Acc: 0.6509, Val Word Acc: 0.2745 | Time: 31.9s\n",
            "Epoch 12 | Train Loss: 0.9612, Train Char Acc: 0.8162, Train Word Acc: 0.3365 | Val Loss: 1.6514, Val Char Acc: 0.6598, Val Word Acc: 0.2898 | Time: 31.0s\n",
            "Epoch 13 | Train Loss: 0.9365, Train Char Acc: 0.8251, Train Word Acc: 0.3572 | Val Loss: 1.6435, Val Char Acc: 0.6622, Val Word Acc: 0.2909 | Time: 31.9s\n",
            "Epoch 14 | Train Loss: 0.9106, Train Char Acc: 0.8352, Train Word Acc: 0.3793 | Val Loss: 1.6197, Val Char Acc: 0.6738, Val Word Acc: 0.3011 | Time: 31.0s\n",
            "Epoch 15 | Train Loss: 0.8887, Train Char Acc: 0.8434, Train Word Acc: 0.4010 | Val Loss: 1.6292, Val Char Acc: 0.6767, Val Word Acc: 0.3043 | Time: 31.9s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.002 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.016 MB of 0.016 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▇█▆▇▂▇▂▆▅▃▆▁▆▁▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▃▅▆▆▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▂▃▃▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▅▅▆▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▂▃▄▅▅▆▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 31.89614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.84341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.88866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.40098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.67673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.62919\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.30434\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb256_hid256_layers3x3_lr0.0001_tf0.7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/r7evo7pz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_155007-r7evo7pz/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 15:58:14,782 - wandb.wandb_agent - INFO - Cleaning up finished run: r7evo7pz\n",
            "2025-05-19 15:58:15,268 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 15:58:15,268 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.4\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 15:58:15,269 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=LSTM --dec_layers=1 --dropout=0.4 --emb_size=256 --enc_layers=3 --epochs=10 --hidden_size=256 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-19 15:58:20,279 - wandb.wandb_agent - INFO - Running runs: ['xnjqnscd']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_155819-xnjqnscd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers3x1_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/xnjqnscd\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.1090, Train Char Acc: 0.1395, Train Word Acc: 0.0000 | Val Loss: 3.1078, Val Char Acc: 0.1392, Val Word Acc: 0.0000 | Time: 25.7s\n",
            "Epoch 02 | Train Loss: 2.6096, Train Char Acc: 0.2747, Train Word Acc: 0.0019 | Val Loss: 2.5592, Val Char Acc: 0.2792, Val Word Acc: 0.0058 | Time: 25.4s\n",
            "Epoch 03 | Train Loss: 2.0880, Train Char Acc: 0.4232, Train Word Acc: 0.0207 | Val Loss: 2.2113, Val Char Acc: 0.3928, Val Word Acc: 0.0425 | Time: 25.4s\n",
            "Epoch 04 | Train Loss: 1.7771, Train Char Acc: 0.5213, Train Word Acc: 0.0549 | Val Loss: 2.0003, Val Char Acc: 0.4501, Val Word Acc: 0.0755 | Time: 25.4s\n",
            "Epoch 05 | Train Loss: 1.5902, Train Char Acc: 0.5850, Train Word Acc: 0.0910 | Val Loss: 1.8860, Val Char Acc: 0.4976, Val Word Acc: 0.1149 | Time: 25.4s\n",
            "Epoch 06 | Train Loss: 1.4703, Train Char Acc: 0.6265, Train Word Acc: 0.1245 | Val Loss: 1.7966, Val Char Acc: 0.5240, Val Word Acc: 0.1305 | Time: 25.5s\n",
            "Epoch 07 | Train Loss: 1.3764, Train Char Acc: 0.6617, Train Word Acc: 0.1569 | Val Loss: 1.7519, Val Char Acc: 0.5490, Val Word Acc: 0.1455 | Time: 25.7s\n",
            "Epoch 08 | Train Loss: 1.3038, Train Char Acc: 0.6886, Train Word Acc: 0.1880 | Val Loss: 1.6949, Val Char Acc: 0.5703, Val Word Acc: 0.1718 | Time: 25.6s\n",
            "Epoch 09 | Train Loss: 1.2409, Train Char Acc: 0.7125, Train Word Acc: 0.2170 | Val Loss: 1.6675, Val Char Acc: 0.5812, Val Word Acc: 0.1826 | Time: 25.6s\n",
            "Epoch 10 | Train Loss: 1.1949, Train Char Acc: 0.7301, Train Word Acc: 0.2423 | Val Loss: 1.6485, Val Char Acc: 0.5922, Val Word Acc: 0.1930 | Time: 25.4s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▇▁▂▁▃▄█▆▅▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▃▄▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▄▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▂▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▅▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▄▃▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▃▄▅▆▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 25.42282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.73011\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.19485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.24234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.59218\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.64851\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.19302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers3x1_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/xnjqnscd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_155819-xnjqnscd/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 16:02:47,318 - wandb.wandb_agent - INFO - Cleaning up finished run: xnjqnscd\n",
            "2025-05-19 16:02:47,837 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 16:02:47,837 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 5\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.4\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 16:02:47,839 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=5 --cell_type=LSTM --dec_layers=1 --dropout=0.4 --emb_size=256 --enc_layers=3 --epochs=10 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-19 16:02:52,849 - wandb.wandb_agent - INFO - Running runs: ['nujwl52c']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_160252-nujwl52c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/lv5aj1ns\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/nujwl52c\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.1571, Train Char Acc: 0.1243, Train Word Acc: 0.0000 | Val Loss: 3.0021, Val Char Acc: 0.1648, Val Word Acc: 0.0002 | Time: 25.7s\n",
            "Epoch 02 | Train Loss: 2.6232, Train Char Acc: 0.2605, Train Word Acc: 0.0026 | Val Loss: 2.4269, Val Char Acc: 0.3015, Val Word Acc: 0.0084 | Time: 25.3s\n",
            "Epoch 03 | Train Loss: 2.1549, Train Char Acc: 0.3837, Train Word Acc: 0.0188 | Val Loss: 2.1044, Val Char Acc: 0.4111, Val Word Acc: 0.0392 | Time: 25.2s\n",
            "Epoch 04 | Train Loss: 1.8733, Train Char Acc: 0.4724, Train Word Acc: 0.0472 | Val Loss: 1.9133, Val Char Acc: 0.4563, Val Word Acc: 0.0628 | Time: 25.2s\n",
            "Epoch 05 | Train Loss: 1.6983, Train Char Acc: 0.5324, Train Word Acc: 0.0794 | Val Loss: 1.7979, Val Char Acc: 0.4954, Val Word Acc: 0.0957 | Time: 25.3s\n",
            "Epoch 06 | Train Loss: 1.5784, Train Char Acc: 0.5750, Train Word Acc: 0.1077 | Val Loss: 1.7275, Val Char Acc: 0.5229, Val Word Acc: 0.1145 | Time: 25.3s\n",
            "Epoch 07 | Train Loss: 1.4847, Train Char Acc: 0.6091, Train Word Acc: 0.1367 | Val Loss: 1.6735, Val Char Acc: 0.5546, Val Word Acc: 0.1317 | Time: 25.3s\n",
            "Epoch 08 | Train Loss: 1.4108, Train Char Acc: 0.6378, Train Word Acc: 0.1643 | Val Loss: 1.6100, Val Char Acc: 0.5746, Val Word Acc: 0.1538 | Time: 25.3s\n",
            "Epoch 09 | Train Loss: 1.3514, Train Char Acc: 0.6607, Train Word Acc: 0.1896 | Val Loss: 1.5853, Val Char Acc: 0.5863, Val Word Acc: 0.1606 | Time: 25.4s\n",
            "Epoch 10 | Train Loss: 1.2988, Train Char Acc: 0.6815, Train Word Acc: 0.2138 | Val Loss: 1.5550, Val Char Acc: 0.6023, Val Word Acc: 0.1811 | Time: 25.7s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.003 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec █▂▂▁▃▂▃▃▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▃▄▅▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▄▃▃▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▄▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▃▃▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 25.71371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.68149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.29878\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.21377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.60234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.55501\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.18105\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers3x1_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/nujwl52c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_160252-nujwl52c/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 16:07:19,815 - wandb.wandb_agent - INFO - Cleaning up finished run: nujwl52c\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
          ]
        }
      ],
      "source": [
        "!wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/lv5aj1ns --count 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30A_wUwI0yZZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
