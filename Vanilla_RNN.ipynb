{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > main.py << 'EOF'\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "# At startup, *before* creating any models/loaders:\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "\n",
        "# ─── CharVocab & Dataset ─────────────────────────────────────────────────────\n",
        "class CharVocab:\n",
        "    def __init__(self, filepaths: List[str]):\n",
        "        self.rom_char2idx: Dict[str,int] = {}\n",
        "        self.dev_char2idx: Dict[str,int] = {}\n",
        "        self.rom_idx2char: Dict[int,str] = {}\n",
        "        self.dev_idx2char: Dict[int,str] = {}\n",
        "        self._build_vocab(filepaths)\n",
        "\n",
        "    def _build_vocab(self, filepaths: List[str]):\n",
        "        rom_chars = set()\n",
        "        dev_chars = set()\n",
        "        for fp in filepaths:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f, delimiter=\"\\t\")\n",
        "                for row in reader:\n",
        "                    if len(row) < 2:\n",
        "                        continue\n",
        "                    devanagari = row[0].strip()\n",
        "                    roman      = row[1].strip()\n",
        "                    rom_chars.update(list(roman))\n",
        "                    dev_chars.update(list(devanagari))\n",
        "\n",
        "        PAD, SOS, EOS = \"<pad>\", \"<sos>\", \"<eos>\"\n",
        "\n",
        "        all_rom = [PAD, SOS, EOS] + sorted(rom_chars)\n",
        "        for i, ch in enumerate(all_rom):\n",
        "            self.rom_char2idx[ch] = i\n",
        "            self.rom_idx2char[i] = ch\n",
        "\n",
        "        all_dev = [PAD, SOS, EOS] + sorted(dev_chars)\n",
        "        for i, ch in enumerate(all_dev):\n",
        "            self.dev_char2idx[ch] = i\n",
        "            self.dev_idx2char[i] = ch\n",
        "\n",
        "        self.rom_pad_idx = self.rom_char2idx[PAD]\n",
        "        self.rom_sos_idx = self.rom_char2idx[SOS]\n",
        "        self.rom_eos_idx = self.rom_char2idx[EOS]\n",
        "\n",
        "        self.dev_pad_idx = self.dev_char2idx[PAD]\n",
        "        self.dev_sos_idx = self.dev_char2idx[SOS]\n",
        "        self.dev_eos_idx = self.dev_char2idx[EOS]\n",
        "\n",
        "    # ─── Add these two properties ────────────────────────────────────────────\n",
        "    @property\n",
        "    def rom_vocab_size(self) -> int:\n",
        "        return len(self.rom_char2idx)\n",
        "\n",
        "    @property\n",
        "    def dev_vocab_size(self) -> int:\n",
        "        return len(self.dev_char2idx)\n",
        "    # ───────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    def roman_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.rom_sos_idx] + [self.rom_char2idx[ch] for ch in s] + [self.rom_eos_idx]\n",
        "\n",
        "    def dev_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.dev_sos_idx] + [self.dev_char2idx[ch] for ch in s] + [self.dev_eos_idx]\n",
        "\n",
        "    def indices_to_dev(self, idxs: List[int]) -> str:\n",
        "        chars = []\n",
        "        for i in idxs:\n",
        "            if i in (self.dev_sos_idx, self.dev_eos_idx, self.dev_pad_idx):\n",
        "                continue\n",
        "            chars.append(self.dev_idx2char[i])\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "\n",
        "def read_tsv(path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Expects each line of the TSV to be:\n",
        "      Devanagari_word    Roman_word    <something_to_ignore>\n",
        "    We only need (Roman, Devanagari) for training.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            # If there are fewer than 2 columns, skip\n",
        "            if len(row) < 2:\n",
        "                continue\n",
        "\n",
        "            # Unpack: Dev is first column, Roman is second, ignore anything else\n",
        "            devana = row[0].strip()\n",
        "            roman  = row[1].strip()\n",
        "\n",
        "            if not roman or not devana:\n",
        "                continue\n",
        "            # Append (roman, devana)—this matches our CharVocab convention\n",
        "            pairs.append((roman, devana))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, filepath, vocab):\n",
        "        super().__init__()\n",
        "        self.pairs = read_tsv(filepath)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        roman, devanagari = self.pairs[idx]\n",
        "        roman_idxs = self.vocab.roman_to_indices(roman)\n",
        "        dev_idxs = self.vocab.dev_to_indices(devanagari)\n",
        "        return torch.tensor(roman_idxs, dtype=torch.long), torch.tensor(dev_idxs, dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        roman_seqs, dev_seqs = zip(*batch)\n",
        "        max_rom_len = max(len(x) for x in roman_seqs)\n",
        "        max_dev_len = max(len(x) for x in dev_seqs)\n",
        "        rom_padded = []\n",
        "        dev_padded = []\n",
        "        for r, d in zip(roman_seqs, dev_seqs):\n",
        "            pad_r = torch.cat([r, r.new_full((max_rom_len - len(r),), r.new_tensor(0))])\n",
        "            pad_d = torch.cat([d, d.new_full((max_dev_len - len(d),), d.new_tensor(0))])\n",
        "            rom_padded.append(pad_r)\n",
        "            dev_padded.append(pad_d)\n",
        "        return torch.stack(rom_padded), torch.stack(dev_padded)\n",
        "\n",
        "\n",
        "# ─── Encoder & Decoder ────────────────────────────────────────────────────────\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(input_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (h_n, c_n) = self.rnn(emb)\n",
        "            return outputs, (h_n, c_n)\n",
        "        else:\n",
        "            outputs, h_n = self.rnn(emb)\n",
        "            return outputs, h_n\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(output_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, tgt_seq, hidden, cell=None, teacher_forcing_ratio=0.0):\n",
        "        B, T = tgt_seq.size()\n",
        "        outputs = torch.zeros(B, T, self.out.out_features, device=tgt_seq.device)\n",
        "        input_step = tgt_seq[:, 0]\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            # hidden is h_n, and `cell` is c_n\n",
        "            h = hidden       # h_n: (num_layers, B, hidden_size)\n",
        "            c = cell         # c_n: (num_layers, B, hidden_size)\n",
        "        else:\n",
        "            # hidden is the single tensor from RNN/GRU; no cell\n",
        "            h = hidden       # (num_layers, B, hidden_size)\n",
        "            c = None\n",
        "\n",
        "        for t in range(1, T):\n",
        "            emb_t = self.embed(input_step).unsqueeze(1)\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                out_step, (h, c) = self.rnn(emb_t, (h, c))\n",
        "            else:\n",
        "                out_step, h = self.rnn(emb_t, h)\n",
        "            logits = self.out(out_step.squeeze(1))\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "            top1 = logits.argmax(dim=1)\n",
        "            next_input = tgt_seq[:, t] if teacher_force else top1\n",
        "            input_step = next_input.view(-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_decode(self, encoder_hidden, encoder_cell, max_len, dev_sos_idx, dev_eos_idx, beam_size):\n",
        "        hidden, cell = encoder_hidden, encoder_cell\n",
        "        Hyp = lambda seq, h, c, scr: (seq, h, c, scr)\n",
        "        live = [( [dev_sos_idx], hidden, cell, 0.0 )]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_hyps = []\n",
        "            for (seq, h, c, score) in live:\n",
        "                last_token = seq[-1]\n",
        "                if last_token == dev_eos_idx:\n",
        "                    completed.append((seq, h, c, score))\n",
        "                    continue\n",
        "\n",
        "                inp = torch.tensor([last_token], dtype=torch.long, device=h.device).unsqueeze(0)\n",
        "                emb_t = self.embed(inp)\n",
        "\n",
        "                if self.cell_type == \"LSTM\":\n",
        "                    out_t, (h2, c2) = self.rnn(emb_t, (h, c))\n",
        "                else:\n",
        "                    out_t, h2 = self.rnn(emb_t, h)\n",
        "                    c2 = None\n",
        "\n",
        "                logits = self.out(out_t.squeeze(1))\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                topk_logprobs, topk_indices = torch.topk(log_probs, k=beam_size, dim=1)\n",
        "                topk_logprobs = topk_logprobs.squeeze(0).tolist()\n",
        "                topk_indices = topk_indices.squeeze(0).tolist()\n",
        "                for lp, idx in zip(topk_logprobs, topk_indices):\n",
        "                    new_hyps.append(( seq + [idx], h2, c2, score + lp ))\n",
        "\n",
        "            new_hyps = sorted(new_hyps, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "            live = new_hyps\n",
        "            if all((hyp[0][-1] == dev_eos_idx) for hyp in live):\n",
        "                completed.extend(live)\n",
        "                break\n",
        "\n",
        "        if not completed:\n",
        "            completed = live\n",
        "        best = max(completed, key=lambda x: x[3])\n",
        "        return best[0]\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: Encoder,\n",
        "        decoder: Decoder,\n",
        "        device: torch.device,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        tgt: torch.Tensor,\n",
        "        teacher_forcing_ratio: float = 0.5,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        src: (B, T_src)\n",
        "        tgt: (B, T_tgt)   # including <sos> … <eos>\n",
        "        Returns:\n",
        "          logits: (B, T_tgt, V_out)\n",
        "        \"\"\"\n",
        "        B, T_src = src.size()\n",
        "        # Prepare a tensor to hold all decoder logits\n",
        "        outputs = torch.zeros(\n",
        "            B, tgt.size(1), self.decoder.out.out_features, device=self.device\n",
        "        )\n",
        "\n",
        "        # 1) Run the encoder\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            enc_outputs, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            enc_outputs, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # 2) Transform encoder's hidden‐state to match decoder layers\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            # h_n, c_n each have shape (enc_layers, B, hidden_size)\n",
        "            if enc_layers >= dec_layers:\n",
        "                # Take the top-most `dec_layers` layers from encoder\n",
        "                h_dec = h_n[-dec_layers:]              # shape: (dec_layers, B, H)\n",
        "                c_dec = c_n[-dec_layers:]              # shape: (dec_layers, B, H)\n",
        "            else:\n",
        "                # enc_layers < dec_layers → prepend zeros to match dec_layers\n",
        "                num_missing = dec_layers - enc_layers   # how many extra layers decoder wants\n",
        "                zeros_h = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                zeros_c = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                # concatenate: (num_missing, B, H) + (enc_layers, B, H) → (dec_layers, B, H)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = c_dec\n",
        "\n",
        "        else:\n",
        "            # RNN or GRU case: h_n has shape (enc_layers, B, hidden_size)\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]   # just take the top dec_layers\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = None\n",
        "\n",
        "        # 3) Run the decoder (training mode, with teacher_forcing_ratio)\n",
        "        logits = self.decoder(\n",
        "            tgt_seq=tgt,\n",
        "            hidden=dec_hidden,\n",
        "            cell=dec_cell,\n",
        "            teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "        )\n",
        "\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        max_len: int,\n",
        "        dev_sos_idx: int,\n",
        "        dev_eos_idx: int,\n",
        "        beam_size: int = 1,\n",
        "    ) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Greedy (beam_size=1) or beam search decoding for a batch of size=1. Returns list of decoded sequences.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        # Assume batch_size=1\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            _, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            _, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # Now transform encoder’s (h_n, c_n) → (dec_hidden, dec_cell) exactly as above:\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "        B = 1  # we only support batch=1 in predict()\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = None\n",
        "\n",
        "        # 4) Now do greedy or beam search decode using (hidden_state, cell_state)\n",
        "        if beam_size == 1:\n",
        "            seq = [dev_sos_idx]\n",
        "            hidden_ = hidden_state\n",
        "            cell_ = cell_state\n",
        "            for _ in range(max_len):\n",
        "                last_token = torch.tensor([seq[-1]], dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "                emb = self.decoder.embed(last_token)  # (1,1,embed_dim)\n",
        "\n",
        "                if self.decoder.cell_type == \"LSTM\":\n",
        "                    out, (h_next, c_next) = self.decoder.rnn(emb, (hidden_, cell_))\n",
        "                    hidden_, cell_ = h_next, c_next\n",
        "                else:\n",
        "                    out, h_next = self.decoder.rnn(emb, hidden_)\n",
        "                    hidden_, cell_ = h_next, None\n",
        "\n",
        "                logits = self.decoder.out(out.squeeze(1))  # (1, V_out)\n",
        "                next_token = logits.argmax(dim=1).item()\n",
        "                seq.append(next_token)\n",
        "                if next_token == dev_eos_idx:\n",
        "                    break\n",
        "            return [seq]\n",
        "\n",
        "        else:\n",
        "            best_seq = self.decoder.beam_search_decode(\n",
        "                encoder_hidden=hidden_state,\n",
        "                encoder_cell=cell_state,\n",
        "                max_len=max_len,\n",
        "                dev_sos_idx=dev_sos_idx,\n",
        "                dev_eos_idx=dev_eos_idx,\n",
        "                beam_size=beam_size,\n",
        "            )\n",
        "            return [best_seq]\n",
        "\n",
        "\n",
        "\n",
        "# ─── train / evaluate ────────────────────────────────────────────────────────\n",
        "def char_accuracy(logits, target, pad_idx):\n",
        "    with torch.no_grad():\n",
        "        pred = logits.argmax(dim=2)\n",
        "        mask = (target != pad_idx)\n",
        "        correct = (pred == target) & mask\n",
        "        total = mask.sum().item()\n",
        "        return correct.sum().item() / max(total, 1)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    device: torch.device,\n",
        "    teacher_forcing_ratio: float,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Runs one epoch of training, returning (train_loss, train_char_acc, train_word_acc).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_char_acc = 0.0\n",
        "    epoch_word_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(device, non_blocking=True)  # (B, T_src)\n",
        "        tgt = tgt.to(device, non_blocking=True)  # (B, T_tgt)\n",
        "        B, T_tgt = tgt.size()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output_logits = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        # output_logits: (B, T_tgt, V_out)\n",
        "\n",
        "        # 1) Compute token‐level (character) loss & accuracy\n",
        "        V = output_logits.size(-1)\n",
        "        loss = criterion(output_logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "        # char‐level accuracy (ignore pad)\n",
        "        with torch.no_grad():\n",
        "            pred_inds = output_logits.argmax(dim=2)      # (B, T_tgt)\n",
        "            char_mask = (tgt != pad_idx)                  # (B, T_tgt)\n",
        "            char_correct = ((pred_inds == tgt) & char_mask).sum().item()\n",
        "            char_total = char_mask.sum().item()\n",
        "            batch_char_acc = char_correct / max(char_total, 1)\n",
        "\n",
        "        # 2) Compute word‐level accuracy: count how many sequences match exactly (ignoring pad)\n",
        "        with torch.no_grad():\n",
        "            # For each example b, we want (pred_inds[b, t] == tgt[b, t]) for ALL t where tgt[b, t] != pad_idx.\n",
        "            # We can OR with (~mask) on both sides:\n",
        "            #   (pred_inds == tgt) | (~char_mask)  → True for any pad position\n",
        "            # Then check .all(dim=1).\n",
        "            match_or_pad = (pred_inds == tgt) | (~char_mask)    # (B, T_tgt) boolean\n",
        "            exact_matches = match_or_pad.all(dim=1)             # (B,) boolean: True if all positions match or are pad\n",
        "            batch_word_acc = exact_matches.sum().item() / B\n",
        "\n",
        "        # Backward + step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_char_acc += batch_char_acc\n",
        "        epoch_word_acc += batch_word_acc\n",
        "        total_batches += 1\n",
        "\n",
        "    return (\n",
        "        epoch_loss / total_batches,\n",
        "        epoch_char_acc / total_batches,\n",
        "        epoch_word_acc / total_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    device: torch.device,\n",
        "    beam_size: int,\n",
        "    max_dev_len: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    One validation pass (no teacher forcing). Returns (val_loss, val_char_acc, val_word_acc).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_char_acc = 0.0\n",
        "    epoch_word_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "            src = src.to(device, non_blocking=True)\n",
        "            tgt = tgt.to(device, non_blocking=True)     # (B, T_tgt)\n",
        "            B, T_tgt = tgt.size()\n",
        "\n",
        "            # 1) Compute loss by feeding gold tgt through decoder with teacher_forcing=0\n",
        "            logits = model(src, tgt, teacher_forcing_ratio=0.0)  # (B, T_tgt, V_out)\n",
        "            V = logits.size(-1)\n",
        "            loss = criterion(logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "            # 2) Character‐level accuracy (token‐level)\n",
        "            pred_inds = logits.argmax(dim=2)           # (B, T_tgt)\n",
        "            char_mask = (tgt != pad_idx)               # (B, T_tgt)\n",
        "            char_correct = ((pred_inds == tgt) & char_mask).sum().item()\n",
        "            char_total = char_mask.sum().item()\n",
        "            batch_char_acc = char_correct / max(char_total, 1)\n",
        "\n",
        "            # 3) Word‐level accuracy on this batch using simple greedy decode\n",
        "            # (Note: you could also use beam_search here if you want word-acc under beam search.\n",
        "            #  For simplicity, we’ll just use the greedy `pred_inds` we already have.)\n",
        "            match_or_pad = (pred_inds == tgt) | (~char_mask)  # True if match or tgt is pad\n",
        "            exact_matches = match_or_pad.all(dim=1)           # (B,) boolean\n",
        "            batch_word_acc = exact_matches.sum().item() / B\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_char_acc += batch_char_acc\n",
        "            epoch_word_acc += batch_word_acc\n",
        "            total_batches += 1\n",
        "\n",
        "    return (\n",
        "        epoch_loss / total_batches,\n",
        "        epoch_char_acc / total_batches,\n",
        "        epoch_word_acc / total_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# ─── main() with parse_known_args ─────────────────────────────────────────────\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content\")\n",
        "    parser.add_argument(\"--train_file\", type=str, default=\"bn.translit.sampled.train.tsv\")\n",
        "    parser.add_argument(\"--dev_file\", type=str, default=\"bn.translit.sampled.dev.tsv\")\n",
        "    parser.add_argument(\"--test_file\", type=str, default=\"bn.translit.sampled.test.tsv\")\n",
        "    parser.add_argument(\"--emb_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--enc_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--dec_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--cell_type\", type=str, default=\"RNN\", choices=[\"RNN\", \"GRU\", \"LSTM\"])\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--tf_ratio\", type=float, default=0.5)\n",
        "    parser.add_argument(\"--beam_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--max_dev_len\", type=int, default=32)\n",
        "    parser.add_argument(\"--project_name\", type=str, default=\"Vanilla_RNN\")\n",
        "    parser.add_argument(\"--run_name\", type=str, default=None)\n",
        "\n",
        "    # ── use parse_known_args to ignore Colab’s extra \"-f …json\"\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_path = os.path.join(args.data_dir, args.train_file)\n",
        "    dev_path   = os.path.join(args.data_dir, args.dev_file)\n",
        "    test_path  = os.path.join(args.data_dir, args.test_file)\n",
        "    vocab = CharVocab([train_path, dev_path, test_path])\n",
        "\n",
        "    train_ds = TransliterationDataset(train_path, vocab)\n",
        "    dev_ds   = TransliterationDataset(dev_path, vocab)\n",
        "    test_ds  = TransliterationDataset(test_path, vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "    dev_loader   = DataLoader(dev_ds,   batch_size=args.batch_size, shuffle=False, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.project_name,\n",
        "        name=args.run_name,\n",
        "        config={\n",
        "            \"emb_size\": args.emb_size,\n",
        "            \"hidden_size\": args.hidden_size,\n",
        "            \"enc_layers\": args.enc_layers,\n",
        "            \"dec_layers\": args.dec_layers,\n",
        "            \"cell_type\": args.cell_type,\n",
        "            \"dropout\": args.dropout,\n",
        "            \"lr\": args.lr,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"epochs\": args.epochs,\n",
        "            \"tf_ratio\": args.tf_ratio,\n",
        "            \"beam_size\": args.beam_size,\n",
        "        },\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_vocab_size=vocab.rom_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.enc_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    decoder = Decoder(\n",
        "        output_vocab_size=vocab.dev_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.dec_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.dev_pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1) Unpack three values from train_one_epoch:\n",
        "        train_loss, train_char_acc, train_word_acc = train_one_epoch(\n",
        "            model=model,\n",
        "            iterator=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            pad_idx=vocab.dev_pad_idx,\n",
        "            device=device,\n",
        "            teacher_forcing_ratio=config.tf_ratio,\n",
        "        )\n",
        "\n",
        "        # 2) Unpack three values from evaluate:\n",
        "        val_loss, val_char_acc, val_word_acc = evaluate(\n",
        "            model=model,\n",
        "            iterator=dev_loader,\n",
        "            criterion=criterion,\n",
        "            pad_idx=vocab.dev_pad_idx,\n",
        "            device=device,\n",
        "            beam_size=config.beam_size,\n",
        "            max_dev_len=args.max_dev_len,\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # 3) Decide “best” model based on whichever metric you prefer (e.g. val_char_acc)\n",
        "        if val_char_acc > best_val_acc:\n",
        "            best_val_acc = val_char_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "        # 4) Log all four accuracy metrics to W&B\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_char_acc\": train_char_acc,\n",
        "                \"train_word_acc\": train_word_acc,   # newly added\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_char_acc\": val_char_acc,\n",
        "                \"val_word_acc\": val_word_acc,       # newly added\n",
        "                \"epoch_time_sec\": elapsed,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 5) Print so you see them in Colab output\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, \"\n",
        "            f\"Train Char Acc: {train_char_acc:.4f}, \"\n",
        "            f\"Train Word Acc: {train_word_acc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Char Acc: {val_char_acc:.4f}, \"\n",
        "            f\"Val Word Acc: {val_word_acc:.4f} | \"\n",
        "            f\"Time: {elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "EOF"
      ],
      "metadata": {
        "id": "YLbBP7SWshbz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > sweep.yaml << 'EOF'\n",
        "program: main.py\n",
        "method: random\n",
        "project: Vanilla_RNN\n",
        "entity: mrsagarbiswas-iit-madras\n",
        "metric:\n",
        "  name: val_char_acc\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  emb_size:\n",
        "    values: [16, 32, 64, 256]\n",
        "  hidden_size:\n",
        "    values: [16, 32, 64, 256]\n",
        "  enc_layers:\n",
        "    values: [1, 2, 3]\n",
        "  epochs:\n",
        "    values: [5, 10, 15]\n",
        "  dec_layers:\n",
        "    values: [1, 2, 3]\n",
        "  cell_type:\n",
        "    values: [\"RNN\", \"GRU\", \"LSTM\"]\n",
        "  dropout:\n",
        "    values: [0.2, 0.3]\n",
        "  lr:\n",
        "    values: [1e-3, 1e-4]\n",
        "  batch_size:\n",
        "    values: [32, 64, 128]\n",
        "  tf_ratio:\n",
        "    values: [0.3, 0.5, 0.7]\n",
        "  beam_size:\n",
        "    values: [1, 3]\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "IHjg2NT0si4x"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuSvKgqQs66p",
        "outputId": "26809045-c34b-4031-a247-28114a1c3432"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Currently logged in as: mrsagarbiswas (mrsagarbiswas-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "SWEEP_ID=$(wandb sweep sweep.yaml)\n",
        "echo \"Sweep ID = $SWEEP_ID\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcBoXK-jtPBr",
        "outputId": "04f28a86-5eee-4395-ad27-3e24f1119c2e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sweep ID = \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Creating sweep from: sweep.yaml\n",
            "wandb: Creating sweep with ID: wpzjutzc\n",
            "wandb: View sweep at: https://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/wpzjutzc\n",
            "wandb: Run sweep agent with: wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/wpzjutzc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/wpzjutzc --count 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc33AXMhyJpg",
        "outputId": "573d0f27-d563-4f76-ab3e-85b7c64d028e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2025-05-18 20:01:04,306 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2025-05-18 20:01:04,648 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 20:01:04,648 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: RNN\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 16\n",
            "\tenc_layers: 2\n",
            "\tepochs: 15\n",
            "\thidden_size: 32\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-18 20:01:04,650 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=RNN --dec_layers=2 --dropout=0.2 --emb_size=16 --enc_layers=2 --epochs=15 --hidden_size=32 --lr=0.0001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-18 20:01:09,661 - wandb.wandb_agent - INFO - Running runs: ['dise9ble']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_200109-dise9ble\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfrosty-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/wpzjutzc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/dise9ble\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.5481, Train Char Acc: 0.1265, Train Word Acc: 0.0000 | Val Loss: 3.2829, Val Char Acc: 0.1712, Val Word Acc: 0.0000 | Time: 22.5s\n",
            "Epoch 02 | Train Loss: 3.1994, Train Char Acc: 0.1751, Train Word Acc: 0.0000 | Val Loss: 3.2433, Val Char Acc: 0.1717, Val Word Acc: 0.0000 | Time: 22.2s\n",
            "Epoch 03 | Train Loss: 3.1057, Train Char Acc: 0.1847, Train Word Acc: 0.0000 | Val Loss: 3.2434, Val Char Acc: 0.1717, Val Word Acc: 0.0000 | Time: 22.1s\n",
            "Epoch 04 | Train Loss: 3.0398, Train Char Acc: 0.1962, Train Word Acc: 0.0000 | Val Loss: 3.2924, Val Char Acc: 0.1497, Val Word Acc: 0.0000 | Time: 22.1s\n",
            "Epoch 05 | Train Loss: 2.9883, Train Char Acc: 0.2097, Train Word Acc: 0.0000 | Val Loss: 3.3095, Val Char Acc: 0.1443, Val Word Acc: 0.0000 | Time: 21.7s\n",
            "Epoch 06 | Train Loss: 2.9658, Train Char Acc: 0.2141, Train Word Acc: 0.0000 | Val Loss: 3.2814, Val Char Acc: 0.1538, Val Word Acc: 0.0000 | Time: 22.1s\n",
            "Epoch 07 | Train Loss: 2.9462, Train Char Acc: 0.2180, Train Word Acc: 0.0000 | Val Loss: 3.2702, Val Char Acc: 0.1576, Val Word Acc: 0.0000 | Time: 22.2s\n",
            "Epoch 08 | Train Loss: 2.9286, Train Char Acc: 0.2216, Train Word Acc: 0.0000 | Val Loss: 3.2207, Val Char Acc: 0.1757, Val Word Acc: 0.0000 | Time: 22.1s\n",
            "Epoch 09 | Train Loss: 2.9104, Train Char Acc: 0.2249, Train Word Acc: 0.0000 | Val Loss: 3.2167, Val Char Acc: 0.1753, Val Word Acc: 0.0000 | Time: 24.3s\n",
            "Epoch 10 | Train Loss: 2.8938, Train Char Acc: 0.2288, Train Word Acc: 0.0000 | Val Loss: 3.2100, Val Char Acc: 0.1761, Val Word Acc: 0.0000 | Time: 21.6s\n",
            "Epoch 11 | Train Loss: 2.8834, Train Char Acc: 0.2313, Train Word Acc: 0.0000 | Val Loss: 3.1891, Val Char Acc: 0.1820, Val Word Acc: 0.0000 | Time: 22.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30A_wUwI0yZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}