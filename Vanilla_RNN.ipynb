{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > main.py << 'EOF'\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "# At startup, *before* creating any models/loaders:\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "\n",
        "# ─── CharVocab & Dataset ─────────────────────────────────────────────────────\n",
        "class CharVocab:\n",
        "    def __init__(self, filepaths: List[str]):\n",
        "        self.rom_char2idx: Dict[str,int] = {}\n",
        "        self.dev_char2idx: Dict[str,int] = {}\n",
        "        self.rom_idx2char: Dict[int,str] = {}\n",
        "        self.dev_idx2char: Dict[int,str] = {}\n",
        "        self._build_vocab(filepaths)\n",
        "\n",
        "    def _build_vocab(self, filepaths: List[str]):\n",
        "        rom_chars = set()\n",
        "        dev_chars = set()\n",
        "        for fp in filepaths:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f, delimiter=\"\\t\")\n",
        "                for row in reader:\n",
        "                    if len(row) < 2:\n",
        "                        continue\n",
        "                    devanagari = row[0].strip()\n",
        "                    roman      = row[1].strip()\n",
        "                    rom_chars.update(list(roman))\n",
        "                    dev_chars.update(list(devanagari))\n",
        "\n",
        "        PAD, SOS, EOS = \"<pad>\", \"<sos>\", \"<eos>\"\n",
        "\n",
        "        all_rom = [PAD, SOS, EOS] + sorted(rom_chars)\n",
        "        for i, ch in enumerate(all_rom):\n",
        "            self.rom_char2idx[ch] = i\n",
        "            self.rom_idx2char[i] = ch\n",
        "\n",
        "        all_dev = [PAD, SOS, EOS] + sorted(dev_chars)\n",
        "        for i, ch in enumerate(all_dev):\n",
        "            self.dev_char2idx[ch] = i\n",
        "            self.dev_idx2char[i] = ch\n",
        "\n",
        "        self.rom_pad_idx = self.rom_char2idx[PAD]\n",
        "        self.rom_sos_idx = self.rom_char2idx[SOS]\n",
        "        self.rom_eos_idx = self.rom_char2idx[EOS]\n",
        "\n",
        "        self.dev_pad_idx = self.dev_char2idx[PAD]\n",
        "        self.dev_sos_idx = self.dev_char2idx[SOS]\n",
        "        self.dev_eos_idx = self.dev_char2idx[EOS]\n",
        "\n",
        "    # ─── Add these two properties ────────────────────────────────────────────\n",
        "    @property\n",
        "    def rom_vocab_size(self) -> int:\n",
        "        return len(self.rom_char2idx)\n",
        "\n",
        "    @property\n",
        "    def dev_vocab_size(self) -> int:\n",
        "        return len(self.dev_char2idx)\n",
        "    # ───────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    def roman_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.rom_sos_idx] + [self.rom_char2idx[ch] for ch in s] + [self.rom_eos_idx]\n",
        "\n",
        "    def dev_to_indices(self, s: str) -> List[int]:\n",
        "        return [self.dev_sos_idx] + [self.dev_char2idx[ch] for ch in s] + [self.dev_eos_idx]\n",
        "\n",
        "    def indices_to_dev(self, idxs: List[int]) -> str:\n",
        "        chars = []\n",
        "        for i in idxs:\n",
        "            if i in (self.dev_sos_idx, self.dev_eos_idx, self.dev_pad_idx):\n",
        "                continue\n",
        "            chars.append(self.dev_idx2char[i])\n",
        "        return \"\".join(chars)\n",
        "\n",
        "\n",
        "\n",
        "def read_tsv(path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Expects each line of the TSV to be:\n",
        "      Devanagari_word    Roman_word    <something_to_ignore>\n",
        "    We only need (Roman, Devanagari) for training.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            # If there are fewer than 2 columns, skip\n",
        "            if len(row) < 2:\n",
        "                continue\n",
        "\n",
        "            # Unpack: Dev is first column, Roman is second, ignore anything else\n",
        "            devana = row[0].strip()\n",
        "            roman  = row[1].strip()\n",
        "\n",
        "            if not roman or not devana:\n",
        "                continue\n",
        "            # Append (roman, devana)—this matches our CharVocab convention\n",
        "            pairs.append((roman, devana))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, filepath, vocab):\n",
        "        super().__init__()\n",
        "        self.pairs = read_tsv(filepath)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        roman, devanagari = self.pairs[idx]\n",
        "        roman_idxs = self.vocab.roman_to_indices(roman)\n",
        "        dev_idxs = self.vocab.dev_to_indices(devanagari)\n",
        "        return torch.tensor(roman_idxs, dtype=torch.long), torch.tensor(dev_idxs, dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        roman_seqs, dev_seqs = zip(*batch)\n",
        "        max_rom_len = max(len(x) for x in roman_seqs)\n",
        "        max_dev_len = max(len(x) for x in dev_seqs)\n",
        "        rom_padded = []\n",
        "        dev_padded = []\n",
        "        for r, d in zip(roman_seqs, dev_seqs):\n",
        "            pad_r = torch.cat([r, r.new_full((max_rom_len - len(r),), r.new_tensor(0))])\n",
        "            pad_d = torch.cat([d, d.new_full((max_dev_len - len(d),), d.new_tensor(0))])\n",
        "            rom_padded.append(pad_r)\n",
        "            dev_padded.append(pad_d)\n",
        "        return torch.stack(rom_padded), torch.stack(dev_padded)\n",
        "\n",
        "\n",
        "# ─── Encoder & Decoder ────────────────────────────────────────────────────────\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(input_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (h_n, c_n) = self.rnn(emb)\n",
        "            return outputs, (h_n, c_n)\n",
        "        else:\n",
        "            outputs, h_n = self.rnn(emb)\n",
        "            return outputs, h_n\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_size, num_layers, cell_type, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(output_vocab_size, embed_dim, padding_idx=0)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.upper()\n",
        "        if self.cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_dim, hidden_size, num_layers=num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0.0\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, tgt_seq, hidden, cell=None, teacher_forcing_ratio=0.0):\n",
        "        B, T = tgt_seq.size()\n",
        "        outputs = torch.zeros(B, T, self.out.out_features, device=tgt_seq.device)\n",
        "        input_step = tgt_seq[:, 0]\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            # hidden is h_n, and `cell` is c_n\n",
        "            h = hidden       # h_n: (num_layers, B, hidden_size)\n",
        "            c = cell         # c_n: (num_layers, B, hidden_size)\n",
        "        else:\n",
        "            # hidden is the single tensor from RNN/GRU; no cell\n",
        "            h = hidden       # (num_layers, B, hidden_size)\n",
        "            c = None\n",
        "\n",
        "        for t in range(1, T):\n",
        "            emb_t = self.embed(input_step).unsqueeze(1)\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                out_step, (h, c) = self.rnn(emb_t, (h, c))\n",
        "            else:\n",
        "                out_step, h = self.rnn(emb_t, h)\n",
        "            logits = self.out(out_step.squeeze(1))\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "            top1 = logits.argmax(dim=1)\n",
        "            next_input = tgt_seq[:, t] if teacher_force else top1\n",
        "            input_step = next_input.view(-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_decode(self, encoder_hidden, encoder_cell, max_len, dev_sos_idx, dev_eos_idx, beam_size):\n",
        "        hidden, cell = encoder_hidden, encoder_cell\n",
        "        Hyp = lambda seq, h, c, scr: (seq, h, c, scr)\n",
        "        live = [( [dev_sos_idx], hidden, cell, 0.0 )]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_hyps = []\n",
        "            for (seq, h, c, score) in live:\n",
        "                last_token = seq[-1]\n",
        "                if last_token == dev_eos_idx:\n",
        "                    completed.append((seq, h, c, score))\n",
        "                    continue\n",
        "\n",
        "                inp = torch.tensor([last_token], dtype=torch.long, device=h.device).unsqueeze(0)\n",
        "                emb_t = self.embed(inp)\n",
        "\n",
        "                if self.cell_type == \"LSTM\":\n",
        "                    out_t, (h2, c2) = self.rnn(emb_t, (h, c))\n",
        "                else:\n",
        "                    out_t, h2 = self.rnn(emb_t, h)\n",
        "                    c2 = None\n",
        "\n",
        "                logits = self.out(out_t.squeeze(1))\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                topk_logprobs, topk_indices = torch.topk(log_probs, k=beam_size, dim=1)\n",
        "                topk_logprobs = topk_logprobs.squeeze(0).tolist()\n",
        "                topk_indices = topk_indices.squeeze(0).tolist()\n",
        "                for lp, idx in zip(topk_logprobs, topk_indices):\n",
        "                    new_hyps.append(( seq + [idx], h2, c2, score + lp ))\n",
        "\n",
        "            new_hyps = sorted(new_hyps, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "            live = new_hyps\n",
        "            if all((hyp[0][-1] == dev_eos_idx) for hyp in live):\n",
        "                completed.extend(live)\n",
        "                break\n",
        "\n",
        "        if not completed:\n",
        "            completed = live\n",
        "        best = max(completed, key=lambda x: x[3])\n",
        "        return best[0]\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: Encoder,\n",
        "        decoder: Decoder,\n",
        "        device: torch.device,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        tgt: torch.Tensor,\n",
        "        teacher_forcing_ratio: float = 0.5,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        src: (B, T_src)\n",
        "        tgt: (B, T_tgt)   # including <sos> … <eos>\n",
        "        Returns:\n",
        "          logits: (B, T_tgt, V_out)\n",
        "        \"\"\"\n",
        "        B, T_src = src.size()\n",
        "        # Prepare a tensor to hold all decoder logits\n",
        "        outputs = torch.zeros(\n",
        "            B, tgt.size(1), self.decoder.out.out_features, device=self.device\n",
        "        )\n",
        "\n",
        "        # 1) Run the encoder\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            enc_outputs, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            enc_outputs, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # 2) Transform encoder's hidden‐state to match decoder layers\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            # h_n, c_n each have shape (enc_layers, B, hidden_size)\n",
        "            if enc_layers >= dec_layers:\n",
        "                # Take the top-most `dec_layers` layers from encoder\n",
        "                h_dec = h_n[-dec_layers:]              # shape: (dec_layers, B, H)\n",
        "                c_dec = c_n[-dec_layers:]              # shape: (dec_layers, B, H)\n",
        "            else:\n",
        "                # enc_layers < dec_layers → prepend zeros to match dec_layers\n",
        "                num_missing = dec_layers - enc_layers   # how many extra layers decoder wants\n",
        "                zeros_h = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                zeros_c = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                # concatenate: (num_missing, B, H) + (enc_layers, B, H) → (dec_layers, B, H)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = c_dec\n",
        "\n",
        "        else:\n",
        "            # RNN or GRU case: h_n has shape (enc_layers, B, hidden_size)\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]   # just take the top dec_layers\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(\n",
        "                    num_missing, B, hidden_size, device=self.device\n",
        "                )\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            dec_hidden = h_dec\n",
        "            dec_cell = None\n",
        "\n",
        "        # 3) Run the decoder (training mode, with teacher_forcing_ratio)\n",
        "        logits = self.decoder(\n",
        "            tgt_seq=tgt,\n",
        "            hidden=dec_hidden,\n",
        "            cell=dec_cell,\n",
        "            teacher_forcing_ratio=teacher_forcing_ratio,\n",
        "        )\n",
        "\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        max_len: int,\n",
        "        dev_sos_idx: int,\n",
        "        dev_eos_idx: int,\n",
        "        beam_size: int = 1,\n",
        "    ) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Greedy (beam_size=1) or beam search decoding for a batch of size=1. Returns list of decoded sequences.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        # Assume batch_size=1\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            _, (h_n, c_n) = self.encoder(src)\n",
        "        else:\n",
        "            _, h_n = self.encoder(src)\n",
        "            c_n = None\n",
        "\n",
        "        # Now transform encoder’s (h_n, c_n) → (dec_hidden, dec_cell) exactly as above:\n",
        "        enc_layers = self.encoder.num_layers\n",
        "        dec_layers = self.decoder.num_layers\n",
        "        hidden_size = self.encoder.hidden_size\n",
        "        B = 1  # we only support batch=1 in predict()\n",
        "\n",
        "        if self.encoder.cell_type == \"LSTM\":\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "                c_dec = c_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                zeros_c = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "                c_dec = torch.cat([zeros_c, c_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = c_dec\n",
        "        else:\n",
        "            if enc_layers >= dec_layers:\n",
        "                h_dec = h_n[-dec_layers:]\n",
        "            else:\n",
        "                num_missing = dec_layers - enc_layers\n",
        "                zeros_h = torch.zeros(num_missing, B, hidden_size, device=self.device)\n",
        "                h_dec = torch.cat([zeros_h, h_n], dim=0)\n",
        "            hidden_state = h_dec\n",
        "            cell_state = None\n",
        "\n",
        "        # 4) Now do greedy or beam search decode using (hidden_state, cell_state)\n",
        "        if beam_size == 1:\n",
        "            seq = [dev_sos_idx]\n",
        "            hidden_ = hidden_state\n",
        "            cell_ = cell_state\n",
        "            for _ in range(max_len):\n",
        "                last_token = torch.tensor([seq[-1]], dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "                emb = self.decoder.embed(last_token)  # (1,1,embed_dim)\n",
        "\n",
        "                if self.decoder.cell_type == \"LSTM\":\n",
        "                    out, (h_next, c_next) = self.decoder.rnn(emb, (hidden_, cell_))\n",
        "                    hidden_, cell_ = h_next, c_next\n",
        "                else:\n",
        "                    out, h_next = self.decoder.rnn(emb, hidden_)\n",
        "                    hidden_, cell_ = h_next, None\n",
        "\n",
        "                logits = self.decoder.out(out.squeeze(1))  # (1, V_out)\n",
        "                next_token = logits.argmax(dim=1).item()\n",
        "                seq.append(next_token)\n",
        "                if next_token == dev_eos_idx:\n",
        "                    break\n",
        "            return [seq]\n",
        "\n",
        "        else:\n",
        "            best_seq = self.decoder.beam_search_decode(\n",
        "                encoder_hidden=hidden_state,\n",
        "                encoder_cell=cell_state,\n",
        "                max_len=max_len,\n",
        "                dev_sos_idx=dev_sos_idx,\n",
        "                dev_eos_idx=dev_eos_idx,\n",
        "                beam_size=beam_size,\n",
        "            )\n",
        "            return [best_seq]\n",
        "\n",
        "\n",
        "\n",
        "# ─── train / evaluate ────────────────────────────────────────────────────────\n",
        "def char_accuracy(logits, target, pad_idx):\n",
        "    with torch.no_grad():\n",
        "        pred = logits.argmax(dim=2)\n",
        "        mask = (target != pad_idx)\n",
        "        correct = (pred == target) & mask\n",
        "        total = mask.sum().item()\n",
        "        return correct.sum().item() / max(total, 1)\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    device: torch.device,\n",
        "    teacher_forcing_ratio: float,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Runs one epoch of training, returning (train_loss, train_char_acc, train_word_acc).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_char_acc = 0.0\n",
        "    epoch_word_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(device, non_blocking=True)  # (B, T_src)\n",
        "        tgt = tgt.to(device, non_blocking=True)  # (B, T_tgt)\n",
        "        B, T_tgt = tgt.size()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output_logits = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        # output_logits: (B, T_tgt, V_out)\n",
        "\n",
        "        # 1) Compute token‐level (character) loss & accuracy\n",
        "        V = output_logits.size(-1)\n",
        "        loss = criterion(output_logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "        # char‐level accuracy (ignore pad)\n",
        "        with torch.no_grad():\n",
        "            pred_inds = output_logits.argmax(dim=2)      # (B, T_tgt)\n",
        "            char_mask = (tgt != pad_idx)                  # (B, T_tgt)\n",
        "            char_correct = ((pred_inds == tgt) & char_mask).sum().item()\n",
        "            char_total = char_mask.sum().item()\n",
        "            batch_char_acc = char_correct / max(char_total, 1)\n",
        "\n",
        "        # 2) Compute word‐level accuracy: count how many sequences match exactly (ignoring pad)\n",
        "        with torch.no_grad():\n",
        "            # For each example b, we want (pred_inds[b, t] == tgt[b, t]) for ALL t where tgt[b, t] != pad_idx.\n",
        "            # We can OR with (~mask) on both sides:\n",
        "            #   (pred_inds == tgt) | (~char_mask)  → True for any pad position\n",
        "            # Then check .all(dim=1).\n",
        "            # Only consider positions 1..T-1 (skip the SOS token at t=0)\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)\n",
        "\n",
        "            # A word is correct if for all non-pad positions we have pred == tgt\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)\n",
        "            batch_word_acc = exact_matches.sum().item() / B\n",
        "\n",
        "\n",
        "        # Backward + step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_char_acc += batch_char_acc\n",
        "        epoch_word_acc += batch_word_acc\n",
        "        total_batches += 1\n",
        "\n",
        "    return (\n",
        "        epoch_loss / total_batches,\n",
        "        epoch_char_acc / total_batches,\n",
        "        epoch_word_acc / total_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: Seq2Seq,\n",
        "    iterator: DataLoader,\n",
        "    criterion: nn.CrossEntropyLoss,\n",
        "    pad_idx: int,\n",
        "    device: torch.device,\n",
        "    beam_size: int,\n",
        "    max_dev_len: int,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    One validation pass (no teacher forcing). Returns (val_loss, val_char_acc, val_word_acc).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_char_acc = 0.0\n",
        "    epoch_word_acc = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "            src = src.to(device, non_blocking=True)\n",
        "            tgt = tgt.to(device, non_blocking=True)     # (B, T_tgt)\n",
        "            B, T_tgt = tgt.size()\n",
        "\n",
        "            # 1) Compute loss by feeding gold tgt through decoder with teacher_forcing=0\n",
        "            logits = model(src, tgt, teacher_forcing_ratio=0.0)  # (B, T_tgt, V_out)\n",
        "            V = logits.size(-1)\n",
        "            loss = criterion(logits.view(-1, V), tgt.view(-1))\n",
        "\n",
        "            # 2) Character‐level accuracy (token‐level)\n",
        "            pred_inds = logits.argmax(dim=2)           # (B, T_tgt)\n",
        "            char_mask = (tgt != pad_idx)               # (B, T_tgt)\n",
        "            char_correct = ((pred_inds == tgt) & char_mask).sum().item()\n",
        "            char_total = char_mask.sum().item()\n",
        "            batch_char_acc = char_correct / max(char_total, 1)\n",
        "\n",
        "            # 3) Word‐level accuracy on this batch using simple greedy decode\n",
        "            # (Note: you could also use beam_search here if you want word-acc under beam search.\n",
        "            #  For simplicity, we’ll just use the greedy `pred_inds` we already have.)\n",
        "            # 3) Word‐level accuracy: skip the <sos> at t=0 and ignore pads\n",
        "            pred_trim = pred_inds[:, 1:]\n",
        "            tgt_trim  = tgt[:,      1:]\n",
        "            mask_trim = (tgt_trim != pad_idx)    # True for positions we should check\n",
        "\n",
        "            # A sequence is “correct” if for every non‑pad position pred == tgt\n",
        "            match_trim    = (pred_trim == tgt_trim) | (~mask_trim)\n",
        "            exact_matches = match_trim.all(dim=1)\n",
        "            batch_word_acc = exact_matches.sum().item() / B\n",
        "\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_char_acc += batch_char_acc\n",
        "            epoch_word_acc += batch_word_acc\n",
        "            total_batches += 1\n",
        "\n",
        "    return (\n",
        "        epoch_loss / total_batches,\n",
        "        epoch_char_acc / total_batches,\n",
        "        epoch_word_acc / total_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# ─── main() with parse_known_args ─────────────────────────────────────────────\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content\")\n",
        "    parser.add_argument(\"--train_file\", type=str, default=\"bn.translit.sampled.train.tsv\")\n",
        "    parser.add_argument(\"--dev_file\", type=str, default=\"bn.translit.sampled.dev.tsv\")\n",
        "    parser.add_argument(\"--test_file\", type=str, default=\"bn.translit.sampled.test.tsv\")\n",
        "    parser.add_argument(\"--emb_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--enc_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--dec_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--cell_type\", type=str, default=\"RNN\", choices=[\"RNN\", \"GRU\", \"LSTM\"])\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--tf_ratio\", type=float, default=0.5)\n",
        "    parser.add_argument(\"--beam_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--max_dev_len\", type=int, default=32)\n",
        "    parser.add_argument(\"--project_name\", type=str, default=\"Vanilla_RNN\")\n",
        "    parser.add_argument(\"--run_name\", type=str, default=None)\n",
        "\n",
        "    # ── use parse_known_args to ignore Colab’s extra \"-f …json\"\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_path = os.path.join(args.data_dir, args.train_file)\n",
        "    dev_path   = os.path.join(args.data_dir, args.dev_file)\n",
        "    test_path  = os.path.join(args.data_dir, args.test_file)\n",
        "    vocab = CharVocab([train_path, dev_path, test_path])\n",
        "\n",
        "    train_ds = TransliterationDataset(train_path, vocab)\n",
        "    dev_ds   = TransliterationDataset(dev_path, vocab)\n",
        "    test_ds  = TransliterationDataset(test_path, vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "    dev_loader   = DataLoader(dev_ds,   batch_size=args.batch_size, shuffle=False, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, collate_fn=TransliterationDataset.collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "    if args.run_name is None:\n",
        "        args.run_name = (\n",
        "            f\"{args.cell_type}\"\n",
        "            f\"_emb{args.emb_size}\"\n",
        "            f\"_hid{args.hidden_size}\"\n",
        "            f\"_layers{args.enc_layers}x{args.dec_layers}\"\n",
        "            f\"_lr{args.lr}\"\n",
        "            f\"_tf{args.tf_ratio}\"\n",
        "        )\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.project_name,\n",
        "        name=args.run_name,\n",
        "        config={\n",
        "            \"emb_size\": args.emb_size,\n",
        "            \"hidden_size\": args.hidden_size,\n",
        "            \"enc_layers\": args.enc_layers,\n",
        "            \"dec_layers\": args.dec_layers,\n",
        "            \"cell_type\": args.cell_type,\n",
        "            \"dropout\": args.dropout,\n",
        "            \"lr\": args.lr,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"epochs\": args.epochs,\n",
        "            \"tf_ratio\": args.tf_ratio,\n",
        "            \"beam_size\": args.beam_size,\n",
        "        },\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    encoder = Encoder(\n",
        "        input_vocab_size=vocab.rom_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.enc_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    decoder = Decoder(\n",
        "        output_vocab_size=vocab.dev_vocab_size,\n",
        "        embed_dim=config.emb_size,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=config.dec_layers,\n",
        "        cell_type=config.cell_type,\n",
        "        dropout=config.dropout,\n",
        "    )\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.dev_pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1) Unpack three values from train_one_epoch:\n",
        "        train_loss, train_char_acc, train_word_acc = train_one_epoch(\n",
        "            model=model,\n",
        "            iterator=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            pad_idx=vocab.dev_pad_idx,\n",
        "            device=device,\n",
        "            teacher_forcing_ratio=config.tf_ratio,\n",
        "        )\n",
        "\n",
        "        # 2) Unpack three values from evaluate:\n",
        "        val_loss, val_char_acc, val_word_acc = evaluate(\n",
        "            model=model,\n",
        "            iterator=dev_loader,\n",
        "            criterion=criterion,\n",
        "            pad_idx=vocab.dev_pad_idx,\n",
        "            device=device,\n",
        "            beam_size=config.beam_size,\n",
        "            max_dev_len=args.max_dev_len,\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # 3) Decide “best” model based on whichever metric you prefer (e.g. val_char_acc)\n",
        "        if val_char_acc > best_val_acc:\n",
        "            best_val_acc = val_char_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "        # 4) Log all four accuracy metrics to W&B\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_char_acc\": train_char_acc,\n",
        "                \"train_word_acc\": train_word_acc,   # newly added\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_char_acc\": val_char_acc,\n",
        "                \"val_word_acc\": val_word_acc,       # newly added\n",
        "                \"epoch_time_sec\": elapsed,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 5) Print so you see them in Colab output\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, \"\n",
        "            f\"Train Char Acc: {train_char_acc:.4f}, \"\n",
        "            f\"Train Word Acc: {train_word_acc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Char Acc: {val_char_acc:.4f}, \"\n",
        "            f\"Val Word Acc: {val_word_acc:.4f} | \"\n",
        "            f\"Time: {elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "EOF"
      ],
      "metadata": {
        "id": "YLbBP7SWshbz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > sweep.yaml << 'EOF'\n",
        "program: main.py\n",
        "method: bayes\n",
        "project: Vanilla_RNN\n",
        "entity: mrsagarbiswas-iit-madras\n",
        "metric:\n",
        "  name: val_char_acc\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  emb_size:\n",
        "    values: [16, 32, 64, 256]\n",
        "  hidden_size:\n",
        "    values: [16, 32, 64, 256]\n",
        "  enc_layers:\n",
        "    values: [1, 2, 3]\n",
        "  epochs:\n",
        "    values: [10, 20]\n",
        "  dec_layers:\n",
        "    values: [1, 2, 3]\n",
        "  cell_type:\n",
        "    values: [\"RNN\", \"GRU\", \"LSTM\"]\n",
        "  dropout:\n",
        "    values: [0.2, 0.3]\n",
        "  lr:\n",
        "    values: [1e-3, 1e-4]\n",
        "  batch_size:\n",
        "    values: [32, 64, 128]\n",
        "  tf_ratio:\n",
        "    values: [0.3, 0.5, 0.7]\n",
        "  beam_size:\n",
        "    values: [1, 3]\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "IHjg2NT0si4x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export WANDB_API_KEY=\"cca0995b9a513846c023d3c1623ea82d6ac165df\"\n",
        "wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuSvKgqQs66p",
        "outputId": "9c107bf4-00e2-4c63-e480-1fc82390ea4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Currently logged in as: mrsagarbiswas (mrsagarbiswas-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "SWEEP_ID=$(wandb sweep sweep.yaml)\n",
        "echo \"Sweep ID = $SWEEP_ID\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcBoXK-jtPBr",
        "outputId": "0fd60851-cfeb-47ed-8c6f-a704143d5cee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sweep ID = \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "wandb: Creating sweep from: sweep.yaml\n",
            "wandb: Creating sweep with ID: m4hl6ra2\n",
            "wandb: View sweep at: https://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\n",
            "wandb: Run sweep agent with: wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/m4hl6ra2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb agent mrsagarbiswas-iit-madras/Vanilla_RNN/m4hl6ra2 --count 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc33AXMhyJpg",
        "outputId": "8bcea77e-5219-40cd-d71d-a2cc3ceeb9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2025-05-18 21:59:13,056 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2025-05-18 21:59:13,289 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 21:59:13,289 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.2\n",
            "\temb_size: 64\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 32\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-18 21:59:13,290 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=3 --cell_type=LSTM --dec_layers=3 --dropout=0.2 --emb_size=64 --enc_layers=3 --epochs=20 --hidden_size=32 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_215917-38sj9o33\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb64_hid32_layers3x3_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/38sj9o33\u001b[0m\n",
            "2025-05-18 21:59:18,301 - wandb.wandb_agent - INFO - Running runs: ['38sj9o33']\n",
            "Epoch 01 | Train Loss: 3.2806, Train Char Acc: 0.1710, Train Word Acc: 0.0000 | Val Loss: 3.1462, Val Char Acc: 0.1906, Val Word Acc: 0.0000 | Time: 100.5s\n",
            "Epoch 02 | Train Loss: 3.1043, Train Char Acc: 0.1972, Train Word Acc: 0.0000 | Val Loss: 3.0752, Val Char Acc: 0.2016, Val Word Acc: 0.0001 | Time: 99.9s\n",
            "Epoch 03 | Train Loss: 2.9985, Train Char Acc: 0.2201, Train Word Acc: 0.0000 | Val Loss: 2.9875, Val Char Acc: 0.2206, Val Word Acc: 0.0001 | Time: 100.7s\n",
            "Epoch 04 | Train Loss: 2.8666, Train Char Acc: 0.2501, Train Word Acc: 0.0002 | Val Loss: 2.9330, Val Char Acc: 0.2346, Val Word Acc: 0.0002 | Time: 99.9s\n",
            "Epoch 05 | Train Loss: 2.7620, Train Char Acc: 0.2730, Train Word Acc: 0.0003 | Val Loss: 2.8787, Val Char Acc: 0.2414, Val Word Acc: 0.0002 | Time: 100.6s\n",
            "Epoch 06 | Train Loss: 2.6768, Train Char Acc: 0.2887, Train Word Acc: 0.0006 | Val Loss: 2.8033, Val Char Acc: 0.2561, Val Word Acc: 0.0009 | Time: 99.9s\n",
            "Epoch 07 | Train Loss: 2.5979, Train Char Acc: 0.3037, Train Word Acc: 0.0011 | Val Loss: 2.7287, Val Char Acc: 0.2711, Val Word Acc: 0.0014 | Time: 100.0s\n",
            "Epoch 08 | Train Loss: 2.5197, Train Char Acc: 0.3210, Train Word Acc: 0.0023 | Val Loss: 2.6402, Val Char Acc: 0.2911, Val Word Acc: 0.0028 | Time: 100.0s\n",
            "Epoch 09 | Train Loss: 2.4412, Train Char Acc: 0.3398, Train Word Acc: 0.0035 | Val Loss: 2.5654, Val Char Acc: 0.3075, Val Word Acc: 0.0048 | Time: 101.3s\n",
            "Epoch 10 | Train Loss: 2.3788, Train Char Acc: 0.3522, Train Word Acc: 0.0043 | Val Loss: 2.5069, Val Char Acc: 0.3189, Val Word Acc: 0.0059 | Time: 102.0s\n",
            "Epoch 11 | Train Loss: 2.3199, Train Char Acc: 0.3651, Train Word Acc: 0.0054 | Val Loss: 2.4537, Val Char Acc: 0.3280, Val Word Acc: 0.0081 | Time: 100.9s\n",
            "Epoch 12 | Train Loss: 2.2680, Train Char Acc: 0.3766, Train Word Acc: 0.0076 | Val Loss: 2.4013, Val Char Acc: 0.3381, Val Word Acc: 0.0087 | Time: 100.2s\n",
            "Epoch 13 | Train Loss: 2.2196, Train Char Acc: 0.3880, Train Word Acc: 0.0094 | Val Loss: 2.3633, Val Char Acc: 0.3476, Val Word Acc: 0.0126 | Time: 99.9s\n",
            "Epoch 14 | Train Loss: 2.1743, Train Char Acc: 0.3982, Train Word Acc: 0.0112 | Val Loss: 2.3044, Val Char Acc: 0.3607, Val Word Acc: 0.0134 | Time: 100.2s\n",
            "Epoch 15 | Train Loss: 2.1312, Train Char Acc: 0.4086, Train Word Acc: 0.0132 | Val Loss: 2.2667, Val Char Acc: 0.3680, Val Word Acc: 0.0168 | Time: 100.0s\n",
            "Epoch 16 | Train Loss: 2.0932, Train Char Acc: 0.4174, Train Word Acc: 0.0152 | Val Loss: 2.2303, Val Char Acc: 0.3785, Val Word Acc: 0.0203 | Time: 99.9s\n",
            "Epoch 17 | Train Loss: 2.0582, Train Char Acc: 0.4265, Train Word Acc: 0.0177 | Val Loss: 2.2017, Val Char Acc: 0.3848, Val Word Acc: 0.0234 | Time: 99.7s\n",
            "Epoch 18 | Train Loss: 2.0257, Train Char Acc: 0.4336, Train Word Acc: 0.0196 | Val Loss: 2.1773, Val Char Acc: 0.3890, Val Word Acc: 0.0278 | Time: 99.4s\n",
            "Epoch 19 | Train Loss: 1.9974, Train Char Acc: 0.4405, Train Word Acc: 0.0214 | Val Loss: 2.1555, Val Char Acc: 0.3943, Val Word Acc: 0.0296 | Time: 100.0s\n",
            "Epoch 20 | Train Loss: 1.9705, Train Char Acc: 0.4469, Train Word Acc: 0.0235 | Val Loss: 2.1281, Val Char Acc: 0.4031, Val Word Acc: 0.0317 | Time: 100.0s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄▂▅▂▄▃▃▃▆█▅▃▂▃▃▂▂▁▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▂▃▄▄▄▅▅▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▇▇▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▅▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 100.028\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.44689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.97048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.02348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.40307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.12808\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb64_hid32_layers3x3_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/38sj9o33\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_215917-38sj9o33/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 22:32:52,279 - wandb.wandb_agent - INFO - Cleaning up finished run: 38sj9o33\n",
            "2025-05-18 22:32:53,528 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 22:32:53,528 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 1\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.3\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 10\n",
            "\thidden_size: 32\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-18 22:32:53,529 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=1 --cell_type=GRU --dec_layers=1 --dropout=0.3 --emb_size=256 --enc_layers=3 --epochs=10 --hidden_size=32 --lr=0.001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_223257-v1tn8ejs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb256_hid32_layers3x1_lr0.001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/v1tn8ejs\u001b[0m\n",
            "2025-05-18 22:32:58,539 - wandb.wandb_agent - INFO - Running runs: ['v1tn8ejs']\n",
            "Epoch 01 | Train Loss: 2.8136, Train Char Acc: 0.2677, Train Word Acc: 0.0006 | Val Loss: 2.7509, Val Char Acc: 0.2730, Val Word Acc: 0.0024 | Time: 36.0s\n",
            "Epoch 02 | Train Loss: 2.4187, Train Char Acc: 0.3466, Train Word Acc: 0.0032 | Val Loss: 2.5794, Val Char Acc: 0.3010, Val Word Acc: 0.0033 | Time: 35.7s\n",
            "Epoch 03 | Train Loss: 2.2612, Train Char Acc: 0.3778, Train Word Acc: 0.0063 | Val Loss: 2.4587, Val Char Acc: 0.3272, Val Word Acc: 0.0042 | Time: 35.9s\n",
            "Epoch 04 | Train Loss: 2.1680, Train Char Acc: 0.3980, Train Word Acc: 0.0094 | Val Loss: 2.3746, Val Char Acc: 0.3494, Val Word Acc: 0.0089 | Time: 36.0s\n",
            "Epoch 05 | Train Loss: 2.0900, Train Char Acc: 0.4156, Train Word Acc: 0.0136 | Val Loss: 2.2942, Val Char Acc: 0.3623, Val Word Acc: 0.0131 | Time: 36.2s\n",
            "Epoch 06 | Train Loss: 2.0392, Train Char Acc: 0.4266, Train Word Acc: 0.0185 | Val Loss: 2.2954, Val Char Acc: 0.3693, Val Word Acc: 0.0163 | Time: 36.0s\n",
            "Epoch 07 | Train Loss: 2.0028, Train Char Acc: 0.4345, Train Word Acc: 0.0213 | Val Loss: 2.2366, Val Char Acc: 0.3762, Val Word Acc: 0.0153 | Time: 36.1s\n",
            "Epoch 08 | Train Loss: 1.9743, Train Char Acc: 0.4402, Train Word Acc: 0.0251 | Val Loss: 2.2046, Val Char Acc: 0.3798, Val Word Acc: 0.0200 | Time: 36.2s\n",
            "Epoch 09 | Train Loss: 1.9454, Train Char Acc: 0.4482, Train Word Acc: 0.0272 | Val Loss: 2.1991, Val Char Acc: 0.3827, Val Word Acc: 0.0261 | Time: 36.1s\n",
            "Epoch 10 | Train Loss: 1.9174, Train Char Acc: 0.4560, Train Word Acc: 0.0303 | Val Loss: 2.2268, Val Char Acc: 0.3779, Val Word Acc: 0.0320 | Time: 36.1s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▅▁▄▅▇▆▆█▆▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▄▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▂▂▃▄▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▄▆▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▄▃▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▃▄▄▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 36.12117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.45596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.91739\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.03035\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.37789\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.22679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb256_hid32_layers3x1_lr0.001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/v1tn8ejs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_223257-v1tn8ejs/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 22:39:08,806 - wandb.wandb_agent - INFO - Cleaning up finished run: v1tn8ejs\n",
            "2025-05-18 22:39:09,284 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 22:39:09,284 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 1\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.2\n",
            "\temb_size: 64\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 256\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-18 22:39:09,286 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=1 --cell_type=GRU --dec_layers=1 --dropout=0.2 --emb_size=64 --enc_layers=3 --epochs=20 --hidden_size=256 --lr=0.001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_223913-o4j6iyha\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb64_hid256_layers3x1_lr0.001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/o4j6iyha\u001b[0m\n",
            "2025-05-18 22:39:14,293 - wandb.wandb_agent - INFO - Running runs: ['o4j6iyha']\n",
            "Epoch 01 | Train Loss: 1.6895, Train Char Acc: 0.5267, Train Word Acc: 0.1238 | Val Loss: 1.5463, Val Char Acc: 0.5626, Val Word Acc: 0.1841 | Time: 73.7s\n",
            "Epoch 02 | Train Loss: 1.1189, Train Char Acc: 0.6890, Train Word Acc: 0.3168 | Val Loss: 1.4854, Val Char Acc: 0.5911, Val Word Acc: 0.2314 | Time: 73.5s\n",
            "Epoch 03 | Train Loss: 0.9884, Train Char Acc: 0.7308, Train Word Acc: 0.4096 | Val Loss: 1.4998, Val Char Acc: 0.5945, Val Word Acc: 0.2350 | Time: 73.8s\n",
            "Epoch 04 | Train Loss: 0.9210, Train Char Acc: 0.7514, Train Word Acc: 0.4661 | Val Loss: 1.5287, Val Char Acc: 0.5911, Val Word Acc: 0.2299 | Time: 73.7s\n",
            "Epoch 05 | Train Loss: 0.8768, Train Char Acc: 0.7646, Train Word Acc: 0.4976 | Val Loss: 1.5678, Val Char Acc: 0.5945, Val Word Acc: 0.2502 | Time: 73.7s\n",
            "Epoch 06 | Train Loss: 0.8526, Train Char Acc: 0.7720, Train Word Acc: 0.5199 | Val Loss: 1.5787, Val Char Acc: 0.5927, Val Word Acc: 0.2326 | Time: 73.5s\n",
            "Epoch 07 | Train Loss: 0.8323, Train Char Acc: 0.7781, Train Word Acc: 0.5356 | Val Loss: 1.5731, Val Char Acc: 0.5949, Val Word Acc: 0.2372 | Time: 73.5s\n",
            "Epoch 08 | Train Loss: 0.8162, Train Char Acc: 0.7829, Train Word Acc: 0.5490 | Val Loss: 1.6173, Val Char Acc: 0.5970, Val Word Acc: 0.2386 | Time: 73.5s\n",
            "Epoch 09 | Train Loss: 0.8128, Train Char Acc: 0.7840, Train Word Acc: 0.5538 | Val Loss: 1.6422, Val Char Acc: 0.5886, Val Word Acc: 0.2295 | Time: 73.4s\n",
            "Epoch 10 | Train Loss: 0.8034, Train Char Acc: 0.7866, Train Word Acc: 0.5619 | Val Loss: 1.6313, Val Char Acc: 0.5882, Val Word Acc: 0.2225 | Time: 73.5s\n",
            "Epoch 11 | Train Loss: 0.7975, Train Char Acc: 0.7885, Train Word Acc: 0.5678 | Val Loss: 1.6479, Val Char Acc: 0.5842, Val Word Acc: 0.2258 | Time: 73.4s\n",
            "Epoch 12 | Train Loss: 0.7968, Train Char Acc: 0.7886, Train Word Acc: 0.5694 | Val Loss: 1.6567, Val Char Acc: 0.5860, Val Word Acc: 0.2266 | Time: 73.7s\n",
            "Epoch 13 | Train Loss: 0.7889, Train Char Acc: 0.7911, Train Word Acc: 0.5756 | Val Loss: 1.6947, Val Char Acc: 0.5857, Val Word Acc: 0.2273 | Time: 73.3s\n",
            "Epoch 14 | Train Loss: 0.7849, Train Char Acc: 0.7925, Train Word Acc: 0.5792 | Val Loss: 1.6793, Val Char Acc: 0.5877, Val Word Acc: 0.2273 | Time: 74.6s\n",
            "Epoch 15 | Train Loss: 0.7851, Train Char Acc: 0.7923, Train Word Acc: 0.5801 | Val Loss: 1.6736, Val Char Acc: 0.5874, Val Word Acc: 0.2276 | Time: 73.6s\n",
            "Epoch 16 | Train Loss: 0.7859, Train Char Acc: 0.7916, Train Word Acc: 0.5786 | Val Loss: 1.7418, Val Char Acc: 0.5761, Val Word Acc: 0.2172 | Time: 74.3s\n",
            "Epoch 17 | Train Loss: 0.7797, Train Char Acc: 0.7935, Train Word Acc: 0.5826 | Val Loss: 1.7108, Val Char Acc: 0.5817, Val Word Acc: 0.2165 | Time: 74.3s\n",
            "Epoch 18 | Train Loss: 0.7795, Train Char Acc: 0.7939, Train Word Acc: 0.5827 | Val Loss: 1.6960, Val Char Acc: 0.5888, Val Word Acc: 0.2301 | Time: 74.1s\n",
            "Epoch 19 | Train Loss: 0.7793, Train Char Acc: 0.7944, Train Word Acc: 0.5866 | Val Loss: 1.7122, Val Char Acc: 0.5816, Val Word Acc: 0.2155 | Time: 73.9s\n",
            "Epoch 20 | Train Loss: 0.7777, Train Char Acc: 0.7942, Train Word Acc: 0.5837 | Val Loss: 1.7018, Val Char Acc: 0.5809, Val Word Acc: 0.2237 | Time: 74.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▃▂▄▃▄▂▂▂▂▂▂▃▁█▃▆▆▅▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▅▆▇▇▇██████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▄▅▆▇▇▇▇████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▇█▇█▇██▆▆▅▆▆▆▆▄▅▆▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▁▁▂▃▄▃▅▅▅▅▆▇▆▆█▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▆▆▆█▆▇▇▆▅▅▆▆▆▆▅▄▆▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 74.52115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.79424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.77771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.58374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.58095\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.70183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.22371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb64_hid256_layers3x1_lr0.001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/o4j6iyha\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_223913-o4j6iyha/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 23:03:55,959 - wandb.wandb_agent - INFO - Cleaning up finished run: o4j6iyha\n",
            "2025-05-18 23:03:56,308 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 23:03:56,308 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 1\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.3\n",
            "\temb_size: 16\n",
            "\tenc_layers: 1\n",
            "\tepochs: 10\n",
            "\thidden_size: 32\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-18 23:03:56,310 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=1 --cell_type=LSTM --dec_layers=2 --dropout=0.3 --emb_size=16 --enc_layers=1 --epochs=10 --hidden_size=32 --lr=0.001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_230400-3wu4u1r7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb16_hid32_layers1x2_lr0.001_tf0.7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/3wu4u1r7\u001b[0m\n",
            "2025-05-18 23:04:01,317 - wandb.wandb_agent - INFO - Running runs: ['3wu4u1r7']\n",
            "Epoch 01 | Train Loss: 3.1628, Train Char Acc: 0.1829, Train Word Acc: 0.0000 | Val Loss: 3.1318, Val Char Acc: 0.2029, Val Word Acc: 0.0002 | Time: 25.6s\n",
            "Epoch 02 | Train Loss: 2.7757, Train Char Acc: 0.2641, Train Word Acc: 0.0003 | Val Loss: 3.0359, Val Char Acc: 0.2105, Val Word Acc: 0.0006 | Time: 25.4s\n",
            "Epoch 03 | Train Loss: 2.5927, Train Char Acc: 0.3041, Train Word Acc: 0.0012 | Val Loss: 2.8760, Val Char Acc: 0.2452, Val Word Acc: 0.0018 | Time: 25.6s\n",
            "Epoch 04 | Train Loss: 2.3633, Train Char Acc: 0.3580, Train Word Acc: 0.0047 | Val Loss: 2.7048, Val Char Acc: 0.2825, Val Word Acc: 0.0042 | Time: 25.5s\n",
            "Epoch 05 | Train Loss: 2.1895, Train Char Acc: 0.3957, Train Word Acc: 0.0086 | Val Loss: 2.6128, Val Char Acc: 0.3024, Val Word Acc: 0.0111 | Time: 25.6s\n",
            "Epoch 06 | Train Loss: 2.0699, Train Char Acc: 0.4234, Train Word Acc: 0.0139 | Val Loss: 2.5063, Val Char Acc: 0.3262, Val Word Acc: 0.0173 | Time: 25.7s\n",
            "Epoch 07 | Train Loss: 1.9820, Train Char Acc: 0.4449, Train Word Acc: 0.0205 | Val Loss: 2.4407, Val Char Acc: 0.3444, Val Word Acc: 0.0234 | Time: 25.7s\n",
            "Epoch 08 | Train Loss: 1.9126, Train Char Acc: 0.4630, Train Word Acc: 0.0263 | Val Loss: 2.3870, Val Char Acc: 0.3530, Val Word Acc: 0.0281 | Time: 25.6s\n",
            "Epoch 09 | Train Loss: 1.8446, Train Char Acc: 0.4813, Train Word Acc: 0.0320 | Val Loss: 2.3391, Val Char Acc: 0.3727, Val Word Acc: 0.0344 | Time: 25.2s\n",
            "Epoch 10 | Train Loss: 1.7952, Train Char Acc: 0.4955, Train Word Acc: 0.0395 | Val Loss: 2.3001, Val Char Acc: 0.3868, Val Word Acc: 0.0418 | Time: 25.3s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.015 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▆▄▆▅▆██▆▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▃▄▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▅▄▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▃▃▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▁▃▄▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▄▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 25.3295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.49548\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.79519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.03951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.38683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.30012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.04185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb16_hid32_layers1x2_lr0.001_tf0.7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/3wu4u1r7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_230400-3wu4u1r7/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 23:08:25,033 - wandb.wandb_agent - INFO - Cleaning up finished run: 3wu4u1r7\n",
            "2025-05-18 23:08:25,419 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 23:08:25,419 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 3\n",
            "\tcell_type: RNN\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.2\n",
            "\temb_size: 256\n",
            "\tenc_layers: 1\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-18 23:08:25,420 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=3 --cell_type=RNN --dec_layers=1 --dropout=0.2 --emb_size=256 --enc_layers=1 --epochs=10 --hidden_size=256 --lr=0.001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-18 23:08:30,431 - wandb.wandb_agent - INFO - Running runs: ['m2vfh8dx']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_230830-m2vfh8dx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRNN_emb256_hid256_layers1x1_lr0.001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/m2vfh8dx\u001b[0m\n",
            "Epoch 01 | Train Loss: 2.8903, Train Char Acc: 0.2282, Train Word Acc: 0.0000 | Val Loss: 3.2026, Val Char Acc: 0.1591, Val Word Acc: 0.0000 | Time: 32.7s\n",
            "Epoch 02 | Train Loss: 2.7793, Train Char Acc: 0.2515, Train Word Acc: 0.0001 | Val Loss: 3.1522, Val Char Acc: 0.1790, Val Word Acc: 0.0000 | Time: 32.2s\n",
            "Epoch 03 | Train Loss: 2.7499, Train Char Acc: 0.2576, Train Word Acc: 0.0001 | Val Loss: 3.2132, Val Char Acc: 0.1668, Val Word Acc: 0.0000 | Time: 32.5s\n",
            "Epoch 04 | Train Loss: 2.7322, Train Char Acc: 0.2618, Train Word Acc: 0.0000 | Val Loss: 3.1743, Val Char Acc: 0.1685, Val Word Acc: 0.0000 | Time: 31.8s\n",
            "Epoch 05 | Train Loss: 2.7186, Train Char Acc: 0.2643, Train Word Acc: 0.0001 | Val Loss: 3.1820, Val Char Acc: 0.1705, Val Word Acc: 0.0000 | Time: 33.0s\n",
            "Epoch 06 | Train Loss: 2.6975, Train Char Acc: 0.2691, Train Word Acc: 0.0001 | Val Loss: 3.1546, Val Char Acc: 0.1664, Val Word Acc: 0.0001 | Time: 32.8s\n",
            "Epoch 07 | Train Loss: 2.6888, Train Char Acc: 0.2701, Train Word Acc: 0.0002 | Val Loss: 3.1443, Val Char Acc: 0.1824, Val Word Acc: 0.0000 | Time: 31.8s\n",
            "Epoch 08 | Train Loss: 2.6912, Train Char Acc: 0.2698, Train Word Acc: 0.0002 | Val Loss: 3.1959, Val Char Acc: 0.1710, Val Word Acc: 0.0000 | Time: 32.6s\n",
            "Epoch 09 | Train Loss: 2.6841, Train Char Acc: 0.2711, Train Word Acc: 0.0002 | Val Loss: 3.1743, Val Char Acc: 0.1796, Val Word Acc: 0.0000 | Time: 32.3s\n",
            "Epoch 10 | Train Loss: 2.6955, Train Char Acc: 0.2689, Train Word Acc: 0.0001 | Val Loss: 3.1975, Val Char Acc: 0.1748, Val Word Acc: 0.0000 | Time: 32.2s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.015 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▆▃▅▁█▇▁▅▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▅▆▆▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▃▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▂▂▂▃▄▅▇█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▇▃▄▄▃█▅▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇▂█▄▅▂▁▆▄▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▁█▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 32.23016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.2689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 2.69549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.00014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.17478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 3.19751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRNN_emb256_hid256_layers1x1_lr0.001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/m2vfh8dx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_230830-m2vfh8dx/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 23:14:05,160 - wandb.wandb_agent - INFO - Cleaning up finished run: m2vfh8dx\n",
            "2025-05-18 23:14:05,939 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 23:14:05,939 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 1\n",
            "\tcell_type: RNN\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.3\n",
            "\temb_size: 64\n",
            "\tenc_layers: 2\n",
            "\tepochs: 10\n",
            "\thidden_size: 256\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-18 23:14:05,940 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=1 --cell_type=RNN --dec_layers=3 --dropout=0.3 --emb_size=64 --enc_layers=2 --epochs=10 --hidden_size=256 --lr=0.001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_231410-i1apv1v1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRNN_emb64_hid256_layers2x3_lr0.001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/i1apv1v1\u001b[0m\n",
            "2025-05-18 23:14:10,951 - wandb.wandb_agent - INFO - Running runs: ['i1apv1v1']\n",
            "Epoch 01 | Train Loss: 3.0692, Train Char Acc: 0.1890, Train Word Acc: 0.0000 | Val Loss: 3.1610, Val Char Acc: 0.1709, Val Word Acc: 0.0000 | Time: 75.2s\n",
            "Epoch 02 | Train Loss: 2.9817, Train Char Acc: 0.2077, Train Word Acc: 0.0000 | Val Loss: 3.0818, Val Char Acc: 0.1939, Val Word Acc: 0.0000 | Time: 75.5s\n",
            "Epoch 03 | Train Loss: 2.9609, Train Char Acc: 0.2113, Train Word Acc: 0.0000 | Val Loss: 3.1264, Val Char Acc: 0.1887, Val Word Acc: 0.0000 | Time: 74.7s\n",
            "Epoch 04 | Train Loss: 2.9527, Train Char Acc: 0.2133, Train Word Acc: 0.0000 | Val Loss: 3.1132, Val Char Acc: 0.1832, Val Word Acc: 0.0000 | Time: 74.5s\n",
            "Epoch 05 | Train Loss: 2.9419, Train Char Acc: 0.2151, Train Word Acc: 0.0000 | Val Loss: 3.0913, Val Char Acc: 0.1954, Val Word Acc: 0.0000 | Time: 74.6s\n",
            "Epoch 06 | Train Loss: 2.9364, Train Char Acc: 0.2157, Train Word Acc: 0.0001 | Val Loss: 3.1184, Val Char Acc: 0.1840, Val Word Acc: 0.0000 | Time: 74.4s\n",
            "Epoch 07 | Train Loss: 2.9303, Train Char Acc: 0.2175, Train Word Acc: 0.0001 | Val Loss: 3.1051, Val Char Acc: 0.1807, Val Word Acc: 0.0000 | Time: 74.3s\n",
            "Epoch 08 | Train Loss: 2.9301, Train Char Acc: 0.2171, Train Word Acc: 0.0000 | Val Loss: 3.1039, Val Char Acc: 0.1864, Val Word Acc: 0.0000 | Time: 74.6s\n",
            "Epoch 09 | Train Loss: 2.9312, Train Char Acc: 0.2165, Train Word Acc: 0.0000 | Val Loss: 3.0932, Val Char Acc: 0.1925, Val Word Acc: 0.0000 | Time: 74.5s\n",
            "Epoch 10 | Train Loss: 2.9264, Train Char Acc: 0.2185, Train Word Acc: 0.0000 | Val Loss: 3.1034, Val Char Acc: 0.1850, Val Word Acc: 0.0000 | Time: 74.6s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▆█▄▃▃▂▁▃▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▅▆▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▇▅▄▄██▄▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁█▆▅█▅▄▅▇▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▁▅▄▂▄▃▃▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 74.59951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.21848\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 2.92644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 3.10345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRNN_emb64_hid256_layers2x3_lr0.001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/i1apv1v1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_231410-i1apv1v1/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 23:26:46,735 - wandb.wandb_agent - INFO - Cleaning up finished run: i1apv1v1\n",
            "2025-05-18 23:26:47,081 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 23:26:47,081 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 3\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.3\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-18 23:26:47,083 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=3 --cell_type=GRU --dec_layers=2 --dropout=0.3 --emb_size=256 --enc_layers=3 --epochs=20 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_232651-bpobveex\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb256_hid256_layers3x2_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/bpobveex\u001b[0m\n",
            "2025-05-18 23:26:52,093 - wandb.wandb_agent - INFO - Running runs: ['bpobveex']\n",
            "Epoch 01 | Train Loss: 2.4212, Train Char Acc: 0.3524, Train Word Acc: 0.0164 | Val Loss: 1.9172, Val Char Acc: 0.4477, Val Word Acc: 0.0586 | Time: 87.9s\n",
            "Epoch 02 | Train Loss: 1.6236, Train Char Acc: 0.5339, Train Word Acc: 0.0948 | Val Loss: 1.6257, Val Char Acc: 0.5260, Val Word Acc: 0.1365 | Time: 87.7s\n",
            "Epoch 03 | Train Loss: 1.3977, Train Char Acc: 0.5985, Train Word Acc: 0.1647 | Val Loss: 1.5169, Val Char Acc: 0.5620, Val Word Acc: 0.1850 | Time: 87.6s\n",
            "Epoch 04 | Train Loss: 1.2604, Train Char Acc: 0.6419, Train Word Acc: 0.2294 | Val Loss: 1.4744, Val Char Acc: 0.5782, Val Word Acc: 0.2176 | Time: 87.6s\n",
            "Epoch 05 | Train Loss: 1.1629, Train Char Acc: 0.6736, Train Word Acc: 0.2828 | Val Loss: 1.4401, Val Char Acc: 0.5923, Val Word Acc: 0.2368 | Time: 87.7s\n",
            "Epoch 06 | Train Loss: 1.0923, Train Char Acc: 0.6966, Train Word Acc: 0.3293 | Val Loss: 1.4167, Val Char Acc: 0.6039, Val Word Acc: 0.2523 | Time: 87.8s\n",
            "Epoch 07 | Train Loss: 1.0300, Train Char Acc: 0.7165, Train Word Acc: 0.3720 | Val Loss: 1.3941, Val Char Acc: 0.6144, Val Word Acc: 0.2809 | Time: 87.5s\n",
            "Epoch 08 | Train Loss: 0.9821, Train Char Acc: 0.7319, Train Word Acc: 0.4096 | Val Loss: 1.3764, Val Char Acc: 0.6228, Val Word Acc: 0.2911 | Time: 88.3s\n",
            "Epoch 09 | Train Loss: 0.9374, Train Char Acc: 0.7458, Train Word Acc: 0.4409 | Val Loss: 1.3804, Val Char Acc: 0.6274, Val Word Acc: 0.3009 | Time: 87.7s\n",
            "Epoch 10 | Train Loss: 0.9038, Train Char Acc: 0.7562, Train Word Acc: 0.4707 | Val Loss: 1.3860, Val Char Acc: 0.6291, Val Word Acc: 0.3036 | Time: 87.4s\n",
            "Epoch 11 | Train Loss: 0.8720, Train Char Acc: 0.7656, Train Word Acc: 0.4954 | Val Loss: 1.3935, Val Char Acc: 0.6315, Val Word Acc: 0.3126 | Time: 87.3s\n",
            "Epoch 12 | Train Loss: 0.8423, Train Char Acc: 0.7751, Train Word Acc: 0.5219 | Val Loss: 1.4132, Val Char Acc: 0.6311, Val Word Acc: 0.3131 | Time: 87.2s\n",
            "Epoch 13 | Train Loss: 0.8174, Train Char Acc: 0.7827, Train Word Acc: 0.5444 | Val Loss: 1.4212, Val Char Acc: 0.6314, Val Word Acc: 0.3129 | Time: 87.3s\n",
            "Epoch 14 | Train Loss: 0.7957, Train Char Acc: 0.7888, Train Word Acc: 0.5615 | Val Loss: 1.4303, Val Char Acc: 0.6324, Val Word Acc: 0.3195 | Time: 87.1s\n",
            "Epoch 15 | Train Loss: 0.7745, Train Char Acc: 0.7958, Train Word Acc: 0.5852 | Val Loss: 1.4495, Val Char Acc: 0.6322, Val Word Acc: 0.3164 | Time: 86.9s\n",
            "Epoch 16 | Train Loss: 0.7555, Train Char Acc: 0.8014, Train Word Acc: 0.6026 | Val Loss: 1.4382, Val Char Acc: 0.6378, Val Word Acc: 0.3245 | Time: 87.3s\n",
            "Epoch 17 | Train Loss: 0.7398, Train Char Acc: 0.8060, Train Word Acc: 0.6196 | Val Loss: 1.4533, Val Char Acc: 0.6383, Val Word Acc: 0.3222 | Time: 87.2s\n",
            "Epoch 18 | Train Loss: 0.7255, Train Char Acc: 0.8102, Train Word Acc: 0.6341 | Val Loss: 1.4622, Val Char Acc: 0.6366, Val Word Acc: 0.3184 | Time: 87.3s\n",
            "Epoch 19 | Train Loss: 0.7108, Train Char Acc: 0.8149, Train Word Acc: 0.6481 | Val Loss: 1.4928, Val Char Acc: 0.6334, Val Word Acc: 0.3137 | Time: 88.5s\n",
            "Epoch 20 | Train Loss: 0.6976, Train Char Acc: 0.8187, Train Word Acc: 0.6631 | Val Loss: 1.4880, Val Char Acc: 0.6374, Val Word Acc: 0.3206 | Time: 87.0s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.017 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.017 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▅▅▄▄▄▅▄▇▄▃▃▂▃▂▁▃▂▃█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▂▃▃▄▄▅▅▆▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▆▆▇▇▇████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▃▄▅▆▆▇▇▇▇██████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 86.95064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.81868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.69756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.66311\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.63741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.48801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.32061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb256_hid256_layers3x2_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/bpobveex\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_232651-bpobveex/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-18 23:56:12,182 - wandb.wandb_agent - INFO - Cleaning up finished run: bpobveex\n",
            "2025-05-18 23:56:12,513 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-18 23:56:12,513 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 3\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 32\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 64\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-18 23:56:12,515 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=3 --cell_type=GRU --dec_layers=2 --dropout=0.2 --emb_size=32 --enc_layers=3 --epochs=20 --hidden_size=64 --lr=0.001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250518_235616-77olz8v2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb32_hid64_layers3x2_lr0.001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/77olz8v2\u001b[0m\n",
            "2025-05-18 23:56:17,523 - wandb.wandb_agent - INFO - Running runs: ['77olz8v2']\n",
            "Epoch 01 | Train Loss: 2.2786, Train Char Acc: 0.3821, Train Word Acc: 0.0214 | Val Loss: 1.9762, Val Char Acc: 0.4456, Val Word Acc: 0.0693 | Time: 79.1s\n",
            "Epoch 02 | Train Loss: 1.5753, Train Char Acc: 0.5563, Train Word Acc: 0.0971 | Val Loss: 1.7784, Val Char Acc: 0.5075, Val Word Acc: 0.1199 | Time: 79.4s\n",
            "Epoch 03 | Train Loss: 1.3995, Train Char Acc: 0.6069, Train Word Acc: 0.1477 | Val Loss: 1.6855, Val Char Acc: 0.5311, Val Word Acc: 0.1597 | Time: 79.3s\n",
            "Epoch 04 | Train Loss: 1.3053, Train Char Acc: 0.6350, Train Word Acc: 0.1824 | Val Loss: 1.6774, Val Char Acc: 0.5493, Val Word Acc: 0.1927 | Time: 79.5s\n",
            "Epoch 05 | Train Loss: 1.2421, Train Char Acc: 0.6537, Train Word Acc: 0.2051 | Val Loss: 1.6807, Val Char Acc: 0.5521, Val Word Acc: 0.2029 | Time: 78.6s\n",
            "Epoch 06 | Train Loss: 1.2065, Train Char Acc: 0.6641, Train Word Acc: 0.2250 | Val Loss: 1.6414, Val Char Acc: 0.5566, Val Word Acc: 0.2097 | Time: 79.3s\n",
            "Epoch 07 | Train Loss: 1.1687, Train Char Acc: 0.6756, Train Word Acc: 0.2444 | Val Loss: 1.6341, Val Char Acc: 0.5603, Val Word Acc: 0.2095 | Time: 78.8s\n",
            "Epoch 08 | Train Loss: 1.1417, Train Char Acc: 0.6838, Train Word Acc: 0.2591 | Val Loss: 1.5960, Val Char Acc: 0.5718, Val Word Acc: 0.2272 | Time: 79.1s\n",
            "Epoch 09 | Train Loss: 1.1300, Train Char Acc: 0.6877, Train Word Acc: 0.2694 | Val Loss: 1.6194, Val Char Acc: 0.5692, Val Word Acc: 0.2266 | Time: 79.2s\n",
            "Epoch 10 | Train Loss: 1.1041, Train Char Acc: 0.6958, Train Word Acc: 0.2816 | Val Loss: 1.6028, Val Char Acc: 0.5752, Val Word Acc: 0.2364 | Time: 78.6s\n",
            "Epoch 11 | Train Loss: 1.0910, Train Char Acc: 0.6995, Train Word Acc: 0.2888 | Val Loss: 1.6158, Val Char Acc: 0.5763, Val Word Acc: 0.2355 | Time: 79.6s\n",
            "Epoch 12 | Train Loss: 1.0770, Train Char Acc: 0.7039, Train Word Acc: 0.2981 | Val Loss: 1.5914, Val Char Acc: 0.5780, Val Word Acc: 0.2379 | Time: 78.7s\n",
            "Epoch 13 | Train Loss: 1.0664, Train Char Acc: 0.7077, Train Word Acc: 0.3089 | Val Loss: 1.5832, Val Char Acc: 0.5828, Val Word Acc: 0.2407 | Time: 79.4s\n",
            "Epoch 14 | Train Loss: 1.0485, Train Char Acc: 0.7129, Train Word Acc: 0.3153 | Val Loss: 1.6009, Val Char Acc: 0.5815, Val Word Acc: 0.2384 | Time: 78.5s\n",
            "Epoch 15 | Train Loss: 1.0442, Train Char Acc: 0.7143, Train Word Acc: 0.3223 | Val Loss: 1.5952, Val Char Acc: 0.5817, Val Word Acc: 0.2415 | Time: 78.7s\n",
            "Epoch 16 | Train Loss: 1.0328, Train Char Acc: 0.7177, Train Word Acc: 0.3292 | Val Loss: 1.6082, Val Char Acc: 0.5792, Val Word Acc: 0.2361 | Time: 79.3s\n",
            "Epoch 17 | Train Loss: 1.0233, Train Char Acc: 0.7209, Train Word Acc: 0.3355 | Val Loss: 1.6253, Val Char Acc: 0.5832, Val Word Acc: 0.2476 | Time: 78.5s\n",
            "Epoch 18 | Train Loss: 1.0161, Train Char Acc: 0.7230, Train Word Acc: 0.3407 | Val Loss: 1.5915, Val Char Acc: 0.5884, Val Word Acc: 0.2502 | Time: 79.8s\n",
            "Epoch 19 | Train Loss: 1.0095, Train Char Acc: 0.7248, Train Word Acc: 0.3444 | Val Loss: 1.6100, Val Char Acc: 0.5835, Val Word Acc: 0.2455 | Time: 78.9s\n",
            "Epoch 20 | Train Loss: 1.0049, Train Char Acc: 0.7261, Train Word Acc: 0.3490 | Val Loss: 1.5993, Val Char Acc: 0.5860, Val Word Acc: 0.2476 | Time: 78.9s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄▆▅▇▂▅▃▄▅▂▇▂▆▁▂▅▁█▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▅▆▆▇▇▇▇▇▇▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▃▄▄▅▅▆▆▆▇▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▃▃▂▂▁▂▁▂▁▁▁▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▃▄▆▆▆▆▇▇▇▇████▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 78.87638\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.72611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.00492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.34902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.58603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.59934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.24764\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGRU_emb32_hid64_layers3x2_lr0.001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/77olz8v2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250518_235616-77olz8v2/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 00:22:45,115 - wandb.wandb_agent - INFO - Cleaning up finished run: 77olz8v2\n",
            "2025-05-19 00:22:45,531 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 00:22:45,531 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.2\n",
            "\temb_size: 32\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 64\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-19 00:22:45,532 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=3 --cell_type=LSTM --dec_layers=3 --dropout=0.2 --emb_size=32 --enc_layers=3 --epochs=20 --hidden_size=64 --lr=0.001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_002249-z81b23ly\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb32_hid64_layers3x3_lr0.001_tf0.7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/z81b23ly\u001b[0m\n",
            "2025-05-19 00:22:50,542 - wandb.wandb_agent - INFO - Running runs: ['z81b23ly']\n",
            "Epoch 01 | Train Loss: 2.9360, Train Char Acc: 0.2237, Train Word Acc: 0.0001 | Val Loss: 2.9086, Val Char Acc: 0.2396, Val Word Acc: 0.0004 | Time: 54.3s\n",
            "Epoch 02 | Train Loss: 2.0567, Train Char Acc: 0.4403, Train Word Acc: 0.0185 | Val Loss: 2.1381, Val Char Acc: 0.4277, Val Word Acc: 0.0657 | Time: 54.0s\n",
            "Epoch 03 | Train Loss: 1.5585, Train Char Acc: 0.5664, Train Word Acc: 0.0762 | Val Loss: 1.9595, Val Char Acc: 0.4885, Val Word Acc: 0.1175 | Time: 54.9s\n",
            "Epoch 04 | Train Loss: 1.3586, Train Char Acc: 0.6226, Train Word Acc: 0.1288 | Val Loss: 1.8927, Val Char Acc: 0.5212, Val Word Acc: 0.1674 | Time: 55.0s\n",
            "Epoch 05 | Train Loss: 1.2485, Train Char Acc: 0.6549, Train Word Acc: 0.1683 | Val Loss: 1.8260, Val Char Acc: 0.5417, Val Word Acc: 0.1974 | Time: 55.7s\n",
            "Epoch 06 | Train Loss: 1.1710, Train Char Acc: 0.6780, Train Word Acc: 0.2057 | Val Loss: 1.7835, Val Char Acc: 0.5557, Val Word Acc: 0.2209 | Time: 54.4s\n",
            "Epoch 07 | Train Loss: 1.1094, Train Char Acc: 0.6967, Train Word Acc: 0.2350 | Val Loss: 1.7397, Val Char Acc: 0.5720, Val Word Acc: 0.2356 | Time: 55.2s\n",
            "Epoch 08 | Train Loss: 1.0690, Train Char Acc: 0.7086, Train Word Acc: 0.2592 | Val Loss: 1.7639, Val Char Acc: 0.5750, Val Word Acc: 0.2546 | Time: 54.1s\n",
            "Epoch 09 | Train Loss: 1.0357, Train Char Acc: 0.7181, Train Word Acc: 0.2802 | Val Loss: 1.7736, Val Char Acc: 0.5765, Val Word Acc: 0.2541 | Time: 54.1s\n",
            "Epoch 10 | Train Loss: 1.0072, Train Char Acc: 0.7266, Train Word Acc: 0.2990 | Val Loss: 1.7358, Val Char Acc: 0.5875, Val Word Acc: 0.2712 | Time: 55.0s\n",
            "Epoch 11 | Train Loss: 0.9822, Train Char Acc: 0.7341, Train Word Acc: 0.3163 | Val Loss: 1.7551, Val Char Acc: 0.5905, Val Word Acc: 0.2794 | Time: 54.0s\n",
            "Epoch 12 | Train Loss: 0.9620, Train Char Acc: 0.7405, Train Word Acc: 0.3323 | Val Loss: 1.7401, Val Char Acc: 0.5930, Val Word Acc: 0.2843 | Time: 55.0s\n",
            "Epoch 13 | Train Loss: 0.9440, Train Char Acc: 0.7459, Train Word Acc: 0.3487 | Val Loss: 1.7175, Val Char Acc: 0.5980, Val Word Acc: 0.2898 | Time: 53.9s\n",
            "Epoch 14 | Train Loss: 0.9321, Train Char Acc: 0.7492, Train Word Acc: 0.3595 | Val Loss: 1.6901, Val Char Acc: 0.6011, Val Word Acc: 0.2862 | Time: 54.5s\n",
            "Epoch 15 | Train Loss: 0.9153, Train Char Acc: 0.7543, Train Word Acc: 0.3705 | Val Loss: 1.7252, Val Char Acc: 0.5992, Val Word Acc: 0.2919 | Time: 54.7s\n",
            "Epoch 16 | Train Loss: 0.9041, Train Char Acc: 0.7575, Train Word Acc: 0.3816 | Val Loss: 1.7177, Val Char Acc: 0.6020, Val Word Acc: 0.2878 | Time: 54.1s\n",
            "Epoch 17 | Train Loss: 0.8916, Train Char Acc: 0.7607, Train Word Acc: 0.3896 | Val Loss: 1.7100, Val Char Acc: 0.6095, Val Word Acc: 0.3015 | Time: 54.8s\n",
            "Epoch 18 | Train Loss: 0.8788, Train Char Acc: 0.7650, Train Word Acc: 0.4012 | Val Loss: 1.7370, Val Char Acc: 0.6039, Val Word Acc: 0.2941 | Time: 54.1s\n",
            "Epoch 19 | Train Loss: 0.8669, Train Char Acc: 0.7686, Train Word Acc: 0.4101 | Val Loss: 1.7266, Val Char Acc: 0.6069, Val Word Acc: 0.3041 | Time: 54.9s\n",
            "Epoch 20 | Train Loss: 0.8618, Train Char Acc: 0.7699, Train Word Acc: 0.4162 | Val Loss: 1.6886, Val Char Acc: 0.6137, Val Word Acc: 0.3095 | Time: 53.9s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.005 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.005 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.005 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▂▁▅▅█▃▆▂▂▅▁▅▁▃▄▂▅▂▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▄▅▆▇▇▇▇▇▇██████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▂▃▄▄▅▅▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▅▆▆▇▇▇▇▇███████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▂▄▅▅▆▆▇▇▇▇▇█▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 53.93897\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.76985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.86177\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.41621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.61366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.68862\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.30949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb32_hid64_layers3x3_lr0.001_tf0.7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/z81b23ly\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_002249-z81b23ly/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 00:41:10,935 - wandb.wandb_agent - INFO - Cleaning up finished run: z81b23ly\n",
            "2025-05-19 00:41:11,445 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 00:41:11,445 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 1\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 32\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 64\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.5\n",
            "2025-05-19 00:41:11,446 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=1 --cell_type=LSTM --dec_layers=2 --dropout=0.2 --emb_size=32 --enc_layers=3 --epochs=20 --hidden_size=64 --lr=0.0001 --tf_ratio=0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_004115-syacpy66\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb32_hid64_layers3x2_lr0.0001_tf0.5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/syacpy66\u001b[0m\n",
            "2025-05-19 00:41:16,457 - wandb.wandb_agent - INFO - Running runs: ['syacpy66']\n",
            "Epoch 01 | Train Loss: 3.2923, Train Char Acc: 0.1620, Train Word Acc: 0.0000 | Val Loss: 3.1653, Val Char Acc: 0.1758, Val Word Acc: 0.0000 | Time: 47.2s\n",
            "Epoch 02 | Train Loss: 3.0579, Train Char Acc: 0.2015, Train Word Acc: 0.0000 | Val Loss: 3.0894, Val Char Acc: 0.1956, Val Word Acc: 0.0000 | Time: 46.7s\n",
            "Epoch 03 | Train Loss: 2.9135, Train Char Acc: 0.2319, Train Word Acc: 0.0001 | Val Loss: 3.0020, Val Char Acc: 0.2209, Val Word Acc: 0.0000 | Time: 46.7s\n",
            "Epoch 04 | Train Loss: 2.7788, Train Char Acc: 0.2636, Train Word Acc: 0.0004 | Val Loss: 2.8899, Val Char Acc: 0.2450, Val Word Acc: 0.0008 | Time: 45.8s\n",
            "Epoch 05 | Train Loss: 2.6581, Train Char Acc: 0.2901, Train Word Acc: 0.0007 | Val Loss: 2.7861, Val Char Acc: 0.2620, Val Word Acc: 0.0013 | Time: 46.8s\n",
            "Epoch 06 | Train Loss: 2.5447, Train Char Acc: 0.3144, Train Word Acc: 0.0013 | Val Loss: 2.6909, Val Char Acc: 0.2743, Val Word Acc: 0.0029 | Time: 46.6s\n",
            "Epoch 07 | Train Loss: 2.4299, Train Char Acc: 0.3399, Train Word Acc: 0.0029 | Val Loss: 2.5805, Val Char Acc: 0.3014, Val Word Acc: 0.0042 | Time: 46.6s\n",
            "Epoch 08 | Train Loss: 2.3261, Train Char Acc: 0.3633, Train Word Acc: 0.0054 | Val Loss: 2.4793, Val Char Acc: 0.3237, Val Word Acc: 0.0079 | Time: 45.9s\n",
            "Epoch 09 | Train Loss: 2.2379, Train Char Acc: 0.3843, Train Word Acc: 0.0092 | Val Loss: 2.4025, Val Char Acc: 0.3401, Val Word Acc: 0.0103 | Time: 46.4s\n",
            "Epoch 10 | Train Loss: 2.1582, Train Char Acc: 0.4027, Train Word Acc: 0.0127 | Val Loss: 2.3466, Val Char Acc: 0.3473, Val Word Acc: 0.0170 | Time: 46.7s\n",
            "Epoch 11 | Train Loss: 2.0823, Train Char Acc: 0.4216, Train Word Acc: 0.0172 | Val Loss: 2.2657, Val Char Acc: 0.3682, Val Word Acc: 0.0207 | Time: 46.7s\n",
            "Epoch 12 | Train Loss: 2.0135, Train Char Acc: 0.4380, Train Word Acc: 0.0213 | Val Loss: 2.2128, Val Char Acc: 0.3821, Val Word Acc: 0.0275 | Time: 46.2s\n",
            "Epoch 13 | Train Loss: 1.9594, Train Char Acc: 0.4511, Train Word Acc: 0.0262 | Val Loss: 2.1580, Val Char Acc: 0.3972, Val Word Acc: 0.0327 | Time: 46.2s\n",
            "Epoch 14 | Train Loss: 1.9107, Train Char Acc: 0.4629, Train Word Acc: 0.0308 | Val Loss: 2.1251, Val Char Acc: 0.4028, Val Word Acc: 0.0360 | Time: 46.7s\n",
            "Epoch 15 | Train Loss: 1.8660, Train Char Acc: 0.4737, Train Word Acc: 0.0354 | Val Loss: 2.0725, Val Char Acc: 0.4159, Val Word Acc: 0.0425 | Time: 46.4s\n",
            "Epoch 16 | Train Loss: 1.8220, Train Char Acc: 0.4850, Train Word Acc: 0.0416 | Val Loss: 2.0438, Val Char Acc: 0.4235, Val Word Acc: 0.0498 | Time: 46.2s\n",
            "Epoch 17 | Train Loss: 1.7884, Train Char Acc: 0.4939, Train Word Acc: 0.0458 | Val Loss: 2.0122, Val Char Acc: 0.4314, Val Word Acc: 0.0530 | Time: 45.9s\n",
            "Epoch 18 | Train Loss: 1.7514, Train Char Acc: 0.5037, Train Word Acc: 0.0510 | Val Loss: 1.9876, Val Char Acc: 0.4368, Val Word Acc: 0.0625 | Time: 46.6s\n",
            "Epoch 19 | Train Loss: 1.7160, Train Char Acc: 0.5135, Train Word Acc: 0.0581 | Val Loss: 1.9547, Val Char Acc: 0.4484, Val Word Acc: 0.0691 | Time: 46.3s\n",
            "Epoch 20 | Train Loss: 1.6878, Train Char Acc: 0.5211, Train Word Acc: 0.0629 | Val Loss: 1.9297, Val Char Acc: 0.4538, Val Word Acc: 0.0780 | Time: 46.2s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec █▅▅▁▆▅▅▁▄▅▅▃▃▅▄▃▁▅▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▂▃▃▃▄▅▅▅▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▇▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▁▁▁▂▂▃▃▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 46.19884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.52114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.68781\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.06292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.45377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.92967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.07802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb32_hid64_layers3x2_lr0.0001_tf0.5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/syacpy66\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_004115-syacpy66/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 00:56:54,950 - wandb.wandb_agent - INFO - Cleaning up finished run: syacpy66\n",
            "2025-05-19 00:56:55,344 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 00:56:55,344 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.2\n",
            "\temb_size: 32\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 64\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-19 00:56:55,346 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=LSTM --dec_layers=3 --dropout=0.2 --emb_size=32 --enc_layers=3 --epochs=20 --hidden_size=64 --lr=0.001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-19 00:57:00,357 - wandb.wandb_agent - INFO - Running runs: ['oxwuu53s']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_005700-oxwuu53s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb32_hid64_layers3x3_lr0.001_tf0.7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/oxwuu53s\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.0632, Train Char Acc: 0.1969, Train Word Acc: 0.0000 | Val Loss: 3.1640, Val Char Acc: 0.1898, Val Word Acc: 0.0000 | Time: 30.5s\n",
            "Epoch 02 | Train Loss: 2.6088, Train Char Acc: 0.3003, Train Word Acc: 0.0011 | Val Loss: 2.6519, Val Char Acc: 0.2997, Val Word Acc: 0.0066 | Time: 31.0s\n",
            "Epoch 03 | Train Loss: 1.9679, Train Char Acc: 0.4604, Train Word Acc: 0.0224 | Val Loss: 2.2323, Val Char Acc: 0.4126, Val Word Acc: 0.0505 | Time: 30.0s\n",
            "Epoch 04 | Train Loss: 1.6288, Train Char Acc: 0.5458, Train Word Acc: 0.0580 | Val Loss: 2.0229, Val Char Acc: 0.4632, Val Word Acc: 0.0902 | Time: 31.0s\n",
            "Epoch 05 | Train Loss: 1.4549, Train Char Acc: 0.5931, Train Word Acc: 0.0983 | Val Loss: 1.9244, Val Char Acc: 0.4957, Val Word Acc: 0.1287 | Time: 30.1s\n",
            "Epoch 06 | Train Loss: 1.3393, Train Char Acc: 0.6266, Train Word Acc: 0.1318 | Val Loss: 1.8805, Val Char Acc: 0.5151, Val Word Acc: 0.1561 | Time: 31.0s\n",
            "Epoch 07 | Train Loss: 1.2610, Train Char Acc: 0.6491, Train Word Acc: 0.1627 | Val Loss: 1.8554, Val Char Acc: 0.5302, Val Word Acc: 0.1748 | Time: 30.1s\n",
            "Epoch 08 | Train Loss: 1.1906, Train Char Acc: 0.6703, Train Word Acc: 0.1883 | Val Loss: 1.8187, Val Char Acc: 0.5391, Val Word Acc: 0.1929 | Time: 30.6s\n",
            "Epoch 09 | Train Loss: 1.1442, Train Char Acc: 0.6841, Train Word Acc: 0.2135 | Val Loss: 1.8015, Val Char Acc: 0.5541, Val Word Acc: 0.2083 | Time: 30.4s\n",
            "Epoch 10 | Train Loss: 1.1020, Train Char Acc: 0.6970, Train Word Acc: 0.2355 | Val Loss: 1.8179, Val Char Acc: 0.5557, Val Word Acc: 0.2233 | Time: 30.4s\n",
            "Epoch 11 | Train Loss: 1.0783, Train Char Acc: 0.7046, Train Word Acc: 0.2531 | Val Loss: 1.7738, Val Char Acc: 0.5643, Val Word Acc: 0.2186 | Time: 30.9s\n",
            "Epoch 12 | Train Loss: 1.0464, Train Char Acc: 0.7138, Train Word Acc: 0.2704 | Val Loss: 1.7411, Val Char Acc: 0.5751, Val Word Acc: 0.2417 | Time: 30.1s\n",
            "Epoch 13 | Train Loss: 1.0258, Train Char Acc: 0.7201, Train Word Acc: 0.2848 | Val Loss: 1.8095, Val Char Acc: 0.5724, Val Word Acc: 0.2410 | Time: 31.1s\n",
            "Epoch 14 | Train Loss: 1.0035, Train Char Acc: 0.7269, Train Word Acc: 0.2986 | Val Loss: 1.7727, Val Char Acc: 0.5788, Val Word Acc: 0.2497 | Time: 30.1s\n",
            "Epoch 15 | Train Loss: 0.9872, Train Char Acc: 0.7313, Train Word Acc: 0.3099 | Val Loss: 1.7873, Val Char Acc: 0.5796, Val Word Acc: 0.2514 | Time: 31.0s\n",
            "Epoch 16 | Train Loss: 0.9695, Train Char Acc: 0.7372, Train Word Acc: 0.3248 | Val Loss: 1.7927, Val Char Acc: 0.5831, Val Word Acc: 0.2636 | Time: 30.0s\n",
            "Epoch 17 | Train Loss: 0.9565, Train Char Acc: 0.7412, Train Word Acc: 0.3348 | Val Loss: 1.7754, Val Char Acc: 0.5853, Val Word Acc: 0.2601 | Time: 31.0s\n",
            "Epoch 18 | Train Loss: 0.9433, Train Char Acc: 0.7449, Train Word Acc: 0.3446 | Val Loss: 1.7336, Val Char Acc: 0.5896, Val Word Acc: 0.2746 | Time: 30.1s\n",
            "Epoch 19 | Train Loss: 0.9291, Train Char Acc: 0.7494, Train Word Acc: 0.3561 | Val Loss: 1.7587, Val Char Acc: 0.5887, Val Word Acc: 0.2686 | Time: 31.1s\n",
            "Epoch 20 | Train Loss: 0.9164, Train Char Acc: 0.7529, Train Word Acc: 0.3639 | Val Loss: 1.7567, Val Char Acc: 0.5948, Val Word Acc: 0.2814 | Time: 30.1s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.005 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.005 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄█▁█▂▇▂▅▄▄▇▂█▂█▁█▂█▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▄▅▆▆▇▇▇▇▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▃▅▆▆▇▇▇▇▇▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▂▃▄▅▅▆▆▇▆▇▇▇▇█▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 30.11067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.75286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.9164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.36393\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.59478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.75669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.28137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb32_hid64_layers3x3_lr0.001_tf0.7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/oxwuu53s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_005700-oxwuu53s/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 01:07:18,945 - wandb.wandb_agent - INFO - Cleaning up finished run: oxwuu53s\n",
            "2025-05-19 01:07:19,363 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 01:07:19,363 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 256\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 256\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 01:07:19,364 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=3 --cell_type=LSTM --dec_layers=2 --dropout=0.2 --emb_size=256 --enc_layers=3 --epochs=20 --hidden_size=256 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_010723-hryn7s9c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb256_hid256_layers3x2_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/hryn7s9c\u001b[0m\n",
            "2025-05-19 01:07:24,374 - wandb.wandb_agent - INFO - Running runs: ['hryn7s9c']\n",
            "Epoch 01 | Train Loss: 2.6211, Train Char Acc: 0.3012, Train Word Acc: 0.0074 | Val Loss: 2.0538, Val Char Acc: 0.4090, Val Word Acc: 0.0321 | Time: 96.4s\n",
            "Epoch 02 | Train Loss: 1.7263, Train Char Acc: 0.5000, Train Word Acc: 0.0710 | Val Loss: 1.7069, Val Char Acc: 0.5007, Val Word Acc: 0.1071 | Time: 96.1s\n",
            "Epoch 03 | Train Loss: 1.4504, Train Char Acc: 0.5784, Train Word Acc: 0.1439 | Val Loss: 1.5838, Val Char Acc: 0.5389, Val Word Acc: 0.1595 | Time: 97.0s\n",
            "Epoch 04 | Train Loss: 1.2924, Train Char Acc: 0.6285, Train Word Acc: 0.2083 | Val Loss: 1.5148, Val Char Acc: 0.5663, Val Word Acc: 0.2042 | Time: 95.6s\n",
            "Epoch 05 | Train Loss: 1.1798, Train Char Acc: 0.6662, Train Word Acc: 0.2737 | Val Loss: 1.4792, Val Char Acc: 0.5788, Val Word Acc: 0.2177 | Time: 96.2s\n",
            "Epoch 06 | Train Loss: 1.0900, Train Char Acc: 0.6959, Train Word Acc: 0.3321 | Val Loss: 1.4537, Val Char Acc: 0.5925, Val Word Acc: 0.2456 | Time: 96.7s\n",
            "Epoch 07 | Train Loss: 1.0198, Train Char Acc: 0.7194, Train Word Acc: 0.3886 | Val Loss: 1.4530, Val Char Acc: 0.5967, Val Word Acc: 0.2581 | Time: 97.3s\n",
            "Epoch 08 | Train Loss: 0.9611, Train Char Acc: 0.7387, Train Word Acc: 0.4346 | Val Loss: 1.4462, Val Char Acc: 0.6023, Val Word Acc: 0.2712 | Time: 97.6s\n",
            "Epoch 09 | Train Loss: 0.9081, Train Char Acc: 0.7551, Train Word Acc: 0.4748 | Val Loss: 1.4531, Val Char Acc: 0.6073, Val Word Acc: 0.2699 | Time: 98.5s\n",
            "Epoch 10 | Train Loss: 0.8653, Train Char Acc: 0.7690, Train Word Acc: 0.5153 | Val Loss: 1.4439, Val Char Acc: 0.6141, Val Word Acc: 0.2795 | Time: 99.1s\n",
            "Epoch 11 | Train Loss: 0.8263, Train Char Acc: 0.7811, Train Word Acc: 0.5504 | Val Loss: 1.4609, Val Char Acc: 0.6155, Val Word Acc: 0.2929 | Time: 96.9s\n",
            "Epoch 12 | Train Loss: 0.7924, Train Char Acc: 0.7915, Train Word Acc: 0.5810 | Val Loss: 1.4671, Val Char Acc: 0.6191, Val Word Acc: 0.2925 | Time: 97.0s\n",
            "Epoch 13 | Train Loss: 0.7653, Train Char Acc: 0.7996, Train Word Acc: 0.6063 | Val Loss: 1.4791, Val Char Acc: 0.6197, Val Word Acc: 0.2942 | Time: 97.5s\n",
            "Epoch 14 | Train Loss: 0.7377, Train Char Acc: 0.8079, Train Word Acc: 0.6341 | Val Loss: 1.5221, Val Char Acc: 0.6174, Val Word Acc: 0.2956 | Time: 96.9s\n",
            "Epoch 15 | Train Loss: 0.7179, Train Char Acc: 0.8140, Train Word Acc: 0.6569 | Val Loss: 1.5510, Val Char Acc: 0.6128, Val Word Acc: 0.2847 | Time: 96.9s\n",
            "Epoch 16 | Train Loss: 0.6972, Train Char Acc: 0.8201, Train Word Acc: 0.6761 | Val Loss: 1.5624, Val Char Acc: 0.6211, Val Word Acc: 0.3005 | Time: 97.3s\n",
            "Epoch 17 | Train Loss: 0.6795, Train Char Acc: 0.8254, Train Word Acc: 0.6964 | Val Loss: 1.5596, Val Char Acc: 0.6198, Val Word Acc: 0.2944 | Time: 97.4s\n",
            "Epoch 18 | Train Loss: 0.6628, Train Char Acc: 0.8300, Train Word Acc: 0.7122 | Val Loss: 1.5616, Val Char Acc: 0.6228, Val Word Acc: 0.2941 | Time: 97.5s\n",
            "Epoch 19 | Train Loss: 0.6481, Train Char Acc: 0.8346, Train Word Acc: 0.7300 | Val Loss: 1.5729, Val Char Acc: 0.6222, Val Word Acc: 0.2919 | Time: 97.5s\n",
            "Epoch 20 | Train Loss: 0.6362, Train Char Acc: 0.8379, Train Word Acc: 0.7426 | Val Loss: 1.5961, Val Char Acc: 0.6235, Val Word Acc: 0.2983 | Time: 97.4s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.005 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.005 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▂▂▄▁▂▃▄▅▇█▄▄▅▄▃▄▅▅▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▄▅▅▆▆▆▇▇▇▇▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▂▂▃▄▄▅▅▅▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▄▅▆▇▇▇▇▇███████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▃▄▅▆▇▇▇▇▇██████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 97.41866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.83788\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.63622\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.74258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.62348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.59608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.29832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb256_hid256_layers3x2_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/hryn7s9c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_010723-hryn7s9c/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 01:39:57,450 - wandb.wandb_agent - INFO - Cleaning up finished run: hryn7s9c\n",
            "2025-05-19 01:39:57,992 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 01:39:57,992 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 128\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 2\n",
            "\tdropout: 0.2\n",
            "\temb_size: 64\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 32\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.7\n",
            "2025-05-19 01:39:57,994 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=128 --beam_size=3 --cell_type=LSTM --dec_layers=2 --dropout=0.2 --emb_size=64 --enc_layers=3 --epochs=20 --hidden_size=32 --lr=0.001 --tf_ratio=0.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_014002-xce7ea7c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb64_hid32_layers3x2_lr0.001_tf0.7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/xce7ea7c\u001b[0m\n",
            "2025-05-19 01:40:03,004 - wandb.wandb_agent - INFO - Running runs: ['xce7ea7c']\n",
            "Epoch 01 | Train Loss: 3.0723, Train Char Acc: 0.2018, Train Word Acc: 0.0001 | Val Loss: 3.1732, Val Char Acc: 0.1822, Val Word Acc: 0.0000 | Time: 26.7s\n",
            "Epoch 02 | Train Loss: 2.7361, Train Char Acc: 0.2722, Train Word Acc: 0.0002 | Val Loss: 2.9285, Val Char Acc: 0.2372, Val Word Acc: 0.0005 | Time: 27.1s\n",
            "Epoch 03 | Train Loss: 2.4176, Train Char Acc: 0.3502, Train Word Acc: 0.0023 | Val Loss: 2.6885, Val Char Acc: 0.2864, Val Word Acc: 0.0050 | Time: 26.7s\n",
            "Epoch 04 | Train Loss: 2.1475, Train Char Acc: 0.4157, Train Word Acc: 0.0095 | Val Loss: 2.4648, Val Char Acc: 0.3447, Val Word Acc: 0.0183 | Time: 26.3s\n",
            "Epoch 05 | Train Loss: 1.9676, Train Char Acc: 0.4586, Train Word Acc: 0.0217 | Val Loss: 2.3232, Val Char Acc: 0.3794, Val Word Acc: 0.0307 | Time: 26.4s\n",
            "Epoch 06 | Train Loss: 1.8383, Train Char Acc: 0.4903, Train Word Acc: 0.0324 | Val Loss: 2.2391, Val Char Acc: 0.4012, Val Word Acc: 0.0485 | Time: 26.3s\n",
            "Epoch 07 | Train Loss: 1.7445, Train Char Acc: 0.5139, Train Word Acc: 0.0438 | Val Loss: 2.1713, Val Char Acc: 0.4223, Val Word Acc: 0.0603 | Time: 26.3s\n",
            "Epoch 08 | Train Loss: 1.6732, Train Char Acc: 0.5323, Train Word Acc: 0.0556 | Val Loss: 2.1307, Val Char Acc: 0.4356, Val Word Acc: 0.0707 | Time: 26.4s\n",
            "Epoch 09 | Train Loss: 1.6223, Train Char Acc: 0.5459, Train Word Acc: 0.0642 | Val Loss: 2.0543, Val Char Acc: 0.4460, Val Word Acc: 0.0811 | Time: 26.6s\n",
            "Epoch 10 | Train Loss: 1.5711, Train Char Acc: 0.5597, Train Word Acc: 0.0723 | Val Loss: 2.0791, Val Char Acc: 0.4542, Val Word Acc: 0.0875 | Time: 27.1s\n",
            "Epoch 11 | Train Loss: 1.5363, Train Char Acc: 0.5697, Train Word Acc: 0.0794 | Val Loss: 2.0467, Val Char Acc: 0.4573, Val Word Acc: 0.0970 | Time: 26.3s\n",
            "Epoch 12 | Train Loss: 1.5078, Train Char Acc: 0.5781, Train Word Acc: 0.0853 | Val Loss: 2.0378, Val Char Acc: 0.4647, Val Word Acc: 0.1062 | Time: 26.2s\n",
            "Epoch 13 | Train Loss: 1.4799, Train Char Acc: 0.5862, Train Word Acc: 0.0927 | Val Loss: 2.0284, Val Char Acc: 0.4710, Val Word Acc: 0.1055 | Time: 26.3s\n",
            "Epoch 14 | Train Loss: 1.4637, Train Char Acc: 0.5910, Train Word Acc: 0.0990 | Val Loss: 2.0153, Val Char Acc: 0.4752, Val Word Acc: 0.1196 | Time: 26.4s\n",
            "Epoch 15 | Train Loss: 1.4431, Train Char Acc: 0.5975, Train Word Acc: 0.1057 | Val Loss: 1.9594, Val Char Acc: 0.4863, Val Word Acc: 0.1240 | Time: 26.3s\n",
            "Epoch 16 | Train Loss: 1.4228, Train Char Acc: 0.6027, Train Word Acc: 0.1089 | Val Loss: 1.9792, Val Char Acc: 0.4875, Val Word Acc: 0.1299 | Time: 26.3s\n",
            "Epoch 17 | Train Loss: 1.4086, Train Char Acc: 0.6072, Train Word Acc: 0.1156 | Val Loss: 1.9499, Val Char Acc: 0.4892, Val Word Acc: 0.1346 | Time: 26.5s\n",
            "Epoch 18 | Train Loss: 1.3944, Train Char Acc: 0.6112, Train Word Acc: 0.1178 | Val Loss: 1.9555, Val Char Acc: 0.4971, Val Word Acc: 0.1336 | Time: 27.1s\n",
            "Epoch 19 | Train Loss: 1.3824, Train Char Acc: 0.6149, Train Word Acc: 0.1236 | Val Loss: 1.9558, Val Char Acc: 0.4977, Val Word Acc: 0.1399 | Time: 26.3s\n",
            "Epoch 20 | Train Loss: 1.3657, Train Char Acc: 0.6195, Train Word Acc: 0.1287 | Val Loss: 1.9592, Val Char Acc: 0.4986, Val Word Acc: 0.1443 | Time: 26.4s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▄█▅▂▃▂▂▂▄█▂▁▁▂▂▂▃█▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▃▅▅▆▆▇▇▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▂▃▅▅▆▆▇▇▇▇▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇█▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 26.40161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.61955\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.36573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.12867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.49858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.95922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.14427\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb64_hid32_layers3x2_lr0.001_tf0.7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/xce7ea7c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_014002-xce7ea7c/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 01:49:00,650 - wandb.wandb_agent - INFO - Cleaning up finished run: xce7ea7c\n",
            "2025-05-19 01:49:01,115 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 01:49:01,116 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 64\n",
            "\tbeam_size: 3\n",
            "\tcell_type: LSTM\n",
            "\tdec_layers: 1\n",
            "\tdropout: 0.2\n",
            "\temb_size: 16\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 64\n",
            "\tlr: 0.0001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 01:49:01,118 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=64 --beam_size=3 --cell_type=LSTM --dec_layers=1 --dropout=0.2 --emb_size=16 --enc_layers=3 --epochs=20 --hidden_size=64 --lr=0.0001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "2025-05-19 01:49:06,132 - wandb.wandb_agent - INFO - Running runs: ['77yy78u3']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_014905-77yy78u3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLSTM_emb16_hid64_layers3x1_lr0.0001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/77yy78u3\u001b[0m\n",
            "Epoch 01 | Train Loss: 3.3393, Train Char Acc: 0.1539, Train Word Acc: 0.0000 | Val Loss: 3.1713, Val Char Acc: 0.1748, Val Word Acc: 0.0000 | Time: 39.4s\n",
            "Epoch 02 | Train Loss: 3.0875, Train Char Acc: 0.1963, Train Word Acc: 0.0000 | Val Loss: 3.1018, Val Char Acc: 0.1900, Val Word Acc: 0.0000 | Time: 38.8s\n",
            "Epoch 03 | Train Loss: 3.0314, Train Char Acc: 0.2037, Train Word Acc: 0.0000 | Val Loss: 3.0754, Val Char Acc: 0.1984, Val Word Acc: 0.0000 | Time: 39.1s\n",
            "Epoch 04 | Train Loss: 2.9704, Train Char Acc: 0.2189, Train Word Acc: 0.0001 | Val Loss: 3.0269, Val Char Acc: 0.2096, Val Word Acc: 0.0000 | Time: 39.4s\n",
            "Epoch 05 | Train Loss: 2.9102, Train Char Acc: 0.2346, Train Word Acc: 0.0001 | Val Loss: 2.9907, Val Char Acc: 0.2132, Val Word Acc: 0.0002 | Time: 38.9s\n",
            "Epoch 06 | Train Loss: 2.8470, Train Char Acc: 0.2475, Train Word Acc: 0.0002 | Val Loss: 2.9038, Val Char Acc: 0.2295, Val Word Acc: 0.0003 | Time: 39.0s\n",
            "Epoch 07 | Train Loss: 2.7757, Train Char Acc: 0.2584, Train Word Acc: 0.0005 | Val Loss: 2.8437, Val Char Acc: 0.2381, Val Word Acc: 0.0005 | Time: 38.8s\n",
            "Epoch 08 | Train Loss: 2.7031, Train Char Acc: 0.2721, Train Word Acc: 0.0009 | Val Loss: 2.7749, Val Char Acc: 0.2504, Val Word Acc: 0.0006 | Time: 38.7s\n",
            "Epoch 09 | Train Loss: 2.6351, Train Char Acc: 0.2857, Train Word Acc: 0.0013 | Val Loss: 2.7082, Val Char Acc: 0.2627, Val Word Acc: 0.0016 | Time: 39.5s\n",
            "Epoch 10 | Train Loss: 2.5553, Train Char Acc: 0.3049, Train Word Acc: 0.0022 | Val Loss: 2.6201, Val Char Acc: 0.2854, Val Word Acc: 0.0032 | Time: 39.8s\n",
            "Epoch 11 | Train Loss: 2.4796, Train Char Acc: 0.3222, Train Word Acc: 0.0037 | Val Loss: 2.5573, Val Char Acc: 0.2975, Val Word Acc: 0.0057 | Time: 39.1s\n",
            "Epoch 12 | Train Loss: 2.4098, Train Char Acc: 0.3371, Train Word Acc: 0.0050 | Val Loss: 2.4873, Val Char Acc: 0.3137, Val Word Acc: 0.0073 | Time: 38.7s\n",
            "Epoch 13 | Train Loss: 2.3440, Train Char Acc: 0.3518, Train Word Acc: 0.0065 | Val Loss: 2.4281, Val Char Acc: 0.3270, Val Word Acc: 0.0069 | Time: 38.7s\n",
            "Epoch 14 | Train Loss: 2.2879, Train Char Acc: 0.3636, Train Word Acc: 0.0082 | Val Loss: 2.3720, Val Char Acc: 0.3396, Val Word Acc: 0.0087 | Time: 39.0s\n",
            "Epoch 15 | Train Loss: 2.2422, Train Char Acc: 0.3738, Train Word Acc: 0.0101 | Val Loss: 2.3321, Val Char Acc: 0.3469, Val Word Acc: 0.0119 | Time: 38.7s\n",
            "Epoch 16 | Train Loss: 2.1929, Train Char Acc: 0.3856, Train Word Acc: 0.0125 | Val Loss: 2.2802, Val Char Acc: 0.3597, Val Word Acc: 0.0152 | Time: 39.5s\n",
            "Epoch 17 | Train Loss: 2.1523, Train Char Acc: 0.3941, Train Word Acc: 0.0142 | Val Loss: 2.2320, Val Char Acc: 0.3721, Val Word Acc: 0.0179 | Time: 38.5s\n",
            "Epoch 18 | Train Loss: 2.1071, Train Char Acc: 0.4051, Train Word Acc: 0.0175 | Val Loss: 2.1993, Val Char Acc: 0.3798, Val Word Acc: 0.0210 | Time: 38.5s\n",
            "Epoch 19 | Train Loss: 2.0744, Train Char Acc: 0.4126, Train Word Acc: 0.0200 | Val Loss: 2.1652, Val Char Acc: 0.3874, Val Word Acc: 0.0247 | Time: 38.8s\n",
            "Epoch 20 | Train Loss: 2.0424, Train Char Acc: 0.4207, Train Word Acc: 0.0224 | Val Loss: 2.1328, Val Char Acc: 0.3952, Val Word Acc: 0.0269 | Time: 38.5s\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.001 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 0.001 MB of 0.017 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec ▆▃▄▆▃▄▃▂▆█▄▂▂▄▂▆▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc ▁▂▂▃▃▃▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc ▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc ▁▁▂▂▂▃▃▃▄▅▅▅▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc ▁▁▁▁▁▁▁▁▁▂▂▃▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_time_sec 38.51305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_char_acc 0.42073\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 2.0424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_word_acc 0.02242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_char_acc 0.39522\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.13281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_word_acc 0.02694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLSTM_emb16_hid64_layers3x1_lr0.0001_tf0.3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/77yy78u3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250519_014905-77yy78u3/logs\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\n",
            "2025-05-19 02:02:17,561 - wandb.wandb_agent - INFO - Cleaning up finished run: 77yy78u3\n",
            "2025-05-19 02:02:18,018 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2025-05-19 02:02:18,018 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tbatch_size: 32\n",
            "\tbeam_size: 1\n",
            "\tcell_type: GRU\n",
            "\tdec_layers: 3\n",
            "\tdropout: 0.2\n",
            "\temb_size: 16\n",
            "\tenc_layers: 3\n",
            "\tepochs: 20\n",
            "\thidden_size: 256\n",
            "\tlr: 0.001\n",
            "\ttf_ratio: 0.3\n",
            "2025-05-19 02:02:18,020 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --batch_size=32 --beam_size=1 --cell_type=GRU --dec_layers=3 --dropout=0.2 --emb_size=16 --enc_layers=3 --epochs=20 --hidden_size=256 --lr=0.001 --tf_ratio=0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrsagarbiswas\u001b[0m (\u001b[33mmrsagarbiswas-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'Vanilla_RNN' when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250519_020222-ws0lg1id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGRU_emb16_hid256_layers3x3_lr0.001_tf0.3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/sweeps/m4hl6ra2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mrsagarbiswas-iit-madras/Vanilla_RNN/runs/ws0lg1id\u001b[0m\n",
            "2025-05-19 02:02:23,031 - wandb.wandb_agent - INFO - Running runs: ['ws0lg1id']\n",
            "Epoch 01 | Train Loss: 1.6898, Train Char Acc: 0.5274, Train Word Acc: 0.1299 | Val Loss: 1.4913, Val Char Acc: 0.5787, Val Word Acc: 0.2108 | Time: 102.5s\n",
            "Epoch 02 | Train Loss: 1.0780, Train Char Acc: 0.6999, Train Word Acc: 0.3376 | Val Loss: 1.4370, Val Char Acc: 0.6064, Val Word Acc: 0.2648 | Time: 102.8s\n",
            "Epoch 03 | Train Loss: 0.9475, Train Char Acc: 0.7401, Train Word Acc: 0.4296 | Val Loss: 1.4750, Val Char Acc: 0.6131, Val Word Acc: 0.2733 | Time: 104.3s\n",
            "Epoch 04 | Train Loss: 0.8791, Train Char Acc: 0.7617, Train Word Acc: 0.4848 | Val Loss: 1.4902, Val Char Acc: 0.6167, Val Word Acc: 0.2915 | Time: 102.2s\n",
            "Epoch 05 | Train Loss: 0.8385, Train Char Acc: 0.7739, Train Word Acc: 0.5165 | Val Loss: 1.4925, Val Char Acc: 0.6210, Val Word Acc: 0.2815 | Time: 102.0s\n",
            "Epoch 06 | Train Loss: 0.8160, Train Char Acc: 0.7801, Train Word Acc: 0.5354 | Val Loss: 1.5102, Val Char Acc: 0.6237, Val Word Acc: 0.2976 | Time: 102.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30A_wUwI0yZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}